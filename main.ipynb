{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b72490b",
   "metadata": {},
   "source": [
    "# Web Page Summarization Evaluation Workflow\n",
    "\n",
    "This notebook evaluates the quality of web page summaries generated by different LLM engines using an LLM-as-a-judge approach with G-Eval methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f18cb5",
   "metadata": {},
   "source": [
    "## 1. Creating a Gold Standard\n",
    "\n",
    "This section creates a gold standard dataset using the GPT-5.2 model to generate high-quality summaries. The gold standard will serve as a reference for evaluating other summarization engines and for distillation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cb2b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 items from baseline dataset\n",
      "Warning: Model 'gpt-5.2-2025-12-11' not recognized by tiktoken. Using 'gpt-5' tokenizer as fallback.\n",
      "Generating gold standard summaries with GPT-5.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:   2%|▏         | 22/1000 [00:22<14:58,  1.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 324199 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  13%|█▎        | 132/1000 [01:58<13:05,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 680772 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  18%|█▊        | 183/1000 [02:45<09:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 454064 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  44%|████▍     | 439/1000 [06:43<08:07,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 457769 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  50%|████▉     | 496/1000 [07:39<09:09,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 352591 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  52%|█████▏    | 518/1000 [07:57<04:52,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 310075 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  65%|██████▍   | 646/1000 [09:54<04:44,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 677936 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  79%|███████▉  | 789/1000 [11:57<01:29,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 374068 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 1000/1000 [15:18<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Gold standard dataset saved to data/goldstandard_1k.json\n",
      "✓ Total items processed: 992\n",
      "⚠ Skipped 8 items due to token limits:\n",
      "  - https://pmc.ncbi.nlm.nih.gov/articles/PMC7271218/\n",
      "  - https://www.jsog.or.jp/activity/pdf/gl_fujinka_2023.pdf\n",
      "  - https://weatherspark.com/h/y/557/2024/Historical-Weather-during-2024-in-San-Francisco-California-United-States\n",
      "  - https://servicehub.ucdavis.edu/servicehub?id=ucd_kb_article&sys_id=cf60ebc293f1e69083cc38797bba1020\n",
      "  - https://s5.static.brasilescola.uol.com.br/vestibular/2022/12/resultado-cederj-2023.pdf\n",
      "  - https://colab.research.google.com/github/hc9903/deepke/blob/master/isa.ipynb\n",
      "  - https://www.insp.mx/resources/images/stories/INSP/Docs/Transparencia/EDICION%202011%20MEDICAMENTOS%20-%20link.pdf\n",
      "  - https://s2.static.brasilescola.uol.com.br/vestibular/2024/01/resultado-cederj-2024.pdf\n",
      "✓ Total estimated cost for gold standard generation (based on tokens and rates): $30.9979\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from agents.summarizer import Summarizer\n",
    "from tqdm.asyncio import tqdm as atqdm\n",
    "\n",
    "# Load baseline data\n",
    "with open('data/baseline_1k.json', 'r', encoding='utf-8') as f:\n",
    "    baseline_data = json.load(f)\n",
    "\n",
    "# Extract the data list from the top-level structure\n",
    "baseline_data = baseline_data['data']\n",
    "print(f\"Loaded {len(baseline_data)} items from baseline dataset\")\n",
    "\n",
    "# Initialize the gold standard summarizer with GPT-5.2\n",
    "gold_summarizer = Summarizer(model=\"gpt-5.2-2025-12-11\")\n",
    "\n",
    "# Initialize variables to track total cost and skipped items\n",
    "total_cost = 0.0\n",
    "gold_standard_data = []\n",
    "skipped_items = []\n",
    "\n",
    "async def summarize_item(item):\n",
    "    \"\"\"Async wrapper to summarize a single item\"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    # Run the synchronous summarize in a thread pool\n",
    "    summary, cost = await loop.run_in_executor(\n",
    "        None, \n",
    "        lambda: gold_summarizer.summarize(item['markdown_content'], get_cost=True, allow_long_context=True)\n",
    "    )\n",
    "    \n",
    "    # Check if the request was skipped due to token limits\n",
    "    if summary is None:\n",
    "        return None, item['url']\n",
    "    \n",
    "    return {\n",
    "        'url': item['url'],\n",
    "        'markdown_content': item['markdown_content'],\n",
    "        'summary': summary\n",
    "    }, cost\n",
    "\n",
    "async def generate_summaries():\n",
    "    \"\"\"Generate all summaries concurrently\"\"\"\n",
    "    global total_cost, gold_standard_data, skipped_items\n",
    "    \n",
    "    # Create tasks for all items (limit concurrency to avoid rate limits)\n",
    "    max_concurrent = 10  # Adjust based on your API rate limits\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def summarize_with_semaphore(item):\n",
    "        async with semaphore:\n",
    "            return await summarize_item(item)\n",
    "    \n",
    "    # Process all items concurrently with progress bar\n",
    "    print(\"Generating gold standard summaries with GPT-5.2...\")\n",
    "    tasks = [summarize_with_semaphore(item) for item in baseline_data]\n",
    "    results = await atqdm.gather(*tasks, desc=\"Processing items\")\n",
    "    \n",
    "    # Collect results and accumulate costs\n",
    "    for result in results:\n",
    "        if result[0] is None:\n",
    "            # Item was skipped due to token limits\n",
    "            skipped_items.append(result[1])\n",
    "        else:\n",
    "            gold_item, cost = result\n",
    "            gold_standard_data.append(gold_item)\n",
    "            total_cost += cost\n",
    "\n",
    "# Run the async function\n",
    "await generate_summaries()\n",
    "\n",
    "# Save the gold standard dataset\n",
    "output_path = 'data/goldstandard_1k.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(gold_standard_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Gold standard dataset saved to {output_path}\")\n",
    "print(f\"✓ Total items processed: {len(gold_standard_data)}\")\n",
    "if skipped_items:\n",
    "    print(f\"⚠ Skipped {len(skipped_items)} items due to token limits:\")\n",
    "    for url in skipped_items:\n",
    "        print(f\"  - {url}\")\n",
    "print(f\"✓ Total estimated cost for gold standard generation (based on tokens and rates): ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d9f9f",
   "metadata": {},
   "source": [
    "For benchmarking, ignoring the long requests is fine. Production will need to handle inference that can handle these requests by cleaning / splitting the requests.\n",
    "\n",
    "At this point I'll split this gold standard to train-validation subsets, so that all evaluations will be done on the same web-pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05675f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 992 items from gold standard\n",
      "Using tokenizer: o200k_base\n",
      "✓ Training candidates: 404 items (≤1,500 chars & ≤64,000 tokens)\n",
      "✓ Validation set base: 588 items (samples not meeting training criteria)\n",
      "\n",
      "Final split:\n",
      "Train set: 404 items (selected from candidates)\n",
      "Validation set: 588 items (all remaining samples)\n",
      "\n",
      "✓ Train set saved to data/goldstandard_train.json\n",
      "✓ Validation set saved to data/goldstandard_validation.json\n",
      "✓ All training examples meet both constraints: ≤64,000 tokens & ≤1,500 chars\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "PAGE_MAX_TOKENS = 64000\n",
    "\n",
    "# Load the gold standard dataset\n",
    "with open('data/goldstandard_1k.json', 'r', encoding='utf-8') as f:\n",
    "    gold_standard = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(gold_standard)} items from gold standard\")\n",
    "\n",
    "# Initialize tokenizer for token counting\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")  # gpt-4.1 uses same tokenizer as gpt-4o\n",
    "print(f\"Using tokenizer: {tokenizer.name}\")\n",
    "\n",
    "# Define constraints for training data\n",
    "SUMMARY_MAX_CHARS = 1500\n",
    "PAGE_MAX_TOKENS = 64000\n",
    "\n",
    "# Identify training candidates: summaries ≤1500 chars AND content ≤64K tokens\n",
    "training_candidates = []\n",
    "validation_data = []\n",
    "\n",
    "for item in gold_standard:\n",
    "    summary_length = len(item['summary'])\n",
    "    token_count = len(tokenizer.encode(item['markdown_content']))\n",
    "    \n",
    "    # Check if item qualifies for training (both constraints)\n",
    "    if summary_length <= SUMMARY_MAX_CHARS and token_count <= PAGE_MAX_TOKENS:\n",
    "        training_candidates.append(item)\n",
    "    else:\n",
    "        validation_data.append(item)\n",
    "\n",
    "print(f\"✓ Training candidates: {len(training_candidates)} items (≤{SUMMARY_MAX_CHARS:,} chars & ≤{PAGE_MAX_TOKENS:,} tokens)\")\n",
    "print(f\"✓ Validation set base: {len(validation_data)} items (samples not meeting training criteria)\")\n",
    "\n",
    "# Take up to 50% of original dataset for training from candidates\n",
    "target_train_size = len(gold_standard) // 2\n",
    "train_data = training_candidates[:target_train_size]\n",
    "\n",
    "# Add any remaining training candidates to validation\n",
    "validation_data.extend(training_candidates[target_train_size:])\n",
    "\n",
    "print(f\"\\nFinal split:\")\n",
    "print(f\"Train set: {len(train_data)} items (selected from candidates)\")\n",
    "print(f\"Validation set: {len(validation_data)} items (all remaining samples)\")\n",
    "\n",
    "# Save train set (for distillation)\n",
    "train_path = 'data/goldstandard_train.json'\n",
    "with open(train_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save validation set (for evaluation)\n",
    "validation_path = 'data/goldstandard_validation.json'\n",
    "with open(validation_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(validation_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Train set saved to {train_path}\")\n",
    "print(f\"✓ Validation set saved to {validation_path}\")\n",
    "print(f\"✓ All training examples meet both constraints: ≤{PAGE_MAX_TOKENS:,} tokens & ≤{SUMMARY_MAX_CHARS:,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa54ed38",
   "metadata": {},
   "source": [
    "## 2. Distilling the Gold Standard into a Smaller Model\n",
    "\n",
    "This part will attempt to distil the intelligence of the GPT-5.2 model into smaller variants of the GPT-4.1 family, in order to create faster-cheaper solutions of similar performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "130b4415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Starting fine-tuning for gpt-4.1-mini-2025-04-14\n",
      "======================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: gpt-4.1-mini-2025-04-14\n",
      "Training samples: 404\n",
      "Total tokens: 2,977,919\n",
      "Epochs: 3\n",
      "Estimated training cost: $44.67\n",
      "Training file: data\\train_gpt-4.1-mini-2025-04-14.jsonl\n",
      "============================================================\n",
      "\n",
      "Fine-tuning job submitted: ftjob-GausB92vCt2Mfm7YyAU2V0J2\n",
      "Monitor at: https://platform.openai.com/finetune/ftjob-GausB92vCt2Mfm7YyAU2V0J2\n",
      "\n",
      "[1768853681] Created fine-tuning job: ftjob-GausB92vCt2Mfm7YyAU2V0J2\n",
      "[1768853681] Validating training file: file-Ch4D5iWYsj6r5JYfj1MLVW\n",
      "[1768853905] Files validated, moving job to queued state\n",
      "[1768853915] Fine-tuning job started\n",
      "[1768854076] Step 1/1212: training loss=1.27\n",
      "[1768854076] Step 2/1212: training loss=1.91\n",
      "[1768854082] Step 3/1212: training loss=1.41\n",
      "[1768854084] Step 4/1212: training loss=0.65\n",
      "[1768854084] Step 5/1212: training loss=1.14\n",
      "[1768854084] Step 6/1212: training loss=1.41\n",
      "[1768854088] Step 7/1212: training loss=0.81\n",
      "[1768854088] Step 8/1212: training loss=1.33\n",
      "[1768854088] Step 9/1212: training loss=0.96\n",
      "[1768854088] Step 10/1212: training loss=1.22\n",
      "[1768854089] Step 11/1212: training loss=0.75\n",
      "[1768854094] Step 12/1212: training loss=1.35\n",
      "[1768854094] Step 13/1212: training loss=0.68\n",
      "[1768854094] Step 14/1212: training loss=0.92\n",
      "[1768854094] Step 15/1212: training loss=1.22\n",
      "[1768854094] Step 16/1212: training loss=1.12\n",
      "[1768854094] Step 17/1212: training loss=0.83\n",
      "[1768854094] Step 18/1212: training loss=0.64\n",
      "[1768854099] Step 19/1212: training loss=0.76\n",
      "[1768854100] Step 20/1212: training loss=0.89\n",
      "[1768854100] Step 21/1212: training loss=0.93\n",
      "[1768854105] Step 22/1212: training loss=0.97\n",
      "[1768854105] Step 23/1212: training loss=0.92\n",
      "[1768854105] Step 24/1212: training loss=0.82\n",
      "[1768854105] Step 25/1212: training loss=0.61\n",
      "[1768854110] Step 26/1212: training loss=0.87\n",
      "[1768854110] Step 27/1212: training loss=0.87\n",
      "[1768854110] Step 28/1212: training loss=0.73\n",
      "[1768854110] Step 29/1212: training loss=0.73\n",
      "[1768854116] Step 30/1212: training loss=1.29\n",
      "[1768854117] Step 31/1212: training loss=0.46\n",
      "[1768854117] Step 32/1212: training loss=0.94\n",
      "[1768854117] Step 33/1212: training loss=0.68\n",
      "[1768854122] Step 34/1212: training loss=0.92\n",
      "[1768854122] Step 35/1212: training loss=1.00\n",
      "[1768854122] Step 36/1212: training loss=0.70\n",
      "[1768854122] Step 37/1212: training loss=0.92\n",
      "[1768854122] Step 38/1212: training loss=1.95\n",
      "[1768854122] Step 39/1212: training loss=0.72\n",
      "[1768854127] Step 40/1212: training loss=0.60\n",
      "[1768854128] Step 41/1212: training loss=0.88\n",
      "[1768854128] Step 42/1212: training loss=0.77\n",
      "[1768854128] Step 43/1212: training loss=0.61\n",
      "[1768854133] Step 44/1212: training loss=0.78\n",
      "[1768854133] Step 45/1212: training loss=0.95\n",
      "[1768854133] Step 46/1212: training loss=0.69\n",
      "[1768854133] Step 47/1212: training loss=1.39\n",
      "[1768854138] Step 48/1212: training loss=0.81\n",
      "[1768854138] Step 49/1212: training loss=2.74\n",
      "[1768854138] Step 50/1212: training loss=0.73\n",
      "[1768854139] Step 51/1212: training loss=1.50\n",
      "[1768854143] Step 52/1212: training loss=0.50\n",
      "[1768854144] Step 53/1212: training loss=0.41\n",
      "[1768854144] Step 54/1212: training loss=2.64\n",
      "[1768854144] Step 55/1212: training loss=0.68\n",
      "[1768854144] Step 56/1212: training loss=0.62\n",
      "[1768854150] Step 57/1212: training loss=0.59\n",
      "[1768854150] Step 58/1212: training loss=2.07\n",
      "[1768854150] Step 59/1212: training loss=0.50\n",
      "[1768854150] Step 60/1212: training loss=0.79\n",
      "[1768854151] Step 61/1212: training loss=0.72\n",
      "[1768854151] Step 62/1212: training loss=0.64\n",
      "[1768854156] Step 63/1212: training loss=2.22\n",
      "[1768854156] Step 64/1212: training loss=1.24\n",
      "[1768854156] Step 65/1212: training loss=0.52\n",
      "[1768854156] Step 66/1212: training loss=0.57\n",
      "[1768854156] Step 67/1212: training loss=0.68\n",
      "[1768854161] Step 68/1212: training loss=0.81\n",
      "[1768854161] Step 69/1212: training loss=0.84\n",
      "[1768854161] Step 70/1212: training loss=0.61\n",
      "[1768854162] Step 71/1212: training loss=0.46\n",
      "[1768854162] Step 72/1212: training loss=0.57\n",
      "[1768854167] Step 73/1212: training loss=0.81\n",
      "[1768854167] Step 74/1212: training loss=2.03\n",
      "[1768854167] Step 75/1212: training loss=0.46\n",
      "[1768854167] Step 76/1212: training loss=0.39\n",
      "[1768854167] Step 77/1212: training loss=0.96\n",
      "[1768854167] Step 78/1212: training loss=0.48\n",
      "[1768854171] Step 79/1212: training loss=1.96\n",
      "[1768854172] Step 80/1212: training loss=0.88\n",
      "[1768854177] Step 81/1212: training loss=0.86\n",
      "[1768854178] Step 82/1212: training loss=1.87\n",
      "[1768854178] Step 83/1212: training loss=2.64\n",
      "[1768854178] Step 84/1212: training loss=0.49\n",
      "[1768854178] Step 85/1212: training loss=0.51\n",
      "[1768854184] Step 86/1212: training loss=0.78\n",
      "[1768854184] Step 87/1212: training loss=0.65\n",
      "[1768854184] Step 88/1212: training loss=0.31\n",
      "[1768854184] Step 89/1212: training loss=2.65\n",
      "[1768854184] Step 90/1212: training loss=0.54\n",
      "[1768854185] Step 91/1212: training loss=0.88\n",
      "[1768854190] Step 92/1212: training loss=0.76\n",
      "[1768854195] Step 93/1212: training loss=0.52\n",
      "[1768854195] Step 94/1212: training loss=0.75\n",
      "[1768854199] Step 95/1212: training loss=0.64\n",
      "[1768854201] Step 96/1212: training loss=0.84\n",
      "[1768854201] Step 97/1212: training loss=1.60\n",
      "[1768854201] Step 98/1212: training loss=1.59\n",
      "[1768854205] Step 99/1212: training loss=0.65\n",
      "[1768854205] Step 100/1212: training loss=0.68\n",
      "[1768854211] Step 101/1212: training loss=0.67\n",
      "[1768854211] Step 102/1212: training loss=0.58\n",
      "[1768854217] Step 103/1212: training loss=0.86\n",
      "[1768854218] Step 104/1212: training loss=0.54\n",
      "[1768854222] Step 105/1212: training loss=0.64\n",
      "[1768854222] Step 106/1212: training loss=0.63\n",
      "[1768854222] Step 107/1212: training loss=0.57\n",
      "[1768854227] Step 108/1212: training loss=0.71\n",
      "[1768854227] Step 109/1212: training loss=0.66\n",
      "[1768854227] Step 110/1212: training loss=0.58\n",
      "[1768854233] Step 111/1212: training loss=0.44\n",
      "[1768854233] Step 112/1212: training loss=0.67\n",
      "[1768854233] Step 113/1212: training loss=1.60\n",
      "[1768854233] Step 114/1212: training loss=0.68\n",
      "[1768854233] Step 115/1212: training loss=2.09\n",
      "[1768854238] Step 116/1212: training loss=0.68\n",
      "[1768854238] Step 117/1212: training loss=0.67\n",
      "[1768854243] Step 118/1212: training loss=0.52\n",
      "[1768854243] Step 119/1212: training loss=0.53\n",
      "[1768854249] Step 120/1212: training loss=0.50\n",
      "[1768854250] Step 121/1212: training loss=0.80\n",
      "[1768854250] Step 122/1212: training loss=0.79\n",
      "[1768854250] Step 123/1212: training loss=0.55\n",
      "[1768854255] Step 124/1212: training loss=2.50\n",
      "[1768854255] Step 125/1212: training loss=1.47\n",
      "[1768854255] Step 126/1212: training loss=0.71\n",
      "[1768854255] Step 127/1212: training loss=0.55\n",
      "[1768854260] Step 128/1212: training loss=0.73\n",
      "[1768854260] Step 129/1212: training loss=0.58\n",
      "[1768854260] Step 130/1212: training loss=0.53\n",
      "[1768854265] Step 131/1212: training loss=0.82\n",
      "[1768854270] Step 132/1212: training loss=0.54\n",
      "[1768854270] Step 133/1212: training loss=1.72\n",
      "[1768854275] Step 134/1212: training loss=0.78\n",
      "[1768854276] Step 135/1212: training loss=1.53\n",
      "[1768854281] Step 136/1212: training loss=0.57\n",
      "[1768854282] Step 137/1212: training loss=0.62\n",
      "[1768854282] Step 138/1212: training loss=0.62\n",
      "[1768854282] Step 139/1212: training loss=2.01\n",
      "[1768854282] Step 140/1212: training loss=0.69\n",
      "[1768854283] Step 141/1212: training loss=0.41\n",
      "[1768854283] Step 142/1212: training loss=0.69\n",
      "[1768854287] Step 143/1212: training loss=0.90\n",
      "[1768854288] Step 144/1212: training loss=0.53\n",
      "[1768854288] Step 145/1212: training loss=0.67\n",
      "[1768854288] Step 146/1212: training loss=0.72\n",
      "[1768854288] Step 147/1212: training loss=0.60\n",
      "[1768854288] Step 148/1212: training loss=0.39\n",
      "[1768854293] Step 149/1212: training loss=0.69\n",
      "[1768854293] Step 150/1212: training loss=0.79\n",
      "[1768854293] Step 151/1212: training loss=0.51\n",
      "[1768854294] Step 152/1212: training loss=0.62\n",
      "[1768854294] Step 153/1212: training loss=0.74\n",
      "[1768854299] Step 154/1212: training loss=0.66\n",
      "[1768854299] Step 155/1212: training loss=0.55\n",
      "[1768854299] Step 156/1212: training loss=0.55\n",
      "[1768854299] Step 157/1212: training loss=0.50\n",
      "[1768854303] Step 158/1212: training loss=2.04\n",
      "[1768854304] Step 159/1212: training loss=0.69\n",
      "[1768854304] Step 160/1212: training loss=0.56\n",
      "[1768854305] Step 161/1212: training loss=0.60\n",
      "[1768854309] Step 162/1212: training loss=2.68\n",
      "[1768854310] Step 163/1212: training loss=0.72\n",
      "[1768854310] Step 164/1212: training loss=0.51\n",
      "[1768854310] Step 165/1212: training loss=0.73\n",
      "[1768854310] Step 166/1212: training loss=0.75\n",
      "[1768854310] Step 167/1212: training loss=0.62\n",
      "[1768854316] Step 168/1212: training loss=0.89\n",
      "[1768854316] Step 169/1212: training loss=0.44\n",
      "[1768854316] Step 170/1212: training loss=0.45\n",
      "[1768854317] Step 171/1212: training loss=0.71\n",
      "[1768854317] Step 172/1212: training loss=0.42\n",
      "[1768854322] Step 173/1212: training loss=0.95\n",
      "[1768854322] Step 174/1212: training loss=2.18\n",
      "[1768854322] Step 175/1212: training loss=0.47\n",
      "[1768854322] Step 176/1212: training loss=0.66\n",
      "[1768854322] Step 177/1212: training loss=0.49\n",
      "[1768854327] Step 178/1212: training loss=0.87\n",
      "[1768854327] Step 179/1212: training loss=1.89\n",
      "[1768854327] Step 180/1212: training loss=2.28\n",
      "[1768854328] Step 181/1212: training loss=0.80\n",
      "[1768854328] Step 182/1212: training loss=0.72\n",
      "[1768854328] Step 183/1212: training loss=0.73\n",
      "[1768854333] Step 184/1212: training loss=0.48\n",
      "[1768854333] Step 185/1212: training loss=0.54\n",
      "[1768854333] Step 186/1212: training loss=2.35\n",
      "[1768854333] Step 187/1212: training loss=0.92\n",
      "[1768854333] Step 188/1212: training loss=0.62\n",
      "[1768854333] Step 189/1212: training loss=2.66\n",
      "[1768854338] Step 190/1212: training loss=0.49\n",
      "[1768854339] Step 191/1212: training loss=0.71\n",
      "[1768854339] Step 192/1212: training loss=0.42\n",
      "[1768854344] Step 193/1212: training loss=0.38\n",
      "[1768854344] Step 194/1212: training loss=0.79\n",
      "[1768854344] Step 195/1212: training loss=1.09\n",
      "[1768854350] Step 196/1212: training loss=0.64\n",
      "[1768854351] Step 197/1212: training loss=0.44\n",
      "[1768854351] Step 198/1212: training loss=0.76\n",
      "[1768854351] Step 199/1212: training loss=0.55\n",
      "[1768854355] Step 200/1212: training loss=0.41\n",
      "[1768854356] Step 201/1212: training loss=0.56\n",
      "[1768854357] Step 202/1212: training loss=0.64\n",
      "[1768854357] Step 203/1212: training loss=0.68\n",
      "[1768854361] Step 204/1212: training loss=0.72\n",
      "[1768854361] Step 205/1212: training loss=0.89\n",
      "[1768854361] Step 206/1212: training loss=0.61\n",
      "[1768854361] Step 207/1212: training loss=0.67\n",
      "[1768854361] Step 208/1212: training loss=0.57\n",
      "[1768854361] Step 209/1212: training loss=0.43\n",
      "[1768854366] Step 210/1212: training loss=0.55\n",
      "[1768854367] Step 211/1212: training loss=0.79\n",
      "[1768854372] Step 212/1212: training loss=0.59\n",
      "[1768854373] Step 213/1212: training loss=0.93\n",
      "[1768854373] Step 214/1212: training loss=0.67\n",
      "[1768854373] Step 215/1212: training loss=0.40\n",
      "[1768854373] Step 216/1212: training loss=1.68\n",
      "[1768854373] Step 217/1212: training loss=0.67\n",
      "[1768854377] Step 218/1212: training loss=0.59\n",
      "[1768854377] Step 219/1212: training loss=0.44\n",
      "[1768854377] Step 220/1212: training loss=0.48\n",
      "[1768854378] Step 221/1212: training loss=0.62\n",
      "[1768854384] Step 222/1212: training loss=0.56\n",
      "[1768854385] Step 223/1212: training loss=0.43\n",
      "[1768854385] Step 224/1212: training loss=0.89\n",
      "[1768854385] Step 225/1212: training loss=0.62\n",
      "[1768854385] Step 226/1212: training loss=0.68\n",
      "[1768854385] Step 227/1212: training loss=1.62\n",
      "[1768854390] Step 228/1212: training loss=0.50\n",
      "[1768854390] Step 229/1212: training loss=0.77\n",
      "[1768854390] Step 230/1212: training loss=3.25\n",
      "[1768854390] Step 231/1212: training loss=0.60\n",
      "[1768854396] Step 232/1212: training loss=0.59\n",
      "[1768854396] Step 233/1212: training loss=0.72\n",
      "[1768854396] Step 234/1212: training loss=0.59\n",
      "[1768854396] Step 235/1212: training loss=0.59\n",
      "[1768854401] Step 236/1212: training loss=0.69\n",
      "[1768854401] Step 237/1212: training loss=0.51\n",
      "[1768854401] Step 238/1212: training loss=0.67\n",
      "[1768854401] Step 239/1212: training loss=0.47\n",
      "[1768854406] Step 240/1212: training loss=0.57\n",
      "[1768854406] Step 241/1212: training loss=0.36\n",
      "[1768854407] Step 242/1212: training loss=1.30\n",
      "[1768854407] Step 243/1212: training loss=2.26\n",
      "[1768854407] Step 244/1212: training loss=0.66\n",
      "[1768854412] Step 245/1212: training loss=0.54\n",
      "[1768854412] Step 246/1212: training loss=0.70\n",
      "[1768854412] Step 247/1212: training loss=0.93\n",
      "[1768854412] Step 248/1212: training loss=2.67\n",
      "[1768854418] Step 249/1212: training loss=0.60\n",
      "[1768854418] Step 250/1212: training loss=0.98\n",
      "[1768854419] Step 251/1212: training loss=0.74\n",
      "[1768854419] Step 252/1212: training loss=0.62\n",
      "[1768854419] Step 253/1212: training loss=0.44\n",
      "[1768854424] Step 254/1212: training loss=0.57\n",
      "[1768854424] Step 255/1212: training loss=0.61\n",
      "[1768854424] Step 256/1212: training loss=0.74\n",
      "[1768854424] Step 257/1212: training loss=0.47\n",
      "[1768854424] Step 258/1212: training loss=0.62\n",
      "[1768854429] Step 259/1212: training loss=0.51\n",
      "[1768854429] Step 260/1212: training loss=0.48\n",
      "[1768854430] Step 261/1212: training loss=0.79\n",
      "[1768854430] Step 262/1212: training loss=0.69\n",
      "[1768854435] Step 263/1212: training loss=0.56\n",
      "[1768854435] Step 264/1212: training loss=2.12\n",
      "[1768854435] Step 265/1212: training loss=0.42\n",
      "[1768854435] Step 266/1212: training loss=0.75\n",
      "[1768854435] Step 267/1212: training loss=0.24\n",
      "[1768854440] Step 268/1212: training loss=0.39\n",
      "[1768854440] Step 269/1212: training loss=0.58\n",
      "[1768854440] Step 270/1212: training loss=0.65\n",
      "[1768854441] Step 271/1212: training loss=0.90\n",
      "[1768854441] Step 272/1212: training loss=0.42\n",
      "[1768854446] Step 273/1212: training loss=0.77\n",
      "[1768854446] Step 274/1212: training loss=0.80\n",
      "[1768854446] Step 275/1212: training loss=0.45\n",
      "[1768854446] Step 276/1212: training loss=1.71\n",
      "[1768854446] Step 277/1212: training loss=0.71\n",
      "[1768854446] Step 278/1212: training loss=0.65\n",
      "[1768854452] Step 279/1212: training loss=0.62\n",
      "[1768854453] Step 280/1212: training loss=0.51\n",
      "[1768854453] Step 281/1212: training loss=0.78\n",
      "[1768854453] Step 282/1212: training loss=0.93\n",
      "[1768854453] Step 283/1212: training loss=0.64\n",
      "[1768854458] Step 284/1212: training loss=2.22\n",
      "[1768854459] Step 285/1212: training loss=0.74\n",
      "[1768854459] Step 286/1212: training loss=1.40\n",
      "[1768854459] Step 287/1212: training loss=0.45\n",
      "[1768854459] Step 288/1212: training loss=0.65\n",
      "[1768854463] Step 289/1212: training loss=0.63\n",
      "[1768854463] Step 290/1212: training loss=0.48\n",
      "[1768854464] Step 291/1212: training loss=0.49\n",
      "[1768854465] Step 292/1212: training loss=0.73\n",
      "[1768854465] Step 293/1212: training loss=0.59\n",
      "[1768854470] Step 294/1212: training loss=0.37\n",
      "[1768854470] Step 295/1212: training loss=0.60\n",
      "[1768854470] Step 296/1212: training loss=0.35\n",
      "[1768854470] Step 297/1212: training loss=1.44\n",
      "[1768854474] Step 298/1212: training loss=0.43\n",
      "[1768854475] Step 299/1212: training loss=1.57\n",
      "[1768854475] Step 300/1212: training loss=0.66\n",
      "[1768854475] Step 301/1212: training loss=0.42\n",
      "[1768854475] Step 302/1212: training loss=0.63\n",
      "[1768854480] Step 303/1212: training loss=0.71\n",
      "[1768854481] Step 304/1212: training loss=0.70\n",
      "[1768854481] Step 305/1212: training loss=0.38\n",
      "[1768854481] Step 306/1212: training loss=0.85\n",
      "[1768854487] Step 307/1212: training loss=0.70\n",
      "[1768854487] Step 308/1212: training loss=0.74\n",
      "[1768854487] Step 309/1212: training loss=0.51\n",
      "[1768854487] Step 310/1212: training loss=0.68\n",
      "[1768854493] Step 311/1212: training loss=0.76\n",
      "[1768854493] Step 312/1212: training loss=0.56\n",
      "[1768854493] Step 313/1212: training loss=0.48\n",
      "[1768854493] Step 314/1212: training loss=0.53\n",
      "[1768854493] Step 315/1212: training loss=0.52\n",
      "[1768854493] Step 316/1212: training loss=0.72\n",
      "[1768854493] Step 317/1212: training loss=0.57\n",
      "[1768854498] Step 318/1212: training loss=0.56\n",
      "[1768854498] Step 319/1212: training loss=0.81\n",
      "[1768854498] Step 320/1212: training loss=0.42\n",
      "[1768854503] Step 321/1212: training loss=2.30\n",
      "[1768854504] Step 322/1212: training loss=0.62\n",
      "[1768854504] Step 323/1212: training loss=0.91\n",
      "[1768854504] Step 324/1212: training loss=0.83\n",
      "[1768854504] Step 325/1212: training loss=1.76\n",
      "[1768854509] Step 326/1212: training loss=1.65\n",
      "[1768854509] Step 327/1212: training loss=0.72\n",
      "[1768854509] Step 328/1212: training loss=0.66\n",
      "[1768854509] Step 329/1212: training loss=0.70\n",
      "[1768854509] Step 330/1212: training loss=0.50\n",
      "[1768854514] Step 331/1212: training loss=0.57\n",
      "[1768854515] Step 332/1212: training loss=0.29\n",
      "[1768854515] Step 333/1212: training loss=0.71\n",
      "[1768854515] Step 334/1212: training loss=0.65\n",
      "[1768854515] Step 335/1212: training loss=0.57\n",
      "[1768854520] Step 336/1212: training loss=0.70\n",
      "[1768854521] Step 337/1212: training loss=0.53\n",
      "[1768854521] Step 338/1212: training loss=0.44\n",
      "[1768854521] Step 339/1212: training loss=0.70\n",
      "[1768854526] Step 340/1212: training loss=0.52\n",
      "[1768854526] Step 341/1212: training loss=0.36\n",
      "[1768854527] Step 342/1212: training loss=0.64\n",
      "[1768854527] Step 343/1212: training loss=0.73\n",
      "[1768854527] Step 344/1212: training loss=0.64\n",
      "[1768854532] Step 345/1212: training loss=0.57\n",
      "[1768854532] Step 346/1212: training loss=0.63\n",
      "[1768854532] Step 347/1212: training loss=0.77\n",
      "[1768854532] Step 348/1212: training loss=1.83\n",
      "[1768854532] Step 349/1212: training loss=0.68\n",
      "[1768854532] Step 350/1212: training loss=1.50\n",
      "[1768854537] Step 351/1212: training loss=0.53\n",
      "[1768854538] Step 352/1212: training loss=0.83\n",
      "[1768854538] Step 353/1212: training loss=0.78\n",
      "[1768854538] Step 354/1212: training loss=2.81\n",
      "[1768854538] Step 355/1212: training loss=0.34\n",
      "[1768854542] Step 356/1212: training loss=0.57\n",
      "[1768854542] Step 357/1212: training loss=0.62\n",
      "[1768854542] Step 358/1212: training loss=0.53\n",
      "[1768854542] Step 359/1212: training loss=0.54\n",
      "[1768854542] Step 360/1212: training loss=0.54\n",
      "[1768854543] Step 361/1212: training loss=0.47\n",
      "[1768854548] Step 362/1212: training loss=0.61\n",
      "[1768854548] Step 363/1212: training loss=0.73\n",
      "[1768854548] Step 364/1212: training loss=0.40\n",
      "[1768854548] Step 365/1212: training loss=0.62\n",
      "[1768854548] Step 366/1212: training loss=0.73\n",
      "[1768854554] Step 367/1212: training loss=0.45\n",
      "[1768854555] Step 368/1212: training loss=0.57\n",
      "[1768854555] Step 369/1212: training loss=0.40\n",
      "[1768854555] Step 370/1212: training loss=2.22\n",
      "[1768854560] Step 371/1212: training loss=0.49\n",
      "[1768854561] Step 372/1212: training loss=0.62\n",
      "[1768854561] Step 373/1212: training loss=0.72\n",
      "[1768854561] Step 374/1212: training loss=0.31\n",
      "[1768854565] Step 375/1212: training loss=0.67\n",
      "[1768854565] Step 376/1212: training loss=1.66\n",
      "[1768854565] Step 377/1212: training loss=1.62\n",
      "[1768854565] Step 378/1212: training loss=0.76\n",
      "[1768854565] Step 379/1212: training loss=0.67\n",
      "[1768854570] Step 380/1212: training loss=0.53\n",
      "[1768854571] Step 381/1212: training loss=1.00\n",
      "[1768854571] Step 382/1212: training loss=0.48\n",
      "[1768854571] Step 383/1212: training loss=0.71\n",
      "[1768854576] Step 384/1212: training loss=0.42\n",
      "[1768854577] Step 385/1212: training loss=0.95\n",
      "[1768854577] Step 386/1212: training loss=0.78\n",
      "[1768854577] Step 387/1212: training loss=0.63\n",
      "[1768854577] Step 388/1212: training loss=2.47\n",
      "[1768854577] Step 389/1212: training loss=0.59\n",
      "[1768854581] Step 390/1212: training loss=0.46\n",
      "[1768854582] Step 391/1212: training loss=0.73\n",
      "[1768854583] Step 392/1212: training loss=0.66\n",
      "[1768854583] Step 393/1212: training loss=0.65\n",
      "[1768854583] Step 394/1212: training loss=0.51\n",
      "[1768854588] Step 395/1212: training loss=0.77\n",
      "[1768854589] Step 396/1212: training loss=0.58\n",
      "[1768854589] Step 397/1212: training loss=1.81\n",
      "[1768854589] Step 398/1212: training loss=0.37\n",
      "[1768854594] Step 399/1212: training loss=0.45\n",
      "[1768854594] Step 400/1212: training loss=0.51\n",
      "[1768854594] Step 401/1212: training loss=0.47\n",
      "[1768854600] Step 402/1212: training loss=0.36\n",
      "[1768854600] Step 403/1212: training loss=0.54\n",
      "[1768854600] Step 404/1212: training loss=0.69\n",
      "[1768854606] Step 405/1212: training loss=0.28\n",
      "[1768854606] Step 406/1212: training loss=0.37\n",
      "[1768854611] Step 407/1212: training loss=0.52\n",
      "[1768854611] Step 408/1212: training loss=0.36\n",
      "[1768854611] Step 409/1212: training loss=0.33\n",
      "[1768854611] Step 410/1212: training loss=0.42\n",
      "[1768854612] Step 411/1212: training loss=0.47\n",
      "[1768854612] Step 412/1212: training loss=1.44\n",
      "[1768854612] Step 413/1212: training loss=0.56\n",
      "[1768854617] Step 414/1212: training loss=0.24\n",
      "[1768854617] Step 415/1212: training loss=0.23\n",
      "[1768854617] Step 416/1212: training loss=0.40\n",
      "[1768854617] Step 417/1212: training loss=0.19\n",
      "[1768854617] Step 418/1212: training loss=0.35\n",
      "[1768854617] Step 419/1212: training loss=0.43\n",
      "[1768854617] Step 420/1212: training loss=0.24\n",
      "[1768854618] Step 421/1212: training loss=0.54\n",
      "[1768854624] Step 422/1212: training loss=0.36\n",
      "[1768854625] Step 423/1212: training loss=0.35\n",
      "[1768854625] Step 424/1212: training loss=0.38\n",
      "[1768854625] Step 425/1212: training loss=0.72\n",
      "[1768854625] Step 426/1212: training loss=0.46\n",
      "[1768854625] Step 427/1212: training loss=0.53\n",
      "[1768854625] Step 428/1212: training loss=0.29\n",
      "[1768854625] Step 429/1212: training loss=0.25\n",
      "[1768854629] Step 430/1212: training loss=0.45\n",
      "[1768854630] Step 431/1212: training loss=0.22\n",
      "[1768854631] Step 432/1212: training loss=0.32\n",
      "[1768854631] Step 433/1212: training loss=0.36\n",
      "[1768854631] Step 434/1212: training loss=0.47\n",
      "[1768854631] Step 435/1212: training loss=0.27\n",
      "[1768854635] Step 436/1212: training loss=0.22\n",
      "[1768854635] Step 437/1212: training loss=0.48\n",
      "[1768854635] Step 438/1212: training loss=1.71\n",
      "[1768854635] Step 439/1212: training loss=0.32\n",
      "[1768854635] Step 440/1212: training loss=0.38\n",
      "[1768854641] Step 441/1212: training loss=0.34\n",
      "[1768854641] Step 442/1212: training loss=0.20\n",
      "[1768854641] Step 443/1212: training loss=0.57\n",
      "[1768854641] Step 444/1212: training loss=0.14\n",
      "[1768854641] Step 445/1212: training loss=0.27\n",
      "[1768854641] Step 446/1212: training loss=2.72\n",
      "[1768854646] Step 447/1212: training loss=0.39\n",
      "[1768854646] Step 448/1212: training loss=0.52\n",
      "[1768854646] Step 449/1212: training loss=0.32\n",
      "[1768854646] Step 450/1212: training loss=0.16\n",
      "[1768854647] Step 451/1212: training loss=0.41\n",
      "[1768854647] Step 452/1212: training loss=0.69\n",
      "[1768854652] Step 453/1212: training loss=0.11\n",
      "[1768854652] Step 454/1212: training loss=0.56\n",
      "[1768854652] Step 455/1212: training loss=0.30\n",
      "[1768854652] Step 456/1212: training loss=0.47\n",
      "[1768854652] Step 457/1212: training loss=0.33\n",
      "[1768854652] Step 458/1212: training loss=0.43\n",
      "[1768854658] Step 459/1212: training loss=0.24\n",
      "[1768854658] Step 460/1212: training loss=0.19\n",
      "[1768854659] Step 461/1212: training loss=0.65\n",
      "[1768854659] Step 462/1212: training loss=0.29\n",
      "[1768854659] Step 463/1212: training loss=0.44\n",
      "[1768854659] Step 464/1212: training loss=1.97\n",
      "[1768854659] Step 465/1212: training loss=0.52\n",
      "[1768854664] Step 466/1212: training loss=0.22\n",
      "[1768854664] Step 467/1212: training loss=0.46\n",
      "[1768854664] Step 468/1212: training loss=0.48\n",
      "[1768854664] Step 469/1212: training loss=0.16\n",
      "[1768854664] Step 470/1212: training loss=0.31\n",
      "[1768854665] Step 471/1212: training loss=0.46\n",
      "[1768854665] Step 472/1212: training loss=0.38\n",
      "[1768854670] Step 473/1212: training loss=0.25\n",
      "[1768854670] Step 474/1212: training loss=0.25\n",
      "[1768854670] Step 475/1212: training loss=1.79\n",
      "[1768854670] Step 476/1212: training loss=0.06\n",
      "[1768854670] Step 477/1212: training loss=1.27\n",
      "[1768854670] Step 478/1212: training loss=0.28\n",
      "[1768854670] Step 479/1212: training loss=0.78\n",
      "[1768854670] Step 480/1212: training loss=0.36\n",
      "[1768854676] Step 481/1212: training loss=0.34\n",
      "[1768854676] Step 482/1212: training loss=1.31\n",
      "[1768854676] Step 483/1212: training loss=0.26\n",
      "[1768854676] Step 484/1212: training loss=0.71\n",
      "[1768854676] Step 485/1212: training loss=1.29\n",
      "[1768854676] Step 486/1212: training loss=0.46\n",
      "[1768854676] Step 487/1212: training loss=0.23\n",
      "[1768854681] Step 488/1212: training loss=0.19\n",
      "[1768854681] Step 489/1212: training loss=0.28\n",
      "[1768854681] Step 490/1212: training loss=0.21\n",
      "[1768854682] Step 491/1212: training loss=0.29\n",
      "[1768854682] Step 492/1212: training loss=0.18\n",
      "[1768854688] Step 493/1212: training loss=2.34\n",
      "[1768854689] Step 494/1212: training loss=2.07\n",
      "[1768854689] Step 495/1212: training loss=0.21\n",
      "[1768854689] Step 496/1212: training loss=0.33\n",
      "[1768854689] Step 497/1212: training loss=0.26\n",
      "[1768854694] Step 498/1212: training loss=1.62\n",
      "[1768854694] Step 499/1212: training loss=0.40\n",
      "[1768854694] Step 500/1212: training loss=0.27\n",
      "[1768854695] Step 501/1212: training loss=0.29\n",
      "[1768854695] Step 502/1212: training loss=0.25\n",
      "[1768854695] Step 503/1212: training loss=0.24\n",
      "[1768854695] Step 504/1212: training loss=1.71\n",
      "[1768854700] Step 505/1212: training loss=0.32\n",
      "[1768854700] Step 506/1212: training loss=0.40\n",
      "[1768854700] Step 507/1212: training loss=0.16\n",
      "[1768854700] Step 508/1212: training loss=0.35\n",
      "[1768854700] Step 509/1212: training loss=0.52\n",
      "[1768854705] Step 510/1212: training loss=0.31\n",
      "[1768854705] Step 511/1212: training loss=0.19\n",
      "[1768854706] Step 512/1212: training loss=0.35\n",
      "[1768854706] Step 513/1212: training loss=0.38\n",
      "[1768854706] Step 514/1212: training loss=0.41\n",
      "[1768854706] Step 515/1212: training loss=0.51\n",
      "[1768854711] Step 516/1212: training loss=0.43\n",
      "[1768854711] Step 517/1212: training loss=0.43\n",
      "[1768854711] Step 518/1212: training loss=0.32\n",
      "[1768854711] Step 519/1212: training loss=0.23\n",
      "[1768854711] Step 520/1212: training loss=0.19\n",
      "[1768854711] Step 521/1212: training loss=0.56\n",
      "[1768854712] Step 522/1212: training loss=0.28\n",
      "[1768854716] Step 523/1212: training loss=0.34\n",
      "[1768854716] Step 524/1212: training loss=0.34\n",
      "[1768854716] Step 525/1212: training loss=0.32\n",
      "[1768854716] Step 526/1212: training loss=0.15\n",
      "[1768854716] Step 527/1212: training loss=0.62\n",
      "[1768854716] Step 528/1212: training loss=0.31\n",
      "[1768854722] Step 529/1212: training loss=0.44\n",
      "[1768854723] Step 530/1212: training loss=1.58\n",
      "[1768854723] Step 531/1212: training loss=0.35\n",
      "[1768854723] Step 532/1212: training loss=0.26\n",
      "[1768854723] Step 533/1212: training loss=0.42\n",
      "[1768854723] Step 534/1212: training loss=0.18\n",
      "[1768854728] Step 535/1212: training loss=1.85\n",
      "[1768854728] Step 536/1212: training loss=0.58\n",
      "[1768854728] Step 537/1212: training loss=0.59\n",
      "[1768854728] Step 538/1212: training loss=0.27\n",
      "[1768854728] Step 539/1212: training loss=0.37\n",
      "[1768854728] Step 540/1212: training loss=0.40\n",
      "[1768854734] Step 541/1212: training loss=0.36\n",
      "[1768854734] Step 542/1212: training loss=0.47\n",
      "[1768854734] Step 543/1212: training loss=0.37\n",
      "[1768854734] Step 544/1212: training loss=0.56\n",
      "[1768854734] Step 545/1212: training loss=0.42\n",
      "[1768854739] Step 546/1212: training loss=0.78\n",
      "[1768854739] Step 547/1212: training loss=0.24\n",
      "[1768854739] Step 548/1212: training loss=0.25\n",
      "[1768854739] Step 549/1212: training loss=0.24\n",
      "[1768854739] Step 550/1212: training loss=0.38\n",
      "[1768854744] Step 551/1212: training loss=0.17\n",
      "[1768854745] Step 552/1212: training loss=0.30\n",
      "[1768854745] Step 553/1212: training loss=0.14\n",
      "[1768854745] Step 554/1212: training loss=0.26\n",
      "[1768854745] Step 555/1212: training loss=1.45\n",
      "[1768854745] Step 556/1212: training loss=0.16\n",
      "[1768854749] Step 557/1212: training loss=1.50\n",
      "[1768854749] Step 558/1212: training loss=2.53\n",
      "[1768854749] Step 559/1212: training loss=0.34\n",
      "[1768854749] Step 560/1212: training loss=0.57\n",
      "[1768854750] Step 561/1212: training loss=0.23\n",
      "[1768854756] Step 562/1212: training loss=0.31\n",
      "[1768854757] Step 563/1212: training loss=0.36\n",
      "[1768854757] Step 564/1212: training loss=1.47\n",
      "[1768854757] Step 565/1212: training loss=0.44\n",
      "[1768854757] Step 566/1212: training loss=0.31\n",
      "[1768854757] Step 567/1212: training loss=0.44\n",
      "[1768854757] Step 568/1212: training loss=0.43\n",
      "[1768854761] Step 569/1212: training loss=0.47\n",
      "[1768854761] Step 570/1212: training loss=0.26\n",
      "[1768854762] Step 571/1212: training loss=0.32\n",
      "[1768854763] Step 572/1212: training loss=0.20\n",
      "[1768854763] Step 573/1212: training loss=0.49\n",
      "[1768854763] Step 574/1212: training loss=0.47\n",
      "[1768854767] Step 575/1212: training loss=1.46\n",
      "[1768854767] Step 576/1212: training loss=0.30\n",
      "[1768854767] Step 577/1212: training loss=0.51\n",
      "[1768854767] Step 578/1212: training loss=0.51\n",
      "[1768854767] Step 579/1212: training loss=1.12\n",
      "[1768854767] Step 580/1212: training loss=0.70\n",
      "[1768854772] Step 581/1212: training loss=0.41\n",
      "[1768854773] Step 582/1212: training loss=0.31\n",
      "[1768854773] Step 583/1212: training loss=0.33\n",
      "[1768854773] Step 584/1212: training loss=0.36\n",
      "[1768854773] Step 585/1212: training loss=0.30\n",
      "[1768854773] Step 586/1212: training loss=0.54\n",
      "[1768854773] Step 587/1212: training loss=0.24\n",
      "[1768854778] Step 588/1212: training loss=0.24\n",
      "[1768854778] Step 589/1212: training loss=2.04\n",
      "[1768854778] Step 590/1212: training loss=0.38\n",
      "[1768854778] Step 591/1212: training loss=0.44\n",
      "[1768854779] Step 592/1212: training loss=0.35\n",
      "[1768854783] Step 593/1212: training loss=0.29\n",
      "[1768854783] Step 594/1212: training loss=0.30\n",
      "[1768854783] Step 595/1212: training loss=0.33\n",
      "[1768854783] Step 596/1212: training loss=1.53\n",
      "[1768854783] Step 597/1212: training loss=0.61\n",
      "[1768854783] Step 598/1212: training loss=0.27\n",
      "[1768854789] Step 599/1212: training loss=0.57\n",
      "[1768854790] Step 600/1212: training loss=0.43\n",
      "[1768854790] Step 601/1212: training loss=0.20\n",
      "[1768854790] Step 602/1212: training loss=0.15\n",
      "[1768854790] Step 603/1212: training loss=0.26\n",
      "[1768854790] Step 604/1212: training loss=0.42\n",
      "[1768854790] Step 605/1212: training loss=0.59\n",
      "[1768854795] Step 606/1212: training loss=0.35\n",
      "[1768854795] Step 607/1212: training loss=0.66\n",
      "[1768854795] Step 608/1212: training loss=1.33\n",
      "[1768854795] Step 609/1212: training loss=0.68\n",
      "[1768854795] Step 610/1212: training loss=0.20\n",
      "[1768854801] Step 611/1212: training loss=0.52\n",
      "[1768854802] Step 612/1212: training loss=0.25\n",
      "[1768854802] Step 613/1212: training loss=0.18\n",
      "[1768854802] Step 614/1212: training loss=0.70\n",
      "[1768854802] Step 615/1212: training loss=0.18\n",
      "[1768854807] Step 616/1212: training loss=0.36\n",
      "[1768854807] Step 617/1212: training loss=0.33\n",
      "[1768854807] Step 618/1212: training loss=0.39\n",
      "[1768854807] Step 619/1212: training loss=0.17\n",
      "[1768854807] Step 620/1212: training loss=0.22\n",
      "[1768854812] Step 621/1212: training loss=0.49\n",
      "[1768854812] Step 622/1212: training loss=0.52\n",
      "[1768854812] Step 623/1212: training loss=0.36\n",
      "[1768854812] Step 624/1212: training loss=0.22\n",
      "[1768854812] Step 625/1212: training loss=0.27\n",
      "[1768854812] Step 626/1212: training loss=0.37\n",
      "[1768854817] Step 627/1212: training loss=0.33\n",
      "[1768854817] Step 628/1212: training loss=0.40\n",
      "[1768854817] Step 629/1212: training loss=0.39\n",
      "[1768854817] Step 630/1212: training loss=0.23\n",
      "[1768854818] Step 631/1212: training loss=0.29\n",
      "[1768854824] Step 632/1212: training loss=1.77\n",
      "[1768854825] Step 633/1212: training loss=1.76\n",
      "[1768854825] Step 634/1212: training loss=0.63\n",
      "[1768854825] Step 635/1212: training loss=0.51\n",
      "[1768854825] Step 636/1212: training loss=0.62\n",
      "[1768854825] Step 637/1212: training loss=0.43\n",
      "[1768854825] Step 638/1212: training loss=0.91\n",
      "[1768854829] Step 639/1212: training loss=0.39\n",
      "[1768854829] Step 640/1212: training loss=0.27\n",
      "[1768854830] Step 641/1212: training loss=0.22\n",
      "[1768854830] Step 642/1212: training loss=0.46\n",
      "[1768854830] Step 643/1212: training loss=0.17\n",
      "[1768854830] Step 644/1212: training loss=0.49\n",
      "[1768854835] Step 645/1212: training loss=0.44\n",
      "[1768854835] Step 646/1212: training loss=0.29\n",
      "[1768854835] Step 647/1212: training loss=2.29\n",
      "[1768854835] Step 648/1212: training loss=0.20\n",
      "[1768854835] Step 649/1212: training loss=1.34\n",
      "[1768854835] Step 650/1212: training loss=2.16\n",
      "[1768854836] Step 651/1212: training loss=0.24\n",
      "[1768854841] Step 652/1212: training loss=0.44\n",
      "[1768854841] Step 653/1212: training loss=0.19\n",
      "[1768854841] Step 654/1212: training loss=0.47\n",
      "[1768854841] Step 655/1212: training loss=0.41\n",
      "[1768854841] Step 656/1212: training loss=0.30\n",
      "[1768854841] Step 657/1212: training loss=0.34\n",
      "[1768854845] Step 658/1212: training loss=0.28\n",
      "[1768854846] Step 659/1212: training loss=0.30\n",
      "[1768854846] Step 660/1212: training loss=1.66\n",
      "[1768854847] Step 661/1212: training loss=0.30\n",
      "[1768854847] Step 662/1212: training loss=0.49\n",
      "[1768854851] Step 663/1212: training loss=0.50\n",
      "[1768854852] Step 664/1212: training loss=0.41\n",
      "[1768854852] Step 665/1212: training loss=0.77\n",
      "[1768854852] Step 666/1212: training loss=0.23\n",
      "[1768854852] Step 667/1212: training loss=0.27\n",
      "[1768854852] Step 668/1212: training loss=0.34\n",
      "[1768854858] Step 669/1212: training loss=0.38\n",
      "[1768854858] Step 670/1212: training loss=0.21\n",
      "[1768854859] Step 671/1212: training loss=0.26\n",
      "[1768854859] Step 672/1212: training loss=0.22\n",
      "[1768854859] Step 673/1212: training loss=1.32\n",
      "[1768854859] Step 674/1212: training loss=2.11\n",
      "[1768854859] Step 675/1212: training loss=0.31\n",
      "[1768854863] Step 676/1212: training loss=0.34\n",
      "[1768854864] Step 677/1212: training loss=0.59\n",
      "[1768854864] Step 678/1212: training loss=0.68\n",
      "[1768854864] Step 679/1212: training loss=0.45\n",
      "[1768854864] Step 680/1212: training loss=0.24\n",
      "[1768854865] Step 681/1212: training loss=1.00\n",
      "[1768854869] Step 682/1212: training loss=0.39\n",
      "[1768854870] Step 683/1212: training loss=0.27\n",
      "[1768854870] Step 684/1212: training loss=0.32\n",
      "[1768854870] Step 685/1212: training loss=0.32\n",
      "[1768854875] Step 686/1212: training loss=0.24\n",
      "[1768854875] Step 687/1212: training loss=0.32\n",
      "[1768854875] Step 688/1212: training loss=1.22\n",
      "[1768854875] Step 689/1212: training loss=0.46\n",
      "[1768854875] Step 690/1212: training loss=0.35\n",
      "[1768854875] Step 691/1212: training loss=0.37\n",
      "[1768854880] Step 692/1212: training loss=0.51\n",
      "[1768854880] Step 693/1212: training loss=0.30\n",
      "[1768854880] Step 694/1212: training loss=0.36\n",
      "[1768854880] Step 695/1212: training loss=0.27\n",
      "[1768854880] Step 696/1212: training loss=0.49\n",
      "[1768854880] Step 697/1212: training loss=1.78\n",
      "[1768854885] Step 698/1212: training loss=0.33\n",
      "[1768854886] Step 699/1212: training loss=0.13\n",
      "[1768854886] Step 700/1212: training loss=2.24\n",
      "[1768854886] Step 701/1212: training loss=0.52\n",
      "[1768854892] Step 702/1212: training loss=0.35\n",
      "[1768854892] Step 703/1212: training loss=0.30\n",
      "[1768854892] Step 704/1212: training loss=0.31\n",
      "[1768854892] Step 705/1212: training loss=0.18\n",
      "[1768854892] Step 706/1212: training loss=0.37\n",
      "[1768854892] Step 707/1212: training loss=0.59\n",
      "[1768854892] Step 708/1212: training loss=0.48\n",
      "[1768854897] Step 709/1212: training loss=0.40\n",
      "[1768854897] Step 710/1212: training loss=0.17\n",
      "[1768854898] Step 711/1212: training loss=0.20\n",
      "[1768854898] Step 712/1212: training loss=0.52\n",
      "[1768854898] Step 713/1212: training loss=0.26\n",
      "[1768854898] Step 714/1212: training loss=0.29\n",
      "[1768854903] Step 715/1212: training loss=0.28\n",
      "[1768854903] Step 716/1212: training loss=0.45\n",
      "[1768854903] Step 717/1212: training loss=0.30\n",
      "[1768854903] Step 718/1212: training loss=0.51\n",
      "[1768854903] Step 719/1212: training loss=1.73\n",
      "[1768854908] Step 720/1212: training loss=0.19\n",
      "[1768854909] Step 721/1212: training loss=0.22\n",
      "[1768854909] Step 722/1212: training loss=0.24\n",
      "[1768854909] Step 723/1212: training loss=0.38\n",
      "[1768854909] Step 724/1212: training loss=0.22\n",
      "[1768854913] Step 725/1212: training loss=0.28\n",
      "[1768854914] Step 726/1212: training loss=0.29\n",
      "[1768854914] Step 727/1212: training loss=0.38\n",
      "[1768854914] Step 728/1212: training loss=0.27\n",
      "[1768854914] Step 729/1212: training loss=1.31\n",
      "[1768854914] Step 730/1212: training loss=1.84\n",
      "[1768854915] Step 731/1212: training loss=0.46\n",
      "[1768854919] Step 732/1212: training loss=0.28\n",
      "[1768854920] Step 733/1212: training loss=1.28\n",
      "[1768854920] Step 734/1212: training loss=0.37\n",
      "[1768854920] Step 735/1212: training loss=0.17\n",
      "[1768854920] Step 736/1212: training loss=0.50\n",
      "[1768854926] Step 737/1212: training loss=0.35\n",
      "[1768854926] Step 738/1212: training loss=0.31\n",
      "[1768854926] Step 739/1212: training loss=0.38\n",
      "[1768854926] Step 740/1212: training loss=2.36\n",
      "[1768854927] Step 741/1212: training loss=0.58\n",
      "[1768854927] Step 742/1212: training loss=0.56\n",
      "[1768854931] Step 743/1212: training loss=1.35\n",
      "[1768854932] Step 744/1212: training loss=0.41\n",
      "[1768854932] Step 745/1212: training loss=0.35\n",
      "[1768854932] Step 746/1212: training loss=1.68\n",
      "[1768854932] Step 747/1212: training loss=1.23\n",
      "[1768854932] Step 748/1212: training loss=0.38\n",
      "[1768854937] Step 749/1212: training loss=0.32\n",
      "[1768854937] Step 750/1212: training loss=0.14\n",
      "[1768854937] Step 751/1212: training loss=0.32\n",
      "[1768854938] Step 752/1212: training loss=0.29\n",
      "[1768854938] Step 753/1212: training loss=0.33\n",
      "[1768854938] Step 754/1212: training loss=0.26\n",
      "[1768854942] Step 755/1212: training loss=0.21\n",
      "[1768854942] Step 756/1212: training loss=0.39\n",
      "[1768854942] Step 757/1212: training loss=1.22\n",
      "[1768854942] Step 758/1212: training loss=0.08\n",
      "[1768854942] Step 759/1212: training loss=0.30\n",
      "[1768854942] Step 760/1212: training loss=1.16\n",
      "[1768854943] Step 761/1212: training loss=0.28\n",
      "[1768854948] Step 762/1212: training loss=0.47\n",
      "[1768854948] Step 763/1212: training loss=0.41\n",
      "[1768854948] Step 764/1212: training loss=0.58\n",
      "[1768854948] Step 765/1212: training loss=2.10\n",
      "[1768854948] Step 766/1212: training loss=0.53\n",
      "[1768854953] Step 767/1212: training loss=0.30\n",
      "[1768854954] Step 768/1212: training loss=0.20\n",
      "[1768854954] Step 769/1212: training loss=0.38\n",
      "[1768854954] Step 770/1212: training loss=1.22\n",
      "[1768854954] Step 771/1212: training loss=0.32\n",
      "[1768854960] Step 772/1212: training loss=0.27\n",
      "[1768854961] Step 773/1212: training loss=0.44\n",
      "[1768854961] Step 774/1212: training loss=1.77\n",
      "[1768854961] Step 775/1212: training loss=0.16\n",
      "[1768854961] Step 776/1212: training loss=0.41\n",
      "[1768854961] Step 777/1212: training loss=0.47\n",
      "[1768854961] Step 778/1212: training loss=0.64\n",
      "[1768854961] Step 779/1212: training loss=0.73\n",
      "[1768854961] Step 780/1212: training loss=0.22\n",
      "[1768854967] Step 781/1212: training loss=0.23\n",
      "[1768854967] Step 782/1212: training loss=0.27\n",
      "[1768854967] Step 783/1212: training loss=0.29\n",
      "[1768854967] Step 784/1212: training loss=0.51\n",
      "[1768854967] Step 785/1212: training loss=0.50\n",
      "[1768854967] Step 786/1212: training loss=0.23\n",
      "[1768854972] Step 787/1212: training loss=0.27\n",
      "[1768854972] Step 788/1212: training loss=0.26\n",
      "[1768854972] Step 789/1212: training loss=1.14\n",
      "[1768854990] Step 790/1212: training loss=0.43\n",
      "[1768854992] Step 791/1212: training loss=0.15\n",
      "[1768854997] Step 792/1212: training loss=0.31\n",
      "[1768854998] Step 793/1212: training loss=0.28\n",
      "[1768854998] Step 794/1212: training loss=0.58\n",
      "[1768854998] Step 795/1212: training loss=0.23\n",
      "[1768854998] Step 796/1212: training loss=1.18\n",
      "[1768854998] Step 797/1212: training loss=0.36\n",
      "[1768854998] Step 798/1212: training loss=0.24\n",
      "[1768854998] Step 799/1212: training loss=0.31\n",
      "[1768855002] Step 800/1212: training loss=0.44\n",
      "[1768855003] Step 801/1212: training loss=0.32\n",
      "[1768855004] Step 802/1212: training loss=0.26\n",
      "[1768855004] Step 803/1212: training loss=0.41\n",
      "[1768855004] Step 804/1212: training loss=0.15\n",
      "[1768855008] Step 805/1212: training loss=0.46\n",
      "[1768855008] Step 806/1212: training loss=0.38\n",
      "[1768855008] Step 807/1212: training loss=0.41\n",
      "[1768855008] Step 808/1212: training loss=0.38\n",
      "[1768855014] Step 809/1212: training loss=1.37\n",
      "[1768855015] Step 810/1212: training loss=0.15\n",
      "[1768855020] Step 811/1212: training loss=0.04\n",
      "[1768855020] Step 812/1212: training loss=0.52\n",
      "[1768855020] Step 813/1212: training loss=0.11\n",
      "[1768855020] Step 814/1212: training loss=0.22\n",
      "[1768855020] Step 815/1212: training loss=0.07\n",
      "[1768855020] Step 816/1212: training loss=0.08\n",
      "[1768855020] Step 817/1212: training loss=0.25\n",
      "[1768855020] Step 818/1212: training loss=0.24\n",
      "[1768855026] Step 819/1212: training loss=0.39\n",
      "[1768855027] Step 820/1212: training loss=0.32\n",
      "[1768855027] Step 821/1212: training loss=0.08\n",
      "[1768855027] Step 822/1212: training loss=0.16\n",
      "[1768855027] Step 823/1212: training loss=0.15\n",
      "[1768855027] Step 824/1212: training loss=0.13\n",
      "[1768855027] Step 825/1212: training loss=0.16\n",
      "[1768855027] Step 826/1212: training loss=0.06\n",
      "[1768855032] Step 827/1212: training loss=0.24\n",
      "[1768855033] Step 828/1212: training loss=0.37\n",
      "[1768855033] Step 829/1212: training loss=0.24\n",
      "[1768855033] Step 830/1212: training loss=0.25\n",
      "[1768855038] Step 831/1212: training loss=0.12\n",
      "[1768855038] Step 832/1212: training loss=0.26\n",
      "[1768855038] Step 833/1212: training loss=0.12\n",
      "[1768855038] Step 834/1212: training loss=0.38\n",
      "[1768855038] Step 835/1212: training loss=0.05\n",
      "[1768855038] Step 836/1212: training loss=0.20\n",
      "[1768855038] Step 837/1212: training loss=0.34\n",
      "[1768855043] Step 838/1212: training loss=0.19\n",
      "[1768855043] Step 839/1212: training loss=0.07\n",
      "[1768855043] Step 840/1212: training loss=0.40\n",
      "[1768855044] Step 841/1212: training loss=0.18\n",
      "[1768855044] Step 842/1212: training loss=0.14\n",
      "[1768855044] Step 843/1212: training loss=0.21\n",
      "[1768855049] Step 844/1212: training loss=1.43\n",
      "[1768855049] Step 845/1212: training loss=0.09\n",
      "[1768855049] Step 846/1212: training loss=0.12\n",
      "[1768855049] Step 847/1212: training loss=0.08\n",
      "[1768855049] Step 848/1212: training loss=0.14\n",
      "[1768855049] Step 849/1212: training loss=0.09\n",
      "[1768855049] Step 850/1212: training loss=0.18\n",
      "[1768855054] Step 851/1212: training loss=0.30\n",
      "[1768855055] Step 852/1212: training loss=0.18\n",
      "[1768855055] Step 853/1212: training loss=0.20\n",
      "[1768855055] Step 854/1212: training loss=0.09\n",
      "[1768855060] Step 855/1212: training loss=1.50\n",
      "[1768855061] Step 856/1212: training loss=0.20\n",
      "[1768855061] Step 857/1212: training loss=0.17\n",
      "[1768855061] Step 858/1212: training loss=1.00\n",
      "[1768855061] Step 859/1212: training loss=0.22\n",
      "[1768855061] Step 860/1212: training loss=0.03\n",
      "[1768855062] Step 861/1212: training loss=0.17\n",
      "[1768855066] Step 862/1212: training loss=0.13\n",
      "[1768855067] Step 863/1212: training loss=0.14\n",
      "[1768855067] Step 864/1212: training loss=0.18\n",
      "[1768855067] Step 865/1212: training loss=0.13\n",
      "[1768855067] Step 866/1212: training loss=0.16\n",
      "[1768855067] Step 867/1212: training loss=0.17\n",
      "[1768855067] Step 868/1212: training loss=0.38\n",
      "[1768855071] Step 869/1212: training loss=1.23\n",
      "[1768855071] Step 870/1212: training loss=0.07\n",
      "[1768855072] Step 871/1212: training loss=0.05\n",
      "[1768855073] Step 872/1212: training loss=0.14\n",
      "[1768855073] Step 873/1212: training loss=0.10\n",
      "[1768855073] Step 874/1212: training loss=0.12\n",
      "[1768855077] Step 875/1212: training loss=0.16\n",
      "[1768855077] Step 876/1212: training loss=0.19\n",
      "[1768855077] Step 877/1212: training loss=0.16\n",
      "[1768855077] Step 878/1212: training loss=0.23\n",
      "[1768855077] Step 879/1212: training loss=0.19\n",
      "[1768855082] Step 880/1212: training loss=0.17\n",
      "[1768855083] Step 881/1212: training loss=0.19\n",
      "[1768855083] Step 882/1212: training loss=0.11\n",
      "[1768855083] Step 883/1212: training loss=0.28\n",
      "[1768855088] Step 884/1212: training loss=0.10\n",
      "[1768855088] Step 885/1212: training loss=0.22\n",
      "[1768855088] Step 886/1212: training loss=0.10\n",
      "[1768855088] Step 887/1212: training loss=0.31\n",
      "[1768855088] Step 888/1212: training loss=0.11\n",
      "[1768855088] Step 889/1212: training loss=0.17\n",
      "[1768855094] Step 890/1212: training loss=0.27\n",
      "[1768855095] Step 891/1212: training loss=1.36\n",
      "[1768855095] Step 892/1212: training loss=0.11\n",
      "[1768855095] Step 893/1212: training loss=1.10\n",
      "[1768855095] Step 894/1212: training loss=0.25\n",
      "[1768855095] Step 895/1212: training loss=0.12\n",
      "[1768855100] Step 896/1212: training loss=0.10\n",
      "[1768855101] Step 897/1212: training loss=0.09\n",
      "[1768855101] Step 898/1212: training loss=0.13\n",
      "[1768855101] Step 899/1212: training loss=0.11\n",
      "[1768855101] Step 900/1212: training loss=0.06\n",
      "[1768855101] Step 901/1212: training loss=0.19\n",
      "[1768855106] Step 902/1212: training loss=0.08\n",
      "[1768855106] Step 903/1212: training loss=0.07\n",
      "[1768855106] Step 904/1212: training loss=0.12\n",
      "[1768855106] Step 905/1212: training loss=0.21\n",
      "[1768855106] Step 906/1212: training loss=0.24\n",
      "[1768855111] Step 907/1212: training loss=0.09\n",
      "[1768855111] Step 908/1212: training loss=0.16\n",
      "[1768855111] Step 909/1212: training loss=0.20\n",
      "[1768855111] Step 910/1212: training loss=0.86\n",
      "[1768855112] Step 911/1212: training loss=0.13\n",
      "[1768855112] Step 912/1212: training loss=0.23\n",
      "[1768855117] Step 913/1212: training loss=0.20\n",
      "[1768855117] Step 914/1212: training loss=0.14\n",
      "[1768855117] Step 915/1212: training loss=0.33\n",
      "[1768855117] Step 916/1212: training loss=0.15\n",
      "[1768855117] Step 917/1212: training loss=0.06\n",
      "[1768855117] Step 918/1212: training loss=2.11\n",
      "[1768855122] Step 919/1212: training loss=0.28\n",
      "[1768855122] Step 920/1212: training loss=0.03\n",
      "[1768855123] Step 921/1212: training loss=0.07\n",
      "[1768855128] Step 922/1212: training loss=0.14\n",
      "[1768855129] Step 923/1212: training loss=0.22\n",
      "[1768855129] Step 924/1212: training loss=0.06\n",
      "[1768855129] Step 925/1212: training loss=0.35\n",
      "[1768855129] Step 926/1212: training loss=0.08\n",
      "[1768855129] Step 927/1212: training loss=0.16\n",
      "[1768855129] Step 928/1212: training loss=0.25\n",
      "[1768855129] Step 929/1212: training loss=0.25\n",
      "[1768855134] Step 930/1212: training loss=1.00\n",
      "[1768855134] Step 931/1212: training loss=0.15\n",
      "[1768855135] Step 932/1212: training loss=0.13\n",
      "[1768855135] Step 933/1212: training loss=1.03\n",
      "[1768855135] Step 934/1212: training loss=0.20\n",
      "[1768855139] Step 935/1212: training loss=0.06\n",
      "[1768855139] Step 936/1212: training loss=0.08\n",
      "[1768855139] Step 937/1212: training loss=0.06\n",
      "[1768855139] Step 938/1212: training loss=0.17\n",
      "[1768855139] Step 939/1212: training loss=0.23\n",
      "[1768855139] Step 940/1212: training loss=0.11\n",
      "[1768855145] Step 941/1212: training loss=0.05\n",
      "[1768855145] Step 942/1212: training loss=0.20\n",
      "[1768855145] Step 943/1212: training loss=0.99\n",
      "[1768855145] Step 944/1212: training loss=0.48\n",
      "[1768855145] Step 945/1212: training loss=0.06\n",
      "[1768855145] Step 946/1212: training loss=0.17\n",
      "[1768855150] Step 947/1212: training loss=0.08\n",
      "[1768855150] Step 948/1212: training loss=0.18\n",
      "[1768855150] Step 949/1212: training loss=0.11\n",
      "[1768855150] Step 950/1212: training loss=0.18\n",
      "[1768855151] Step 951/1212: training loss=0.22\n",
      "[1768855156] Step 952/1212: training loss=0.25\n",
      "[1768855156] Step 953/1212: training loss=1.00\n",
      "[1768855156] Step 954/1212: training loss=2.16\n",
      "[1768855156] Step 955/1212: training loss=0.03\n",
      "[1768855156] Step 956/1212: training loss=1.94\n",
      "[1768855156] Step 957/1212: training loss=0.08\n",
      "[1768855156] Step 958/1212: training loss=0.21\n",
      "[1768855161] Step 959/1212: training loss=0.24\n",
      "[1768855162] Step 960/1212: training loss=0.25\n",
      "[1768855163] Step 961/1212: training loss=0.05\n",
      "[1768855163] Step 962/1212: training loss=0.15\n",
      "[1768855163] Step 963/1212: training loss=0.07\n",
      "[1768855163] Step 964/1212: training loss=0.08\n",
      "[1768855167] Step 965/1212: training loss=2.30\n",
      "[1768855168] Step 966/1212: training loss=0.10\n",
      "[1768855168] Step 967/1212: training loss=0.22\n",
      "[1768855168] Step 968/1212: training loss=0.08\n",
      "[1768855168] Step 969/1212: training loss=0.12\n",
      "[1768855168] Step 970/1212: training loss=0.23\n",
      "[1768855173] Step 971/1212: training loss=0.10\n",
      "[1768855174] Step 972/1212: training loss=0.14\n",
      "[1768855174] Step 973/1212: training loss=0.12\n",
      "[1768855174] Step 974/1212: training loss=0.39\n",
      "[1768855174] Step 975/1212: training loss=0.11\n",
      "[1768855179] Step 976/1212: training loss=0.21\n",
      "[1768855180] Step 977/1212: training loss=0.19\n",
      "[1768855180] Step 978/1212: training loss=0.09\n",
      "[1768855180] Step 979/1212: training loss=0.21\n",
      "[1768855180] Step 980/1212: training loss=0.26\n",
      "[1768855180] Step 981/1212: training loss=0.13\n",
      "[1768855185] Step 982/1212: training loss=0.07\n",
      "[1768855185] Step 983/1212: training loss=0.15\n",
      "[1768855185] Step 984/1212: training loss=0.01\n",
      "[1768855185] Step 985/1212: training loss=0.24\n",
      "[1768855185] Step 986/1212: training loss=0.01\n",
      "[1768855185] Step 987/1212: training loss=0.14\n",
      "[1768855190] Step 988/1212: training loss=1.62\n",
      "[1768855190] Step 989/1212: training loss=0.05\n",
      "[1768855190] Step 990/1212: training loss=0.07\n",
      "[1768855191] Step 991/1212: training loss=0.13\n",
      "[1768855191] Step 992/1212: training loss=0.04\n",
      "[1768855197] Step 993/1212: training loss=0.19\n",
      "[1768855198] Step 994/1212: training loss=0.11\n",
      "[1768855198] Step 995/1212: training loss=0.13\n",
      "[1768855198] Step 996/1212: training loss=0.08\n",
      "[1768855198] Step 997/1212: training loss=1.12\n",
      "[1768855198] Step 998/1212: training loss=0.23\n",
      "[1768855198] Step 999/1212: training loss=0.22\n",
      "[1768855202] Step 1000/1212: training loss=0.18\n",
      "[1768855203] Step 1001/1212: training loss=0.15\n",
      "[1768855203] Step 1002/1212: training loss=1.49\n",
      "[1768855203] Step 1003/1212: training loss=0.07\n",
      "[1768855203] Step 1004/1212: training loss=0.10\n",
      "[1768855208] Step 1005/1212: training loss=0.20\n",
      "[1768855208] Step 1006/1212: training loss=0.28\n",
      "[1768855208] Step 1007/1212: training loss=0.14\n",
      "[1768855208] Step 1008/1212: training loss=0.21\n",
      "[1768855208] Step 1009/1212: training loss=0.06\n",
      "[1768855208] Step 1010/1212: training loss=1.52\n",
      "[1768855213] Step 1011/1212: training loss=0.17\n",
      "[1768855214] Step 1012/1212: training loss=0.25\n",
      "[1768855214] Step 1013/1212: training loss=0.13\n",
      "[1768855214] Step 1014/1212: training loss=0.21\n",
      "[1768855218] Step 1015/1212: training loss=0.16\n",
      "[1768855218] Step 1016/1212: training loss=1.25\n",
      "[1768855218] Step 1017/1212: training loss=0.06\n",
      "[1768855218] Step 1018/1212: training loss=0.57\n",
      "[1768855218] Step 1019/1212: training loss=0.11\n",
      "[1768855218] Step 1020/1212: training loss=0.10\n",
      "[1768855224] Step 1021/1212: training loss=0.13\n",
      "[1768855224] Step 1022/1212: training loss=0.12\n",
      "[1768855224] Step 1023/1212: training loss=0.07\n",
      "[1768855224] Step 1024/1212: training loss=0.12\n",
      "[1768855224] Step 1025/1212: training loss=0.14\n",
      "[1768855230] Step 1026/1212: training loss=0.24\n",
      "[1768855231] Step 1027/1212: training loss=0.08\n",
      "[1768855231] Step 1028/1212: training loss=0.07\n",
      "[1768855231] Step 1029/1212: training loss=1.88\n",
      "[1768855231] Step 1030/1212: training loss=0.11\n",
      "[1768855231] Step 1031/1212: training loss=0.31\n",
      "[1768855231] Step 1032/1212: training loss=0.16\n",
      "[1768855236] Step 1033/1212: training loss=0.10\n",
      "[1768855236] Step 1034/1212: training loss=0.17\n",
      "[1768855236] Step 1035/1212: training loss=0.20\n",
      "[1768855236] Step 1036/1212: training loss=0.11\n",
      "[1768855241] Step 1037/1212: training loss=0.20\n",
      "[1768855241] Step 1038/1212: training loss=0.07\n",
      "[1768855241] Step 1039/1212: training loss=0.09\n",
      "[1768855241] Step 1040/1212: training loss=0.08\n",
      "[1768855242] Step 1041/1212: training loss=0.22\n",
      "[1768855247] Step 1042/1212: training loss=0.17\n",
      "[1768855247] Step 1043/1212: training loss=0.18\n",
      "[1768855247] Step 1044/1212: training loss=0.11\n",
      "[1768855247] Step 1045/1212: training loss=0.07\n",
      "[1768855247] Step 1046/1212: training loss=0.13\n",
      "[1768855247] Step 1047/1212: training loss=0.33\n",
      "[1768855247] Step 1048/1212: training loss=0.19\n",
      "[1768855252] Step 1049/1212: training loss=0.05\n",
      "[1768855252] Step 1050/1212: training loss=0.07\n",
      "[1768855253] Step 1051/1212: training loss=0.17\n",
      "[1768855253] Step 1052/1212: training loss=0.10\n",
      "[1768855253] Step 1053/1212: training loss=0.18\n",
      "[1768855257] Step 1054/1212: training loss=0.16\n",
      "[1768855258] Step 1055/1212: training loss=0.11\n",
      "[1768855258] Step 1056/1212: training loss=0.08\n",
      "[1768855258] Step 1057/1212: training loss=0.11\n",
      "[1768855258] Step 1058/1212: training loss=0.14\n",
      "[1768855258] Step 1059/1212: training loss=0.04\n",
      "[1768855258] Step 1060/1212: training loss=0.38\n",
      "[1768855264] Step 1061/1212: training loss=0.20\n",
      "[1768855265] Step 1062/1212: training loss=0.11\n",
      "[1768855265] Step 1063/1212: training loss=0.90\n",
      "[1768855265] Step 1064/1212: training loss=0.20\n",
      "[1768855265] Step 1065/1212: training loss=0.09\n",
      "[1768855265] Step 1066/1212: training loss=1.06\n",
      "[1768855265] Step 1067/1212: training loss=0.13\n",
      "[1768855265] Step 1068/1212: training loss=1.03\n",
      "[1768855270] Step 1069/1212: training loss=0.21\n",
      "[1768855270] Step 1070/1212: training loss=0.87\n",
      "[1768855270] Step 1071/1212: training loss=0.05\n",
      "[1768855271] Step 1072/1212: training loss=0.15\n",
      "[1768855271] Step 1073/1212: training loss=0.29\n",
      "[1768855271] Step 1074/1212: training loss=0.08\n",
      "[1768855275] Step 1075/1212: training loss=0.18\n",
      "[1768855275] Step 1076/1212: training loss=0.11\n",
      "[1768855275] Step 1077/1212: training loss=1.53\n",
      "[1768855275] Step 1078/1212: training loss=0.12\n",
      "[1768855275] Step 1079/1212: training loss=0.13\n",
      "[1768855275] Step 1080/1212: training loss=0.08\n",
      "[1768855281] Step 1081/1212: training loss=0.13\n",
      "[1768855281] Step 1082/1212: training loss=0.33\n",
      "[1768855281] Step 1083/1212: training loss=1.10\n",
      "[1768855281] Step 1084/1212: training loss=0.08\n",
      "[1768855281] Step 1085/1212: training loss=0.05\n",
      "[1768855281] Step 1086/1212: training loss=0.12\n",
      "[1768855286] Step 1087/1212: training loss=0.18\n",
      "[1768855286] Step 1088/1212: training loss=1.80\n",
      "[1768855286] Step 1089/1212: training loss=0.12\n",
      "[1768855286] Step 1090/1212: training loss=0.08\n",
      "[1768855287] Step 1091/1212: training loss=0.08\n",
      "[1768855292] Step 1092/1212: training loss=0.30\n",
      "[1768855292] Step 1093/1212: training loss=0.05\n",
      "[1768855292] Step 1094/1212: training loss=1.09\n",
      "[1768855292] Step 1095/1212: training loss=0.27\n",
      "[1768855292] Step 1096/1212: training loss=0.13\n",
      "[1768855292] Step 1097/1212: training loss=0.31\n",
      "[1768855292] Step 1098/1212: training loss=0.11\n",
      "[1768855298] Step 1099/1212: training loss=0.12\n",
      "[1768855299] Step 1100/1212: training loss=0.14\n",
      "[1768855299] Step 1101/1212: training loss=0.07\n",
      "[1768855299] Step 1102/1212: training loss=0.15\n",
      "[1768855299] Step 1103/1212: training loss=0.11\n",
      "[1768855299] Step 1104/1212: training loss=0.07\n",
      "[1768855299] Step 1105/1212: training loss=0.12\n",
      "[1768855304] Step 1106/1212: training loss=0.15\n",
      "[1768855305] Step 1107/1212: training loss=0.10\n",
      "[1768855305] Step 1108/1212: training loss=0.07\n",
      "[1768855305] Step 1109/1212: training loss=1.76\n",
      "[1768855305] Step 1110/1212: training loss=0.13\n",
      "[1768855310] Step 1111/1212: training loss=0.16\n",
      "[1768855310] Step 1112/1212: training loss=0.35\n",
      "[1768855310] Step 1113/1212: training loss=0.31\n",
      "[1768855310] Step 1114/1212: training loss=0.26\n",
      "[1768855310] Step 1115/1212: training loss=0.19\n",
      "[1768855315] Step 1116/1212: training loss=1.01\n",
      "[1768855315] Step 1117/1212: training loss=0.02\n",
      "[1768855315] Step 1118/1212: training loss=0.52\n",
      "[1768855315] Step 1119/1212: training loss=1.14\n",
      "[1768855315] Step 1120/1212: training loss=0.27\n",
      "[1768855316] Step 1121/1212: training loss=0.03\n",
      "[1768855321] Step 1122/1212: training loss=0.05\n",
      "[1768855321] Step 1123/1212: training loss=1.22\n",
      "[1768855321] Step 1124/1212: training loss=0.07\n",
      "[1768855321] Step 1125/1212: training loss=0.18\n",
      "[1768855321] Step 1126/1212: training loss=1.37\n",
      "[1768855321] Step 1127/1212: training loss=1.57\n",
      "[1768855325] Step 1128/1212: training loss=0.25\n",
      "[1768855326] Step 1129/1212: training loss=0.25\n",
      "[1768855326] Step 1130/1212: training loss=0.04\n",
      "[1768855327] Step 1131/1212: training loss=0.18\n",
      "[1768855327] Step 1132/1212: training loss=0.38\n",
      "[1768855332] Step 1133/1212: training loss=0.34\n",
      "[1768855333] Step 1134/1212: training loss=0.13\n",
      "[1768855333] Step 1135/1212: training loss=0.29\n",
      "[1768855333] Step 1136/1212: training loss=0.19\n",
      "[1768855333] Step 1137/1212: training loss=0.13\n",
      "[1768855333] Step 1138/1212: training loss=1.27\n",
      "[1768855333] Step 1139/1212: training loss=0.07\n",
      "[1768855338] Step 1140/1212: training loss=0.14\n",
      "[1768855338] Step 1141/1212: training loss=0.05\n",
      "[1768855339] Step 1142/1212: training loss=0.33\n",
      "[1768855339] Step 1143/1212: training loss=0.01\n",
      "[1768855339] Step 1144/1212: training loss=0.24\n",
      "[1768855339] Step 1145/1212: training loss=0.13\n",
      "[1768855344] Step 1146/1212: training loss=0.39\n",
      "[1768855344] Step 1147/1212: training loss=0.25\n",
      "[1768855344] Step 1148/1212: training loss=0.21\n",
      "[1768855344] Step 1149/1212: training loss=0.15\n",
      "[1768855344] Step 1150/1212: training loss=0.15\n",
      "[1768855344] Step 1151/1212: training loss=0.25\n",
      "[1768855350] Step 1152/1212: training loss=0.17\n",
      "[1768855350] Step 1153/1212: training loss=1.48\n",
      "[1768855350] Step 1154/1212: training loss=0.06\n",
      "[1768855350] Step 1155/1212: training loss=0.06\n",
      "[1768855350] Step 1156/1212: training loss=0.13\n",
      "[1768855350] Step 1157/1212: training loss=0.09\n",
      "[1768855350] Step 1158/1212: training loss=0.03\n",
      "[1768855355] Step 1159/1212: training loss=0.14\n",
      "[1768855355] Step 1160/1212: training loss=1.64\n",
      "[1768855356] Step 1161/1212: training loss=1.20\n",
      "[1768855356] Step 1162/1212: training loss=0.21\n",
      "[1768855360] Step 1163/1212: training loss=0.11\n",
      "[1768855361] Step 1164/1212: training loss=0.15\n",
      "[1768855361] Step 1165/1212: training loss=0.28\n",
      "[1768855361] Step 1166/1212: training loss=0.35\n",
      "[1768855361] Step 1167/1212: training loss=0.08\n",
      "[1768855361] Step 1168/1212: training loss=1.69\n",
      "[1768855361] Step 1169/1212: training loss=0.09\n",
      "[1768855367] Step 1170/1212: training loss=1.87\n",
      "[1768855368] Step 1171/1212: training loss=0.19\n",
      "[1768855368] Step 1172/1212: training loss=0.05\n",
      "[1768855368] Step 1173/1212: training loss=0.06\n",
      "[1768855368] Step 1174/1212: training loss=0.88\n",
      "[1768855368] Step 1175/1212: training loss=0.17\n",
      "[1768855373] Step 1176/1212: training loss=0.10\n",
      "[1768855373] Step 1177/1212: training loss=0.26\n",
      "[1768855373] Step 1178/1212: training loss=0.04\n",
      "[1768855373] Step 1179/1212: training loss=0.27\n",
      "[1768855373] Step 1180/1212: training loss=0.10\n",
      "[1768855374] Step 1181/1212: training loss=0.12\n",
      "[1768855378] Step 1182/1212: training loss=0.08\n",
      "[1768855379] Step 1183/1212: training loss=0.08\n",
      "[1768855379] Step 1184/1212: training loss=0.09\n",
      "[1768855379] Step 1185/1212: training loss=0.17\n",
      "[1768855379] Step 1186/1212: training loss=0.25\n",
      "[1768855379] Step 1187/1212: training loss=0.22\n",
      "[1768855379] Step 1188/1212: training loss=0.05\n",
      "[1768855384] Step 1189/1212: training loss=0.18\n",
      "[1768855384] Step 1190/1212: training loss=0.22\n",
      "[1768855384] Step 1191/1212: training loss=0.40\n",
      "[1768855385] Step 1192/1212: training loss=0.92\n",
      "[1768855385] Step 1193/1212: training loss=0.25\n",
      "[1768855385] Step 1194/1212: training loss=0.14\n",
      "[1768855390] Step 1195/1212: training loss=1.12\n",
      "[1768855390] Step 1196/1212: training loss=0.10\n",
      "[1768855390] Step 1197/1212: training loss=0.17\n",
      "[1768855390] Step 1198/1212: training loss=1.44\n",
      "[1768855390] Step 1199/1212: training loss=0.32\n",
      "[1768855394] Step 1200/1212: training loss=1.11\n",
      "[1768855395] Step 1201/1212: training loss=0.28\n",
      "[1768855395] Step 1202/1212: training loss=0.15\n",
      "[1768855395] Step 1203/1212: training loss=0.09\n",
      "[1768855395] Step 1204/1212: training loss=1.05\n",
      "[1768855401] Step 1205/1212: training loss=0.09\n",
      "[1768855402] Step 1206/1212: training loss=0.09\n",
      "[1768855402] Step 1207/1212: training loss=0.10\n",
      "[1768855402] Step 1208/1212: training loss=0.13\n",
      "[1768855402] Step 1209/1212: training loss=0.19\n",
      "[1768855402] Step 1210/1212: training loss=0.91\n",
      "[1768855403] Step 1211/1212: training loss=0.35\n",
      "[1768855408] Step 1212/1212: training loss=0.21\n",
      "[1768855425] Checkpoint created at step 404\n",
      "[1768855425] Checkpoint created at step 808\n",
      "[1768855425] New fine-tuned model created\n",
      "[1768855425] Evaluating model against our usage policies\n",
      "[1768856271] Moderation checks for snapshot ft:gpt-4.1-mini-2025-04-14:tavily::CzqO8otr passed.\n",
      "[1768856271] Usage policy evaluations completed, model is now enabled for sampling\n",
      "[1768856273] The job has successfully completed\n",
      "\n",
      "⚠ No loss data found in training events\n",
      "\n",
      "============================================================\n",
      "✓ Fine-tuning completed!\n",
      "Fine-tuned model: ft:gpt-4.1-mini-2025-04-14:tavily::CzqO8otr\n",
      "Trained tokens: 8,949,513\n",
      "Actual cost: $44.75\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ gpt-4.1-mini-2025-04-14 → ft:gpt-4.1-mini-2025-04-14:tavily::CzqO8otr\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Starting fine-tuning for gpt-4.1-nano-2025-04-14\n",
      "======================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: gpt-4.1-nano-2025-04-14\n",
      "Training samples: 404\n",
      "Total tokens: 2,977,919\n",
      "Epochs: 3\n",
      "Estimated training cost: $13.40\n",
      "Training file: data\\train_gpt-4.1-nano-2025-04-14.jsonl\n",
      "============================================================\n",
      "\n",
      "Fine-tuning job submitted: ftjob-ziRrmcs9LE0MmquQtUQk4EC3\n",
      "Monitor at: https://platform.openai.com/finetune/ftjob-ziRrmcs9LE0MmquQtUQk4EC3\n",
      "\n",
      "[1768856301] Created fine-tuning job: ftjob-ziRrmcs9LE0MmquQtUQk4EC3\n",
      "[1768856301] Validating training file: file-RoXvP2uCUFtrGMSrUMb42T\n",
      "[1768856556] Files validated, moving job to queued state\n",
      "[1768856559] Fine-tuning job started\n",
      "[1768856668] Step 1/1212: training loss=1.65\n",
      "[1768856670] Step 2/1212: training loss=1.66\n",
      "[1768856670] Step 3/1212: training loss=2.12\n",
      "[1768856672] Step 4/1212: training loss=1.85\n",
      "[1768856672] Step 5/1212: training loss=1.68\n",
      "[1768856672] Step 6/1212: training loss=1.86\n",
      "[1768856674] Step 7/1212: training loss=2.12\n",
      "[1768856676] Step 8/1212: training loss=1.75\n",
      "[1768856678] Step 9/1212: training loss=2.31\n",
      "[1768856688] Step 10/1212: training loss=1.63\n",
      "[1768856690] Step 11/1212: training loss=1.48\n",
      "[1768856695] Step 12/1212: training loss=1.36\n",
      "[1768856695] Step 13/1212: training loss=1.29\n",
      "[1768856695] Step 14/1212: training loss=1.53\n",
      "[1768856697] Step 15/1212: training loss=1.44\n",
      "[1768856697] Step 16/1212: training loss=1.72\n",
      "[1768856699] Step 17/1212: training loss=1.66\n",
      "[1768856699] Step 18/1212: training loss=1.13\n",
      "[1768856701] Step 19/1212: training loss=1.48\n",
      "[1768856703] Step 20/1212: training loss=1.41\n",
      "[1768856705] Step 21/1212: training loss=1.05\n",
      "[1768856707] Step 22/1212: training loss=1.53\n",
      "[1768856707] Step 23/1212: training loss=1.00\n",
      "[1768856709] Step 24/1212: training loss=1.19\n",
      "[1768856709] Step 25/1212: training loss=0.89\n",
      "[1768856711] Step 26/1212: training loss=1.61\n",
      "[1768856713] Step 27/1212: training loss=1.14\n",
      "[1768856713] Step 28/1212: training loss=1.54\n",
      "[1768856713] Step 29/1212: training loss=1.15\n",
      "[1768856715] Step 30/1212: training loss=1.43\n",
      "[1768856715] Step 31/1212: training loss=1.07\n",
      "[1768856717] Step 32/1212: training loss=1.24\n",
      "[1768856719] Step 33/1212: training loss=0.92\n",
      "[1768856721] Step 34/1212: training loss=0.88\n",
      "[1768856721] Step 35/1212: training loss=1.17\n",
      "[1768856723] Step 36/1212: training loss=0.90\n",
      "[1768856725] Step 37/1212: training loss=1.01\n",
      "[1768856725] Step 38/1212: training loss=0.68\n",
      "[1768856727] Step 39/1212: training loss=1.19\n",
      "[1768856729] Step 40/1212: training loss=0.99\n",
      "[1768856729] Step 41/1212: training loss=1.34\n",
      "[1768856731] Step 42/1212: training loss=1.32\n",
      "[1768856731] Step 43/1212: training loss=1.13\n",
      "[1768856735] Step 44/1212: training loss=0.86\n",
      "[1768856735] Step 45/1212: training loss=1.10\n",
      "[1768856735] Step 46/1212: training loss=0.64\n",
      "[1768856740] Step 47/1212: training loss=1.06\n",
      "[1768856742] Step 48/1212: training loss=0.65\n",
      "[1768856742] Step 49/1212: training loss=1.02\n",
      "[1768856744] Step 50/1212: training loss=1.20\n",
      "[1768856744] Step 51/1212: training loss=1.20\n",
      "[1768856744] Step 52/1212: training loss=1.01\n",
      "[1768856746] Step 53/1212: training loss=1.15\n",
      "[1768856746] Step 54/1212: training loss=0.94\n",
      "[1768856748] Step 55/1212: training loss=1.28\n",
      "[1768856748] Step 56/1212: training loss=0.81\n",
      "[1768856750] Step 57/1212: training loss=1.16\n",
      "[1768856750] Step 58/1212: training loss=0.92\n",
      "[1768856752] Step 59/1212: training loss=1.17\n",
      "[1768856752] Step 60/1212: training loss=0.87\n",
      "[1768856752] Step 61/1212: training loss=0.57\n",
      "[1768856754] Step 62/1212: training loss=0.72\n",
      "[1768856754] Step 63/1212: training loss=1.12\n",
      "[1768856760] Step 64/1212: training loss=1.19\n",
      "[1768856760] Step 65/1212: training loss=0.90\n",
      "[1768856762] Step 66/1212: training loss=1.25\n",
      "[1768856762] Step 67/1212: training loss=2.01\n",
      "[1768856766] Step 68/1212: training loss=1.32\n",
      "[1768856766] Step 69/1212: training loss=1.00\n",
      "[1768856768] Step 70/1212: training loss=1.52\n",
      "[1768856768] Step 71/1212: training loss=1.03\n",
      "[1768856768] Step 72/1212: training loss=0.81\n",
      "[1768856774] Step 73/1212: training loss=1.05\n",
      "[1768856774] Step 74/1212: training loss=0.92\n",
      "[1768856774] Step 75/1212: training loss=1.56\n",
      "[1768856780] Step 76/1212: training loss=1.67\n",
      "[1768856780] Step 77/1212: training loss=0.84\n",
      "[1768856780] Step 78/1212: training loss=0.95\n",
      "[1768856782] Step 79/1212: training loss=1.09\n",
      "[1768856782] Step 80/1212: training loss=0.87\n",
      "[1768856784] Step 81/1212: training loss=1.09\n",
      "[1768856787] Step 82/1212: training loss=0.71\n",
      "[1768856787] Step 83/1212: training loss=1.08\n",
      "[1768856787] Step 84/1212: training loss=0.99\n",
      "[1768856789] Step 85/1212: training loss=1.25\n",
      "[1768856789] Step 86/1212: training loss=0.89\n",
      "[1768856791] Step 87/1212: training loss=1.23\n",
      "[1768856793] Step 88/1212: training loss=0.80\n",
      "[1768856795] Step 89/1212: training loss=0.77\n",
      "[1768856795] Step 90/1212: training loss=1.23\n",
      "[1768856807] Step 91/1212: training loss=1.33\n",
      "[1768856809] Step 92/1212: training loss=1.03\n",
      "[1768856821] Step 93/1212: training loss=0.91\n",
      "[1768856823] Step 94/1212: training loss=1.68\n",
      "[1768856823] Step 95/1212: training loss=0.65\n",
      "[1768856825] Step 96/1212: training loss=0.89\n",
      "[1768856827] Step 97/1212: training loss=0.86\n",
      "[1768856829] Step 98/1212: training loss=1.01\n",
      "[1768856829] Step 99/1212: training loss=1.21\n",
      "[1768856831] Step 100/1212: training loss=0.98\n",
      "[1768856833] Step 101/1212: training loss=0.84\n",
      "[1768856833] Step 102/1212: training loss=1.10\n",
      "[1768856838] Step 103/1212: training loss=1.00\n",
      "[1768856848] Step 104/1212: training loss=1.10\n",
      "[1768856852] Step 105/1212: training loss=0.87\n",
      "[1768856852] Step 106/1212: training loss=0.91\n",
      "[1768856852] Step 107/1212: training loss=1.00\n",
      "[1768856858] Step 108/1212: training loss=0.61\n",
      "[1768856860] Step 109/1212: training loss=0.88\n",
      "[1768856862] Step 110/1212: training loss=1.22\n",
      "[1768856864] Step 111/1212: training loss=1.07\n",
      "[1768856864] Step 112/1212: training loss=0.91\n",
      "[1768856866] Step 113/1212: training loss=0.97\n",
      "[1768856866] Step 114/1212: training loss=0.97\n",
      "[1768856868] Step 115/1212: training loss=1.03\n",
      "[1768856870] Step 116/1212: training loss=1.14\n",
      "[1768856870] Step 117/1212: training loss=0.96\n",
      "[1768856870] Step 118/1212: training loss=0.93\n",
      "[1768856872] Step 119/1212: training loss=0.52\n",
      "[1768856874] Step 120/1212: training loss=0.88\n",
      "[1768856874] Step 121/1212: training loss=0.54\n",
      "[1768856881] Step 122/1212: training loss=1.11\n",
      "[1768856881] Step 123/1212: training loss=0.75\n",
      "[1768856885] Step 124/1212: training loss=0.73\n",
      "[1768856885] Step 125/1212: training loss=0.75\n",
      "[1768856887] Step 126/1212: training loss=1.15\n",
      "[1768856887] Step 127/1212: training loss=0.80\n",
      "[1768856889] Step 128/1212: training loss=1.13\n",
      "[1768856891] Step 129/1212: training loss=0.93\n",
      "[1768856893] Step 130/1212: training loss=1.02\n",
      "[1768856893] Step 131/1212: training loss=1.16\n",
      "[1768856893] Step 132/1212: training loss=0.84\n",
      "[1768856895] Step 133/1212: training loss=1.33\n",
      "[1768856895] Step 134/1212: training loss=1.06\n",
      "[1768856899] Step 135/1212: training loss=0.86\n",
      "[1768856901] Step 136/1212: training loss=0.48\n",
      "[1768856901] Step 137/1212: training loss=1.10\n",
      "[1768856904] Step 138/1212: training loss=0.67\n",
      "[1768856904] Step 139/1212: training loss=1.02\n",
      "[1768856904] Step 140/1212: training loss=0.64\n",
      "[1768856906] Step 141/1212: training loss=1.25\n",
      "[1768856906] Step 142/1212: training loss=0.75\n",
      "[1768856906] Step 143/1212: training loss=0.95\n",
      "[1768856908] Step 144/1212: training loss=1.14\n",
      "[1768856910] Step 145/1212: training loss=1.76\n",
      "[1768856910] Step 146/1212: training loss=0.78\n",
      "[1768856912] Step 147/1212: training loss=1.07\n",
      "[1768856914] Step 148/1212: training loss=0.60\n",
      "[1768856914] Step 149/1212: training loss=0.75\n",
      "[1768856914] Step 150/1212: training loss=0.82\n",
      "[1768856926] Step 151/1212: training loss=0.88\n",
      "[1768856926] Step 152/1212: training loss=1.21\n",
      "[1768856926] Step 153/1212: training loss=0.97\n",
      "[1768856928] Step 154/1212: training loss=0.93\n",
      "[1768856930] Step 155/1212: training loss=0.87\n",
      "[1768856930] Step 156/1212: training loss=1.08\n",
      "[1768856932] Step 157/1212: training loss=0.82\n",
      "[1768856945] Step 158/1212: training loss=1.15\n",
      "[1768856945] Step 159/1212: training loss=0.90\n",
      "[1768856945] Step 160/1212: training loss=1.45\n",
      "[1768856947] Step 161/1212: training loss=0.75\n",
      "[1768856947] Step 162/1212: training loss=1.04\n",
      "[1768856949] Step 163/1212: training loss=1.51\n",
      "[1768856949] Step 164/1212: training loss=0.92\n",
      "[1768856951] Step 165/1212: training loss=0.59\n",
      "[1768856951] Step 166/1212: training loss=0.84\n",
      "[1768856953] Step 167/1212: training loss=0.99\n",
      "[1768856953] Step 168/1212: training loss=1.17\n",
      "[1768856955] Step 169/1212: training loss=0.78\n",
      "[1768856955] Step 170/1212: training loss=0.61\n",
      "[1768856957] Step 171/1212: training loss=1.41\n",
      "[1768856957] Step 172/1212: training loss=1.18\n",
      "[1768856959] Step 173/1212: training loss=0.85\n",
      "[1768856963] Step 174/1212: training loss=1.14\n",
      "[1768856965] Step 175/1212: training loss=0.82\n",
      "[1768856965] Step 176/1212: training loss=1.41\n",
      "[1768856969] Step 177/1212: training loss=0.82\n",
      "[1768856971] Step 178/1212: training loss=0.61\n",
      "[1768856971] Step 179/1212: training loss=0.75\n",
      "[1768856973] Step 180/1212: training loss=0.75\n",
      "[1768856973] Step 181/1212: training loss=0.87\n",
      "[1768856973] Step 182/1212: training loss=0.96\n",
      "[1768856975] Step 183/1212: training loss=0.90\n",
      "[1768856975] Step 184/1212: training loss=1.14\n",
      "[1768856979] Step 185/1212: training loss=1.14\n",
      "[1768856979] Step 186/1212: training loss=0.89\n",
      "[1768856983] Step 187/1212: training loss=1.16\n",
      "[1768856983] Step 188/1212: training loss=0.59\n",
      "[1768856985] Step 189/1212: training loss=0.60\n",
      "[1768856985] Step 190/1212: training loss=1.84\n",
      "[1768856987] Step 191/1212: training loss=1.00\n",
      "[1768856990] Step 192/1212: training loss=0.84\n",
      "[1768856990] Step 193/1212: training loss=0.80\n",
      "[1768856992] Step 194/1212: training loss=0.88\n",
      "[1768856992] Step 195/1212: training loss=0.94\n",
      "[1768856992] Step 196/1212: training loss=0.67\n",
      "[1768856994] Step 197/1212: training loss=1.03\n",
      "[1768856994] Step 198/1212: training loss=0.74\n",
      "[1768856994] Step 199/1212: training loss=1.05\n",
      "[1768856996] Step 200/1212: training loss=0.96\n",
      "[1768856996] Step 201/1212: training loss=1.03\n",
      "[1768856998] Step 202/1212: training loss=1.07\n",
      "[1768856998] Step 203/1212: training loss=0.84\n",
      "[1768857000] Step 204/1212: training loss=0.90\n",
      "[1768857000] Step 205/1212: training loss=1.10\n",
      "[1768857000] Step 206/1212: training loss=1.13\n",
      "[1768857002] Step 207/1212: training loss=0.68\n",
      "[1768857002] Step 208/1212: training loss=0.69\n",
      "[1768857002] Step 209/1212: training loss=0.82\n",
      "[1768857004] Step 210/1212: training loss=0.50\n",
      "[1768857004] Step 211/1212: training loss=0.79\n",
      "[1768857006] Step 212/1212: training loss=0.79\n",
      "[1768857006] Step 213/1212: training loss=0.93\n",
      "[1768857008] Step 214/1212: training loss=0.65\n",
      "[1768857010] Step 215/1212: training loss=1.00\n",
      "[1768857010] Step 216/1212: training loss=1.36\n",
      "[1768857012] Step 217/1212: training loss=1.45\n",
      "[1768857012] Step 218/1212: training loss=1.39\n",
      "[1768857016] Step 219/1212: training loss=1.64\n",
      "[1768857016] Step 220/1212: training loss=0.90\n",
      "[1768857016] Step 221/1212: training loss=0.96\n",
      "[1768857018] Step 222/1212: training loss=0.54\n",
      "[1768857020] Step 223/1212: training loss=1.06\n",
      "[1768857020] Step 224/1212: training loss=1.07\n",
      "[1768857022] Step 225/1212: training loss=1.16\n",
      "[1768857024] Step 226/1212: training loss=0.56\n",
      "[1768857024] Step 227/1212: training loss=1.02\n",
      "[1768857024] Step 228/1212: training loss=0.91\n",
      "[1768857026] Step 229/1212: training loss=0.50\n",
      "[1768857026] Step 230/1212: training loss=0.99\n",
      "[1768857028] Step 231/1212: training loss=0.66\n",
      "[1768857032] Step 232/1212: training loss=1.09\n",
      "[1768857034] Step 233/1212: training loss=1.02\n",
      "[1768857034] Step 234/1212: training loss=1.36\n",
      "[1768857034] Step 235/1212: training loss=0.88\n",
      "[1768857036] Step 236/1212: training loss=0.60\n",
      "[1768857036] Step 237/1212: training loss=0.76\n",
      "[1768857039] Step 238/1212: training loss=0.53\n",
      "[1768857039] Step 239/1212: training loss=0.68\n",
      "[1768857039] Step 240/1212: training loss=1.04\n",
      "[1768857041] Step 241/1212: training loss=0.76\n",
      "[1768857043] Step 242/1212: training loss=0.84\n",
      "[1768857043] Step 243/1212: training loss=0.57\n",
      "[1768857045] Step 244/1212: training loss=0.94\n",
      "[1768857045] Step 245/1212: training loss=1.20\n",
      "[1768857047] Step 246/1212: training loss=0.67\n",
      "[1768857047] Step 247/1212: training loss=1.13\n",
      "[1768857049] Step 248/1212: training loss=0.60\n",
      "[1768857049] Step 249/1212: training loss=0.75\n",
      "[1768857051] Step 250/1212: training loss=0.78\n",
      "[1768857051] Step 251/1212: training loss=1.13\n",
      "[1768857053] Step 252/1212: training loss=0.96\n",
      "[1768857053] Step 253/1212: training loss=1.29\n",
      "[1768857053] Step 254/1212: training loss=1.15\n",
      "[1768857055] Step 255/1212: training loss=1.04\n",
      "[1768857055] Step 256/1212: training loss=0.97\n",
      "[1768857055] Step 257/1212: training loss=0.74\n",
      "[1768857059] Step 258/1212: training loss=0.94\n",
      "[1768857061] Step 259/1212: training loss=0.70\n",
      "[1768857063] Step 260/1212: training loss=0.71\n",
      "[1768857065] Step 261/1212: training loss=1.06\n",
      "[1768857065] Step 262/1212: training loss=0.92\n",
      "[1768857069] Step 263/1212: training loss=1.49\n",
      "[1768857069] Step 264/1212: training loss=0.78\n",
      "[1768857071] Step 265/1212: training loss=0.91\n",
      "[1768857075] Step 266/1212: training loss=1.17\n",
      "[1768857077] Step 267/1212: training loss=0.81\n",
      "[1768857077] Step 268/1212: training loss=0.86\n",
      "[1768857079] Step 269/1212: training loss=0.90\n",
      "[1768857079] Step 270/1212: training loss=0.83\n",
      "[1768857081] Step 271/1212: training loss=1.27\n",
      "[1768857081] Step 272/1212: training loss=0.86\n",
      "[1768857088] Step 273/1212: training loss=0.47\n",
      "[1768857090] Step 274/1212: training loss=1.00\n",
      "[1768857092] Step 275/1212: training loss=0.87\n",
      "[1768857092] Step 276/1212: training loss=0.73\n",
      "[1768857094] Step 277/1212: training loss=0.67\n",
      "[1768857094] Step 278/1212: training loss=1.21\n",
      "[1768857096] Step 279/1212: training loss=0.86\n",
      "[1768857096] Step 280/1212: training loss=0.88\n",
      "[1768857098] Step 281/1212: training loss=0.64\n",
      "[1768857098] Step 282/1212: training loss=0.53\n",
      "[1768857098] Step 283/1212: training loss=0.87\n",
      "[1768857100] Step 284/1212: training loss=0.69\n",
      "[1768857100] Step 285/1212: training loss=0.76\n",
      "[1768857100] Step 286/1212: training loss=0.53\n",
      "[1768857102] Step 287/1212: training loss=0.62\n",
      "[1768857104] Step 288/1212: training loss=0.64\n",
      "[1768857104] Step 289/1212: training loss=0.46\n",
      "[1768857106] Step 290/1212: training loss=0.57\n",
      "[1768857106] Step 291/1212: training loss=0.87\n",
      "[1768857108] Step 292/1212: training loss=0.81\n",
      "[1768857108] Step 293/1212: training loss=0.58\n",
      "[1768857110] Step 294/1212: training loss=0.55\n",
      "[1768857110] Step 295/1212: training loss=0.96\n",
      "[1768857112] Step 296/1212: training loss=0.67\n",
      "[1768857114] Step 297/1212: training loss=1.16\n",
      "[1768857114] Step 298/1212: training loss=0.42\n",
      "[1768857114] Step 299/1212: training loss=0.95\n",
      "[1768857116] Step 300/1212: training loss=1.01\n",
      "[1768857116] Step 301/1212: training loss=0.78\n",
      "[1768857116] Step 302/1212: training loss=1.14\n",
      "[1768857118] Step 303/1212: training loss=0.97\n",
      "[1768857120] Step 304/1212: training loss=0.62\n",
      "[1768857120] Step 305/1212: training loss=1.12\n",
      "[1768857120] Step 306/1212: training loss=0.78\n",
      "[1768857122] Step 307/1212: training loss=0.96\n",
      "[1768857124] Step 308/1212: training loss=1.05\n",
      "[1768857124] Step 309/1212: training loss=0.86\n",
      "[1768857126] Step 310/1212: training loss=0.69\n",
      "[1768857128] Step 311/1212: training loss=1.22\n",
      "[1768857128] Step 312/1212: training loss=1.08\n",
      "[1768857130] Step 313/1212: training loss=0.62\n",
      "[1768857131] Step 314/1212: training loss=0.99\n",
      "[1768857131] Step 315/1212: training loss=0.88\n",
      "[1768857133] Step 316/1212: training loss=0.85\n",
      "[1768857133] Step 317/1212: training loss=0.75\n",
      "[1768857133] Step 318/1212: training loss=1.04\n",
      "[1768857135] Step 319/1212: training loss=1.14\n",
      "[1768857137] Step 320/1212: training loss=0.97\n",
      "[1768857137] Step 321/1212: training loss=0.95\n",
      "[1768857141] Step 322/1212: training loss=1.09\n",
      "[1768857141] Step 323/1212: training loss=0.94\n",
      "[1768857141] Step 324/1212: training loss=0.78\n",
      "[1768857143] Step 325/1212: training loss=0.77\n",
      "[1768857145] Step 326/1212: training loss=1.05\n",
      "[1768857145] Step 327/1212: training loss=1.33\n",
      "[1768857147] Step 328/1212: training loss=1.06\n",
      "[1768857149] Step 329/1212: training loss=0.72\n",
      "[1768857149] Step 330/1212: training loss=0.89\n",
      "[1768857151] Step 331/1212: training loss=1.09\n",
      "[1768857155] Step 332/1212: training loss=0.79\n",
      "[1768857157] Step 333/1212: training loss=0.83\n",
      "[1768857161] Step 334/1212: training loss=1.35\n",
      "[1768857165] Step 335/1212: training loss=0.98\n",
      "[1768857165] Step 336/1212: training loss=0.49\n",
      "[1768857165] Step 337/1212: training loss=0.92\n",
      "[1768857167] Step 338/1212: training loss=1.82\n",
      "[1768857169] Step 339/1212: training loss=0.82\n",
      "[1768857173] Step 340/1212: training loss=1.10\n",
      "[1768857175] Step 341/1212: training loss=0.78\n",
      "[1768857175] Step 342/1212: training loss=0.67\n",
      "[1768857178] Step 343/1212: training loss=0.88\n",
      "[1768857178] Step 344/1212: training loss=0.63\n",
      "[1768857178] Step 345/1212: training loss=1.06\n",
      "[1768857180] Step 346/1212: training loss=0.62\n",
      "[1768857180] Step 347/1212: training loss=0.85\n",
      "[1768857182] Step 348/1212: training loss=0.89\n",
      "[1768857182] Step 349/1212: training loss=1.11\n",
      "[1768857184] Step 350/1212: training loss=1.05\n",
      "[1768857186] Step 351/1212: training loss=0.86\n",
      "[1768857186] Step 352/1212: training loss=1.30\n",
      "[1768857186] Step 353/1212: training loss=1.12\n",
      "[1768857188] Step 354/1212: training loss=0.71\n",
      "[1768857190] Step 355/1212: training loss=0.92\n",
      "[1768857192] Step 356/1212: training loss=0.59\n",
      "[1768857192] Step 357/1212: training loss=0.62\n",
      "[1768857192] Step 358/1212: training loss=0.89\n",
      "[1768857194] Step 359/1212: training loss=0.62\n",
      "[1768857194] Step 360/1212: training loss=0.94\n",
      "[1768857194] Step 361/1212: training loss=0.58\n",
      "[1768857196] Step 362/1212: training loss=0.64\n",
      "[1768857198] Step 363/1212: training loss=0.66\n",
      "[1768857198] Step 364/1212: training loss=0.94\n",
      "[1768857200] Step 365/1212: training loss=0.86\n",
      "[1768857202] Step 366/1212: training loss=0.79\n",
      "[1768857204] Step 367/1212: training loss=1.07\n",
      "[1768857206] Step 368/1212: training loss=0.82\n",
      "[1768857210] Step 369/1212: training loss=0.87\n",
      "[1768857210] Step 370/1212: training loss=0.83\n",
      "[1768857212] Step 371/1212: training loss=0.59\n",
      "[1768857214] Step 372/1212: training loss=0.67\n",
      "[1768857214] Step 373/1212: training loss=0.93\n",
      "[1768857216] Step 374/1212: training loss=1.15\n",
      "[1768857218] Step 375/1212: training loss=1.11\n",
      "[1768857218] Step 376/1212: training loss=0.57\n",
      "[1768857218] Step 377/1212: training loss=1.29\n",
      "[1768857220] Step 378/1212: training loss=0.85\n",
      "[1768857222] Step 379/1212: training loss=1.30\n",
      "[1768857225] Step 380/1212: training loss=1.04\n",
      "[1768857225] Step 381/1212: training loss=1.10\n",
      "[1768857229] Step 382/1212: training loss=0.66\n",
      "[1768857231] Step 383/1212: training loss=0.96\n",
      "[1768857231] Step 384/1212: training loss=0.72\n",
      "[1768857233] Step 385/1212: training loss=0.96\n",
      "[1768857233] Step 386/1212: training loss=1.36\n",
      "[1768857233] Step 387/1212: training loss=0.82\n",
      "[1768857235] Step 388/1212: training loss=0.57\n",
      "[1768857235] Step 389/1212: training loss=0.66\n",
      "[1768857235] Step 390/1212: training loss=0.65\n",
      "[1768857237] Step 391/1212: training loss=0.76\n",
      "[1768857239] Step 392/1212: training loss=0.82\n",
      "[1768857241] Step 393/1212: training loss=0.72\n",
      "[1768857241] Step 394/1212: training loss=0.80\n",
      "[1768857247] Step 395/1212: training loss=1.09\n",
      "[1768857247] Step 396/1212: training loss=0.72\n",
      "[1768857249] Step 397/1212: training loss=1.13\n",
      "[1768857249] Step 398/1212: training loss=0.93\n",
      "[1768857249] Step 399/1212: training loss=0.75\n",
      "[1768857251] Step 400/1212: training loss=0.72\n",
      "[1768857251] Step 401/1212: training loss=0.58\n",
      "[1768857253] Step 402/1212: training loss=0.72\n",
      "[1768857253] Step 403/1212: training loss=0.84\n",
      "[1768857253] Step 404/1212: training loss=0.88\n",
      "[1768857259] Step 405/1212: training loss=1.52\n",
      "[1768857259] Step 406/1212: training loss=0.66\n",
      "[1768857259] Step 407/1212: training loss=0.95\n",
      "[1768857261] Step 408/1212: training loss=0.79\n",
      "[1768857261] Step 409/1212: training loss=1.20\n",
      "[1768857261] Step 410/1212: training loss=0.97\n",
      "[1768857263] Step 411/1212: training loss=0.57\n",
      "[1768857263] Step 412/1212: training loss=1.52\n",
      "[1768857266] Step 413/1212: training loss=0.65\n",
      "[1768857266] Step 414/1212: training loss=0.75\n",
      "[1768857268] Step 415/1212: training loss=0.69\n",
      "[1768857268] Step 416/1212: training loss=1.10\n",
      "[1768857270] Step 417/1212: training loss=0.71\n",
      "[1768857270] Step 418/1212: training loss=0.57\n",
      "[1768857270] Step 419/1212: training loss=0.40\n",
      "[1768857272] Step 420/1212: training loss=0.95\n",
      "[1768857272] Step 421/1212: training loss=0.75\n",
      "[1768857274] Step 422/1212: training loss=0.90\n",
      "[1768857274] Step 423/1212: training loss=0.76\n",
      "[1768857274] Step 424/1212: training loss=0.46\n",
      "[1768857276] Step 425/1212: training loss=0.81\n",
      "[1768857276] Step 426/1212: training loss=0.46\n",
      "[1768857276] Step 427/1212: training loss=0.96\n",
      "[1768857278] Step 428/1212: training loss=0.79\n",
      "[1768857278] Step 429/1212: training loss=0.91\n",
      "[1768857280] Step 430/1212: training loss=0.94\n",
      "[1768857280] Step 431/1212: training loss=0.62\n",
      "[1768857280] Step 432/1212: training loss=0.67\n",
      "[1768857282] Step 433/1212: training loss=0.59\n",
      "[1768857282] Step 434/1212: training loss=0.44\n",
      "[1768857282] Step 435/1212: training loss=0.83\n",
      "[1768857284] Step 436/1212: training loss=0.58\n",
      "[1768857284] Step 437/1212: training loss=0.55\n",
      "[1768857284] Step 438/1212: training loss=0.50\n",
      "[1768857286] Step 439/1212: training loss=1.28\n",
      "[1768857288] Step 440/1212: training loss=0.47\n",
      "[1768857288] Step 441/1212: training loss=0.68\n",
      "[1768857290] Step 442/1212: training loss=0.98\n",
      "[1768857292] Step 443/1212: training loss=0.82\n",
      "[1768857292] Step 444/1212: training loss=0.74\n",
      "[1768857294] Step 445/1212: training loss=0.73\n",
      "[1768857294] Step 446/1212: training loss=0.66\n",
      "[1768857294] Step 447/1212: training loss=0.49\n",
      "[1768857296] Step 448/1212: training loss=0.23\n",
      "[1768857296] Step 449/1212: training loss=0.65\n",
      "[1768857298] Step 450/1212: training loss=1.00\n",
      "[1768857300] Step 451/1212: training loss=0.68\n",
      "[1768857300] Step 452/1212: training loss=0.73\n",
      "[1768857300] Step 453/1212: training loss=0.53\n",
      "[1768857302] Step 454/1212: training loss=0.65\n",
      "[1768857302] Step 455/1212: training loss=1.06\n",
      "[1768857302] Step 456/1212: training loss=0.80\n",
      "[1768857304] Step 457/1212: training loss=0.84\n",
      "[1768857304] Step 458/1212: training loss=0.70\n",
      "[1768857304] Step 459/1212: training loss=0.52\n",
      "[1768857306] Step 460/1212: training loss=0.68\n",
      "[1768857306] Step 461/1212: training loss=0.45\n",
      "[1768857308] Step 462/1212: training loss=1.02\n",
      "[1768857311] Step 463/1212: training loss=0.56\n",
      "[1768857311] Step 464/1212: training loss=0.79\n",
      "[1768857311] Step 465/1212: training loss=0.79\n",
      "[1768857313] Step 466/1212: training loss=0.84\n",
      "[1768857317] Step 467/1212: training loss=0.98\n",
      "[1768857317] Step 468/1212: training loss=1.05\n",
      "[1768857317] Step 469/1212: training loss=0.94\n",
      "[1768857319] Step 470/1212: training loss=0.48\n",
      "[1768857319] Step 471/1212: training loss=1.80\n",
      "[1768857319] Step 472/1212: training loss=1.10\n",
      "[1768857321] Step 473/1212: training loss=0.79\n",
      "[1768857321] Step 474/1212: training loss=0.82\n",
      "[1768857323] Step 475/1212: training loss=0.83\n",
      "[1768857323] Step 476/1212: training loss=0.81\n",
      "[1768857323] Step 477/1212: training loss=0.52\n",
      "[1768857325] Step 478/1212: training loss=0.72\n",
      "[1768857325] Step 479/1212: training loss=0.69\n",
      "[1768857325] Step 480/1212: training loss=0.40\n",
      "[1768857327] Step 481/1212: training loss=1.01\n",
      "[1768857329] Step 482/1212: training loss=0.82\n",
      "[1768857329] Step 483/1212: training loss=1.06\n",
      "[1768857331] Step 484/1212: training loss=1.17\n",
      "[1768857331] Step 485/1212: training loss=0.76\n",
      "[1768857333] Step 486/1212: training loss=0.52\n",
      "[1768857333] Step 487/1212: training loss=0.96\n",
      "[1768857333] Step 488/1212: training loss=0.65\n",
      "[1768857335] Step 489/1212: training loss=0.58\n",
      "[1768857335] Step 490/1212: training loss=0.86\n",
      "[1768857335] Step 491/1212: training loss=0.97\n",
      "[1768857337] Step 492/1212: training loss=0.38\n",
      "[1768857337] Step 493/1212: training loss=0.48\n",
      "[1768857337] Step 494/1212: training loss=0.72\n",
      "[1768857339] Step 495/1212: training loss=1.25\n",
      "[1768857339] Step 496/1212: training loss=0.84\n",
      "[1768857341] Step 497/1212: training loss=0.68\n",
      "[1768857341] Step 498/1212: training loss=0.91\n",
      "[1768857343] Step 499/1212: training loss=0.96\n",
      "[1768857343] Step 500/1212: training loss=0.69\n",
      "[1768857345] Step 501/1212: training loss=0.45\n",
      "[1768857345] Step 502/1212: training loss=0.79\n",
      "[1768857345] Step 503/1212: training loss=0.62\n",
      "[1768857347] Step 504/1212: training loss=0.60\n",
      "[1768857347] Step 505/1212: training loss=1.06\n",
      "[1768857347] Step 506/1212: training loss=0.79\n",
      "[1768857349] Step 507/1212: training loss=0.43\n",
      "[1768857349] Step 508/1212: training loss=0.74\n",
      "[1768857351] Step 509/1212: training loss=0.96\n",
      "[1768857351] Step 510/1212: training loss=0.74\n",
      "[1768857351] Step 511/1212: training loss=0.50\n",
      "[1768857356] Step 512/1212: training loss=0.85\n",
      "[1768857356] Step 513/1212: training loss=0.67\n",
      "[1768857356] Step 514/1212: training loss=0.34\n",
      "[1768857358] Step 515/1212: training loss=0.65\n",
      "[1768857358] Step 516/1212: training loss=0.50\n",
      "[1768857358] Step 517/1212: training loss=0.77\n",
      "[1768857360] Step 518/1212: training loss=0.77\n",
      "[1768857360] Step 519/1212: training loss=0.73\n",
      "[1768857360] Step 520/1212: training loss=0.60\n",
      "[1768857366] Step 521/1212: training loss=0.72\n",
      "[1768857366] Step 522/1212: training loss=0.44\n",
      "[1768857366] Step 523/1212: training loss=0.70\n",
      "[1768857368] Step 524/1212: training loss=0.97\n",
      "[1768857368] Step 525/1212: training loss=0.56\n",
      "[1768857370] Step 526/1212: training loss=0.85\n",
      "[1768857370] Step 527/1212: training loss=0.79\n",
      "[1768857372] Step 528/1212: training loss=0.97\n",
      "[1768857372] Step 529/1212: training loss=0.64\n",
      "[1768857372] Step 530/1212: training loss=0.95\n",
      "[1768857374] Step 531/1212: training loss=0.56\n",
      "[1768857374] Step 532/1212: training loss=0.74\n",
      "[1768857376] Step 533/1212: training loss=0.48\n",
      "[1768857376] Step 534/1212: training loss=0.70\n",
      "[1768857376] Step 535/1212: training loss=0.58\n",
      "[1768857378] Step 536/1212: training loss=0.47\n",
      "[1768857378] Step 537/1212: training loss=0.78\n",
      "[1768857380] Step 538/1212: training loss=1.04\n",
      "[1768857380] Step 539/1212: training loss=0.60\n",
      "[1768857380] Step 540/1212: training loss=0.52\n",
      "[1768857382] Step 541/1212: training loss=0.81\n",
      "[1768857382] Step 542/1212: training loss=0.65\n",
      "[1768857384] Step 543/1212: training loss=0.76\n",
      "[1768857384] Step 544/1212: training loss=0.62\n",
      "[1768857384] Step 545/1212: training loss=0.61\n",
      "[1768857386] Step 546/1212: training loss=0.73\n",
      "[1768857386] Step 547/1212: training loss=0.84\n",
      "[1768857386] Step 548/1212: training loss=1.25\n",
      "[1768857388] Step 549/1212: training loss=0.80\n",
      "[1768857388] Step 550/1212: training loss=0.61\n",
      "[1768857390] Step 551/1212: training loss=0.77\n",
      "[1768857390] Step 552/1212: training loss=0.55\n",
      "[1768857392] Step 553/1212: training loss=0.68\n",
      "[1768857392] Step 554/1212: training loss=0.87\n",
      "[1768857392] Step 555/1212: training loss=0.68\n",
      "[1768857394] Step 556/1212: training loss=0.68\n",
      "[1768857394] Step 557/1212: training loss=0.52\n",
      "[1768857394] Step 558/1212: training loss=0.62\n",
      "[1768857396] Step 559/1212: training loss=0.85\n",
      "[1768857396] Step 560/1212: training loss=0.57\n",
      "[1768857399] Step 561/1212: training loss=1.49\n",
      "[1768857399] Step 562/1212: training loss=0.91\n",
      "[1768857399] Step 563/1212: training loss=0.79\n",
      "[1768857401] Step 564/1212: training loss=0.51\n",
      "[1768857401] Step 565/1212: training loss=0.83\n",
      "[1768857401] Step 566/1212: training loss=0.58\n",
      "[1768857403] Step 567/1212: training loss=0.50\n",
      "[1768857405] Step 568/1212: training loss=0.84\n",
      "[1768857405] Step 569/1212: training loss=0.50\n",
      "[1768857407] Step 570/1212: training loss=0.49\n",
      "[1768857407] Step 571/1212: training loss=0.73\n",
      "[1768857407] Step 572/1212: training loss=0.54\n",
      "[1768857409] Step 573/1212: training loss=0.81\n",
      "[1768857411] Step 574/1212: training loss=0.74\n",
      "[1768857411] Step 575/1212: training loss=1.05\n",
      "[1768857415] Step 576/1212: training loss=0.80\n",
      "[1768857415] Step 577/1212: training loss=0.57\n",
      "[1768857417] Step 578/1212: training loss=0.62\n",
      "[1768857417] Step 579/1212: training loss=0.75\n",
      "[1768857417] Step 580/1212: training loss=0.77\n",
      "[1768857419] Step 581/1212: training loss=0.99\n",
      "[1768857421] Step 582/1212: training loss=0.93\n",
      "[1768857421] Step 583/1212: training loss=0.46\n",
      "[1768857421] Step 584/1212: training loss=0.62\n",
      "[1768857424] Step 585/1212: training loss=0.65\n",
      "[1768857426] Step 586/1212: training loss=1.24\n",
      "[1768857428] Step 587/1212: training loss=0.81\n",
      "[1768857430] Step 588/1212: training loss=1.19\n",
      "[1768857432] Step 589/1212: training loss=0.60\n",
      "[1768857434] Step 590/1212: training loss=1.01\n",
      "[1768857434] Step 591/1212: training loss=0.65\n",
      "[1768857434] Step 592/1212: training loss=0.49\n",
      "[1768857436] Step 593/1212: training loss=0.93\n",
      "[1768857436] Step 594/1212: training loss=0.88\n",
      "[1768857438] Step 595/1212: training loss=0.38\n",
      "[1768857438] Step 596/1212: training loss=0.82\n",
      "[1768857440] Step 597/1212: training loss=0.77\n",
      "[1768857440] Step 598/1212: training loss=0.94\n",
      "[1768857440] Step 599/1212: training loss=0.98\n",
      "[1768857442] Step 600/1212: training loss=0.85\n",
      "[1768857442] Step 601/1212: training loss=0.53\n",
      "[1768857444] Step 602/1212: training loss=0.38\n",
      "[1768857444] Step 603/1212: training loss=0.54\n",
      "[1768857444] Step 604/1212: training loss=0.77\n",
      "[1768857446] Step 605/1212: training loss=0.91\n",
      "[1768857446] Step 606/1212: training loss=0.51\n",
      "[1768857448] Step 607/1212: training loss=0.98\n",
      "[1768857448] Step 608/1212: training loss=0.69\n",
      "[1768857448] Step 609/1212: training loss=0.58\n",
      "[1768857450] Step 610/1212: training loss=0.62\n",
      "[1768857450] Step 611/1212: training loss=0.98\n",
      "[1768857452] Step 612/1212: training loss=1.20\n",
      "[1768857452] Step 613/1212: training loss=0.76\n",
      "[1768857454] Step 614/1212: training loss=0.89\n",
      "[1768857456] Step 615/1212: training loss=0.68\n",
      "[1768857456] Step 616/1212: training loss=0.95\n",
      "[1768857458] Step 617/1212: training loss=0.59\n",
      "[1768857458] Step 618/1212: training loss=0.93\n",
      "[1768857458] Step 619/1212: training loss=0.85\n",
      "[1768857460] Step 620/1212: training loss=0.66\n",
      "[1768857460] Step 621/1212: training loss=0.55\n",
      "[1768857462] Step 622/1212: training loss=0.80\n",
      "[1768857462] Step 623/1212: training loss=0.62\n",
      "[1768857462] Step 624/1212: training loss=0.84\n",
      "[1768857464] Step 625/1212: training loss=0.68\n",
      "[1768857465] Step 626/1212: training loss=0.74\n",
      "[1768857465] Step 627/1212: training loss=0.71\n",
      "[1768857467] Step 628/1212: training loss=0.81\n",
      "[1768857467] Step 629/1212: training loss=0.88\n",
      "[1768857467] Step 630/1212: training loss=0.81\n",
      "[1768857469] Step 631/1212: training loss=0.68\n",
      "[1768857469] Step 632/1212: training loss=0.62\n",
      "[1768857471] Step 633/1212: training loss=0.65\n",
      "[1768857471] Step 634/1212: training loss=0.85\n",
      "[1768857473] Step 635/1212: training loss=0.67\n",
      "[1768857473] Step 636/1212: training loss=0.90\n",
      "[1768857475] Step 637/1212: training loss=0.89\n",
      "[1768857475] Step 638/1212: training loss=0.70\n",
      "[1768857475] Step 639/1212: training loss=0.64\n",
      "[1768857475] Step 640/1212: training loss=0.73\n",
      "[1768857477] Step 641/1212: training loss=0.40\n",
      "[1768857477] Step 642/1212: training loss=0.85\n",
      "[1768857479] Step 643/1212: training loss=0.75\n",
      "[1768857479] Step 644/1212: training loss=0.87\n",
      "[1768857481] Step 645/1212: training loss=0.70\n",
      "[1768857481] Step 646/1212: training loss=0.42\n",
      "[1768857483] Step 647/1212: training loss=0.82\n",
      "[1768857483] Step 648/1212: training loss=0.52\n",
      "[1768857483] Step 649/1212: training loss=0.64\n",
      "[1768857485] Step 650/1212: training loss=1.21\n",
      "[1768857485] Step 651/1212: training loss=1.27\n",
      "[1768857487] Step 652/1212: training loss=0.70\n",
      "[1768857487] Step 653/1212: training loss=0.46\n",
      "[1768857489] Step 654/1212: training loss=0.42\n",
      "[1768857489] Step 655/1212: training loss=0.69\n",
      "[1768857489] Step 656/1212: training loss=0.74\n",
      "[1768857489] Step 657/1212: training loss=0.52\n",
      "[1768857491] Step 658/1212: training loss=0.79\n",
      "[1768857491] Step 659/1212: training loss=1.14\n",
      "[1768857493] Step 660/1212: training loss=0.78\n",
      "[1768857493] Step 661/1212: training loss=0.69\n",
      "[1768857495] Step 662/1212: training loss=0.94\n",
      "[1768857497] Step 663/1212: training loss=0.91\n",
      "[1768857497] Step 664/1212: training loss=1.14\n",
      "[1768857497] Step 665/1212: training loss=0.96\n",
      "[1768857499] Step 666/1212: training loss=0.66\n",
      "[1768857499] Step 667/1212: training loss=0.93\n",
      "[1768857499] Step 668/1212: training loss=0.78\n",
      "[1768857501] Step 669/1212: training loss=1.00\n",
      "[1768857501] Step 670/1212: training loss=0.60\n",
      "[1768857503] Step 671/1212: training loss=0.85\n",
      "[1768857503] Step 672/1212: training loss=0.91\n",
      "[1768857503] Step 673/1212: training loss=0.95\n",
      "[1768857505] Step 674/1212: training loss=0.49\n",
      "[1768857505] Step 675/1212: training loss=0.84\n",
      "[1768857505] Step 676/1212: training loss=0.44\n",
      "[1768857507] Step 677/1212: training loss=0.49\n",
      "[1768857507] Step 678/1212: training loss=0.61\n",
      "[1768857507] Step 679/1212: training loss=0.93\n",
      "[1768857510] Step 680/1212: training loss=0.73\n",
      "[1768857510] Step 681/1212: training loss=1.09\n",
      "[1768857510] Step 682/1212: training loss=0.76\n",
      "[1768857512] Step 683/1212: training loss=0.76\n",
      "[1768857512] Step 684/1212: training loss=0.55\n",
      "[1768857512] Step 685/1212: training loss=0.71\n",
      "[1768857514] Step 686/1212: training loss=0.65\n",
      "[1768857514] Step 687/1212: training loss=0.94\n",
      "[1768857514] Step 688/1212: training loss=0.36\n",
      "[1768857516] Step 689/1212: training loss=0.78\n",
      "[1768857516] Step 690/1212: training loss=1.18\n",
      "[1768857518] Step 691/1212: training loss=0.91\n",
      "[1768857518] Step 692/1212: training loss=0.63\n",
      "[1768857518] Step 693/1212: training loss=0.47\n",
      "[1768857520] Step 694/1212: training loss=0.77\n",
      "[1768857520] Step 695/1212: training loss=0.72\n",
      "[1768857520] Step 696/1212: training loss=0.67\n",
      "[1768857522] Step 697/1212: training loss=0.63\n",
      "[1768857522] Step 698/1212: training loss=0.76\n",
      "[1768857524] Step 699/1212: training loss=0.78\n",
      "[1768857524] Step 700/1212: training loss=0.48\n",
      "[1768857528] Step 701/1212: training loss=0.94\n",
      "[1768857528] Step 702/1212: training loss=0.71\n",
      "[1768857528] Step 703/1212: training loss=0.59\n",
      "[1768857530] Step 704/1212: training loss=0.69\n",
      "[1768857532] Step 705/1212: training loss=0.90\n",
      "[1768857532] Step 706/1212: training loss=0.80\n",
      "[1768857532] Step 707/1212: training loss=1.48\n",
      "[1768857534] Step 708/1212: training loss=0.64\n",
      "[1768857534] Step 709/1212: training loss=0.51\n",
      "[1768857534] Step 710/1212: training loss=0.81\n",
      "[1768857536] Step 711/1212: training loss=0.54\n",
      "[1768857536] Step 712/1212: training loss=1.43\n",
      "[1768857536] Step 713/1212: training loss=0.33\n",
      "[1768857538] Step 714/1212: training loss=0.65\n",
      "[1768857538] Step 715/1212: training loss=1.18\n",
      "[1768857538] Step 716/1212: training loss=0.65\n",
      "[1768857540] Step 717/1212: training loss=0.72\n",
      "[1768857540] Step 718/1212: training loss=0.54\n",
      "[1768857542] Step 719/1212: training loss=0.72\n",
      "[1768857542] Step 720/1212: training loss=0.51\n",
      "[1768857542] Step 721/1212: training loss=0.57\n",
      "[1768857544] Step 722/1212: training loss=0.98\n",
      "[1768857544] Step 723/1212: training loss=0.74\n",
      "[1768857546] Step 724/1212: training loss=0.65\n",
      "[1768857546] Step 725/1212: training loss=0.91\n",
      "[1768857546] Step 726/1212: training loss=0.87\n",
      "[1768857548] Step 727/1212: training loss=0.51\n",
      "[1768857548] Step 728/1212: training loss=0.46\n",
      "[1768857548] Step 729/1212: training loss=0.72\n",
      "[1768857550] Step 730/1212: training loss=0.66\n",
      "[1768857552] Step 731/1212: training loss=0.45\n",
      "[1768857552] Step 732/1212: training loss=0.52\n",
      "[1768857552] Step 733/1212: training loss=0.79\n",
      "[1768857554] Step 734/1212: training loss=0.82\n",
      "[1768857555] Step 735/1212: training loss=0.97\n",
      "[1768857557] Step 736/1212: training loss=0.84\n",
      "[1768857557] Step 737/1212: training loss=0.84\n",
      "[1768857557] Step 738/1212: training loss=0.80\n",
      "[1768857559] Step 739/1212: training loss=0.99\n",
      "[1768857559] Step 740/1212: training loss=0.99\n",
      "[1768857561] Step 741/1212: training loss=0.44\n",
      "[1768857561] Step 742/1212: training loss=0.39\n",
      "[1768857563] Step 743/1212: training loss=1.18\n",
      "[1768857563] Step 744/1212: training loss=0.48\n",
      "[1768857565] Step 745/1212: training loss=0.96\n",
      "[1768857565] Step 746/1212: training loss=0.58\n",
      "[1768857580] Step 747/1212: training loss=0.56\n",
      "[1768857580] Step 748/1212: training loss=0.76\n",
      "[1768857582] Step 749/1212: training loss=0.57\n",
      "[1768857582] Step 750/1212: training loss=0.73\n",
      "[1768857582] Step 751/1212: training loss=0.42\n",
      "[1768857584] Step 752/1212: training loss=0.96\n",
      "[1768857584] Step 753/1212: training loss=0.49\n",
      "[1768857586] Step 754/1212: training loss=0.69\n",
      "[1768857586] Step 755/1212: training loss=0.76\n",
      "[1768857588] Step 756/1212: training loss=0.82\n",
      "[1768857588] Step 757/1212: training loss=0.62\n",
      "[1768857590] Step 758/1212: training loss=0.75\n",
      "[1768857590] Step 759/1212: training loss=0.90\n",
      "[1768857590] Step 760/1212: training loss=1.14\n",
      "[1768857592] Step 761/1212: training loss=0.42\n",
      "[1768857592] Step 762/1212: training loss=0.64\n",
      "[1768857592] Step 763/1212: training loss=0.43\n",
      "[1768857594] Step 764/1212: training loss=0.94\n",
      "[1768857594] Step 765/1212: training loss=0.74\n",
      "[1768857596] Step 766/1212: training loss=0.53\n",
      "[1768857596] Step 767/1212: training loss=0.66\n",
      "[1768857600] Step 768/1212: training loss=0.88\n",
      "[1768857600] Step 769/1212: training loss=0.63\n",
      "[1768857602] Step 770/1212: training loss=0.67\n",
      "[1768857602] Step 771/1212: training loss=0.79\n",
      "[1768857602] Step 772/1212: training loss=0.71\n",
      "[1768857604] Step 773/1212: training loss=0.65\n",
      "[1768857604] Step 774/1212: training loss=0.94\n",
      "[1768857604] Step 775/1212: training loss=0.90\n",
      "[1768857606] Step 776/1212: training loss=0.74\n",
      "[1768857606] Step 777/1212: training loss=0.54\n",
      "[1768857608] Step 778/1212: training loss=1.62\n",
      "[1768857608] Step 779/1212: training loss=0.97\n",
      "[1768857610] Step 780/1212: training loss=0.90\n",
      "[1768857610] Step 781/1212: training loss=0.88\n",
      "[1768857612] Step 782/1212: training loss=0.95\n",
      "[1768857612] Step 783/1212: training loss=0.91\n",
      "[1768857612] Step 784/1212: training loss=0.95\n",
      "[1768857614] Step 785/1212: training loss=1.01\n",
      "[1768857614] Step 786/1212: training loss=0.79\n",
      "[1768857618] Step 787/1212: training loss=0.58\n",
      "[1768857618] Step 788/1212: training loss=0.89\n",
      "[1768857620] Step 789/1212: training loss=0.94\n",
      "[1768857620] Step 790/1212: training loss=0.73\n",
      "[1768857620] Step 791/1212: training loss=0.69\n",
      "[1768857622] Step 792/1212: training loss=0.83\n",
      "[1768857622] Step 793/1212: training loss=0.85\n",
      "[1768857622] Step 794/1212: training loss=0.39\n",
      "[1768857624] Step 795/1212: training loss=0.68\n",
      "[1768857624] Step 796/1212: training loss=0.63\n",
      "[1768857627] Step 797/1212: training loss=1.38\n",
      "[1768857629] Step 798/1212: training loss=0.53\n",
      "[1768857629] Step 799/1212: training loss=0.52\n",
      "[1768857629] Step 800/1212: training loss=0.51\n",
      "[1768857631] Step 801/1212: training loss=0.71\n",
      "[1768857631] Step 802/1212: training loss=0.82\n",
      "[1768857633] Step 803/1212: training loss=0.83\n",
      "[1768857633] Step 804/1212: training loss=0.78\n",
      "[1768857633] Step 805/1212: training loss=0.64\n",
      "[1768857636] Step 806/1212: training loss=0.65\n",
      "[1768857636] Step 807/1212: training loss=1.04\n",
      "[1768857636] Step 808/1212: training loss=0.82\n",
      "[1768857639] Step 809/1212: training loss=0.89\n",
      "[1768857641] Step 810/1212: training loss=0.86\n",
      "[1768857641] Step 811/1212: training loss=0.81\n",
      "[1768857643] Step 812/1212: training loss=0.43\n",
      "[1768857643] Step 813/1212: training loss=0.73\n",
      "[1768857643] Step 814/1212: training loss=0.57\n",
      "[1768857645] Step 815/1212: training loss=0.51\n",
      "[1768857645] Step 816/1212: training loss=0.77\n",
      "[1768857647] Step 817/1212: training loss=0.29\n",
      "[1768857649] Step 818/1212: training loss=0.65\n",
      "[1768857649] Step 819/1212: training loss=0.60\n",
      "[1768857649] Step 820/1212: training loss=0.59\n",
      "[1768857651] Step 821/1212: training loss=0.73\n",
      "[1768857651] Step 822/1212: training loss=0.59\n",
      "[1768857651] Step 823/1212: training loss=0.61\n",
      "[1768857653] Step 824/1212: training loss=0.66\n",
      "[1768857653] Step 825/1212: training loss=0.56\n",
      "[1768857655] Step 826/1212: training loss=0.53\n",
      "[1768857655] Step 827/1212: training loss=0.57\n",
      "[1768857655] Step 828/1212: training loss=0.38\n",
      "[1768857657] Step 829/1212: training loss=0.72\n",
      "[1768857657] Step 830/1212: training loss=0.35\n",
      "[1768857659] Step 831/1212: training loss=0.61\n",
      "[1768857659] Step 832/1212: training loss=0.87\n",
      "[1768857661] Step 833/1212: training loss=0.67\n",
      "[1768857661] Step 834/1212: training loss=0.62\n",
      "[1768857663] Step 835/1212: training loss=0.43\n",
      "[1768857663] Step 836/1212: training loss=0.90\n",
      "[1768857665] Step 837/1212: training loss=1.01\n",
      "[1768857665] Step 838/1212: training loss=0.92\n",
      "[1768857665] Step 839/1212: training loss=0.66\n",
      "[1768857667] Step 840/1212: training loss=0.49\n",
      "[1768857667] Step 841/1212: training loss=0.79\n",
      "[1768857667] Step 842/1212: training loss=0.43\n",
      "[1768857669] Step 843/1212: training loss=0.67\n",
      "[1768857669] Step 844/1212: training loss=0.61\n",
      "[1768857669] Step 845/1212: training loss=0.81\n",
      "[1768857673] Step 846/1212: training loss=0.29\n",
      "[1768857673] Step 847/1212: training loss=0.83\n",
      "[1768857673] Step 848/1212: training loss=0.70\n",
      "[1768857676] Step 849/1212: training loss=0.37\n",
      "[1768857676] Step 850/1212: training loss=0.91\n",
      "[1768857678] Step 851/1212: training loss=0.77\n",
      "[1768857678] Step 852/1212: training loss=0.55\n",
      "[1768857680] Step 853/1212: training loss=0.65\n",
      "[1768857680] Step 854/1212: training loss=0.67\n",
      "[1768857682] Step 855/1212: training loss=0.43\n",
      "[1768857682] Step 856/1212: training loss=0.47\n",
      "[1768857682] Step 857/1212: training loss=0.45\n",
      "[1768857682] Step 858/1212: training loss=0.40\n",
      "[1768857684] Step 859/1212: training loss=0.66\n",
      "[1768857686] Step 860/1212: training loss=0.91\n",
      "[1768857686] Step 861/1212: training loss=0.58\n",
      "[1768857686] Step 862/1212: training loss=0.49\n",
      "[1768857688] Step 863/1212: training loss=0.83\n",
      "[1768857690] Step 864/1212: training loss=0.63\n",
      "[1768857690] Step 865/1212: training loss=0.74\n",
      "[1768857692] Step 866/1212: training loss=0.56\n",
      "[1768857692] Step 867/1212: training loss=0.56\n",
      "[1768857692] Step 868/1212: training loss=0.48\n",
      "[1768857696] Step 869/1212: training loss=0.73\n",
      "[1768857696] Step 870/1212: training loss=0.73\n",
      "[1768857696] Step 871/1212: training loss=0.75\n",
      "[1768857698] Step 872/1212: training loss=0.40\n",
      "[1768857698] Step 873/1212: training loss=0.44\n",
      "[1768857698] Step 874/1212: training loss=0.60\n",
      "[1768857700] Step 875/1212: training loss=0.81\n",
      "[1768857700] Step 876/1212: training loss=0.31\n",
      "[1768857702] Step 877/1212: training loss=1.01\n",
      "[1768857702] Step 878/1212: training loss=1.07\n",
      "[1768857704] Step 879/1212: training loss=0.28\n",
      "[1768857704] Step 880/1212: training loss=0.58\n",
      "[1768857704] Step 881/1212: training loss=0.61\n",
      "[1768857706] Step 882/1212: training loss=0.92\n",
      "[1768857706] Step 883/1212: training loss=0.73\n",
      "[1768857706] Step 884/1212: training loss=1.00\n",
      "[1768857708] Step 885/1212: training loss=0.69\n",
      "[1768857708] Step 886/1212: training loss=0.35\n",
      "[1768857708] Step 887/1212: training loss=0.41\n",
      "[1768857710] Step 888/1212: training loss=0.61\n",
      "[1768857710] Step 889/1212: training loss=0.58\n",
      "[1768857710] Step 890/1212: training loss=0.53\n",
      "[1768857712] Step 891/1212: training loss=0.53\n",
      "[1768857712] Step 892/1212: training loss=0.60\n",
      "[1768857712] Step 893/1212: training loss=0.69\n",
      "[1768857714] Step 894/1212: training loss=1.34\n",
      "[1768857716] Step 895/1212: training loss=1.09\n",
      "[1768857717] Step 896/1212: training loss=1.07\n",
      "[1768857719] Step 897/1212: training loss=0.68\n",
      "[1768857719] Step 898/1212: training loss=0.55\n",
      "[1768857721] Step 899/1212: training loss=0.54\n",
      "[1768857721] Step 900/1212: training loss=0.59\n",
      "[1768857723] Step 901/1212: training loss=0.71\n",
      "[1768857723] Step 902/1212: training loss=0.40\n",
      "[1768857725] Step 903/1212: training loss=0.76\n",
      "[1768857725] Step 904/1212: training loss=0.86\n",
      "[1768857725] Step 905/1212: training loss=1.04\n",
      "[1768857727] Step 906/1212: training loss=0.41\n",
      "[1768857727] Step 907/1212: training loss=1.11\n",
      "[1768857727] Step 908/1212: training loss=0.85\n",
      "[1768857733] Step 909/1212: training loss=1.19\n",
      "[1768857733] Step 910/1212: training loss=0.70\n",
      "[1768857733] Step 911/1212: training loss=0.69\n",
      "[1768857735] Step 912/1212: training loss=0.68\n",
      "[1768857737] Step 913/1212: training loss=0.49\n",
      "[1768857737] Step 914/1212: training loss=0.73\n",
      "[1768857737] Step 915/1212: training loss=0.84\n",
      "[1768857739] Step 916/1212: training loss=0.68\n",
      "[1768857739] Step 917/1212: training loss=0.63\n",
      "[1768857741] Step 918/1212: training loss=0.42\n",
      "[1768857741] Step 919/1212: training loss=0.32\n",
      "[1768857741] Step 920/1212: training loss=0.35\n",
      "[1768857743] Step 921/1212: training loss=0.34\n",
      "[1768857743] Step 922/1212: training loss=0.53\n",
      "[1768857743] Step 923/1212: training loss=0.59\n",
      "[1768857745] Step 924/1212: training loss=0.59\n",
      "[1768857745] Step 925/1212: training loss=0.79\n",
      "[1768857747] Step 926/1212: training loss=1.05\n",
      "[1768857747] Step 927/1212: training loss=0.53\n",
      "[1768857749] Step 928/1212: training loss=0.57\n",
      "[1768857749] Step 929/1212: training loss=0.65\n",
      "[1768857749] Step 930/1212: training loss=0.36\n",
      "[1768857751] Step 931/1212: training loss=0.33\n",
      "[1768857751] Step 932/1212: training loss=0.37\n",
      "[1768857751] Step 933/1212: training loss=0.54\n",
      "[1768857753] Step 934/1212: training loss=0.81\n",
      "[1768857753] Step 935/1212: training loss=0.64\n",
      "[1768857753] Step 936/1212: training loss=0.58\n",
      "[1768857755] Step 937/1212: training loss=0.44\n",
      "[1768857755] Step 938/1212: training loss=0.40\n",
      "[1768857755] Step 939/1212: training loss=0.63\n",
      "[1768857757] Step 940/1212: training loss=0.57\n",
      "[1768857760] Step 941/1212: training loss=0.83\n",
      "[1768857760] Step 942/1212: training loss=0.37\n",
      "[1768857762] Step 943/1212: training loss=0.44\n",
      "[1768857762] Step 944/1212: training loss=0.41\n",
      "[1768857762] Step 945/1212: training loss=0.49\n",
      "[1768857764] Step 946/1212: training loss=0.57\n",
      "[1768857764] Step 947/1212: training loss=0.72\n",
      "[1768857766] Step 948/1212: training loss=0.60\n",
      "[1768857766] Step 949/1212: training loss=0.56\n",
      "[1768857766] Step 950/1212: training loss=0.83\n",
      "[1768857768] Step 951/1212: training loss=1.24\n",
      "[1768857768] Step 952/1212: training loss=0.38\n",
      "[1768857770] Step 953/1212: training loss=0.80\n",
      "[1768857770] Step 954/1212: training loss=0.52\n",
      "[1768857770] Step 955/1212: training loss=1.10\n",
      "[1768857770] Step 956/1212: training loss=0.58\n",
      "[1768857772] Step 957/1212: training loss=0.64\n",
      "[1768857772] Step 958/1212: training loss=0.69\n",
      "[1768857772] Step 959/1212: training loss=0.84\n",
      "[1768857774] Step 960/1212: training loss=0.51\n",
      "[1768857774] Step 961/1212: training loss=0.64\n",
      "[1768857776] Step 962/1212: training loss=0.85\n",
      "[1768857776] Step 963/1212: training loss=0.81\n",
      "[1768857778] Step 964/1212: training loss=0.72\n",
      "[1768857778] Step 965/1212: training loss=0.62\n",
      "[1768857780] Step 966/1212: training loss=0.78\n",
      "[1768857780] Step 967/1212: training loss=0.50\n",
      "[1768857780] Step 968/1212: training loss=0.78\n",
      "[1768857782] Step 969/1212: training loss=0.46\n",
      "[1768857782] Step 970/1212: training loss=1.02\n",
      "[1768857782] Step 971/1212: training loss=0.45\n",
      "[1768857784] Step 972/1212: training loss=0.74\n",
      "[1768857786] Step 973/1212: training loss=0.58\n",
      "[1768857788] Step 974/1212: training loss=0.83\n",
      "[1768857788] Step 975/1212: training loss=0.78\n",
      "[1768857788] Step 976/1212: training loss=0.53\n",
      "[1768857790] Step 977/1212: training loss=0.47\n",
      "[1768857792] Step 978/1212: training loss=0.70\n",
      "[1768857792] Step 979/1212: training loss=0.46\n",
      "[1768857794] Step 980/1212: training loss=0.35\n",
      "[1768857794] Step 981/1212: training loss=0.38\n",
      "[1768857796] Step 982/1212: training loss=0.53\n",
      "[1768857796] Step 983/1212: training loss=0.60\n",
      "[1768857796] Step 984/1212: training loss=0.63\n",
      "[1768857798] Step 985/1212: training loss=1.39\n",
      "[1768857798] Step 986/1212: training loss=0.53\n",
      "[1768857801] Step 987/1212: training loss=0.56\n",
      "[1768857801] Step 988/1212: training loss=0.32\n",
      "[1768857805] Step 989/1212: training loss=0.62\n",
      "[1768857805] Step 990/1212: training loss=0.68\n",
      "[1768857807] Step 991/1212: training loss=0.91\n",
      "[1768857809] Step 992/1212: training loss=0.84\n",
      "[1768857809] Step 993/1212: training loss=0.71\n",
      "[1768857809] Step 994/1212: training loss=0.35\n",
      "[1768857811] Step 995/1212: training loss=0.58\n",
      "[1768857811] Step 996/1212: training loss=0.72\n",
      "[1768857811] Step 997/1212: training loss=0.57\n",
      "[1768857813] Step 998/1212: training loss=1.03\n",
      "[1768857813] Step 999/1212: training loss=0.50\n",
      "[1768857815] Step 1000/1212: training loss=0.66\n",
      "[1768857815] Step 1001/1212: training loss=0.66\n",
      "[1768857815] Step 1002/1212: training loss=0.67\n",
      "[1768857817] Step 1003/1212: training loss=0.48\n",
      "[1768857819] Step 1004/1212: training loss=0.78\n",
      "[1768857819] Step 1005/1212: training loss=1.31\n",
      "[1768857819] Step 1006/1212: training loss=0.85\n",
      "[1768857821] Step 1007/1212: training loss=0.72\n",
      "[1768857821] Step 1008/1212: training loss=0.30\n",
      "[1768857821] Step 1009/1212: training loss=0.36\n",
      "[1768857823] Step 1010/1212: training loss=0.47\n",
      "[1768857823] Step 1011/1212: training loss=0.82\n",
      "[1768857825] Step 1012/1212: training loss=0.66\n",
      "[1768857825] Step 1013/1212: training loss=0.63\n",
      "[1768857825] Step 1014/1212: training loss=0.63\n",
      "[1768857834] Step 1015/1212: training loss=0.31\n",
      "[1768857834] Step 1017/1212: training loss=0.63\n",
      "[1768857834] Step 1018/1212: training loss=0.61\n",
      "[1768857834] Step 1019/1212: training loss=0.91\n",
      "[1768857834] Step 1020/1212: training loss=0.42\n",
      "[1768857834] Step 1021/1212: training loss=0.70\n",
      "[1768857834] Step 1022/1212: training loss=0.48\n",
      "[1768857834] Step 1023/1212: training loss=0.86\n",
      "[1768857836] Step 1024/1212: training loss=0.50\n",
      "[1768857838] Step 1025/1212: training loss=0.58\n",
      "[1768857838] Step 1026/1212: training loss=0.65\n",
      "[1768857838] Step 1027/1212: training loss=0.71\n",
      "[1768857840] Step 1028/1212: training loss=0.53\n",
      "[1768857840] Step 1029/1212: training loss=0.84\n",
      "[1768857840] Step 1030/1212: training loss=0.65\n",
      "[1768857842] Step 1031/1212: training loss=0.73\n",
      "[1768857842] Step 1032/1212: training loss=0.49\n",
      "[1768857844] Step 1033/1212: training loss=0.79\n",
      "[1768857844] Step 1034/1212: training loss=0.63\n",
      "[1768857844] Step 1035/1212: training loss=0.60\n",
      "[1768857846] Step 1036/1212: training loss=0.71\n",
      "[1768857846] Step 1037/1212: training loss=1.04\n",
      "[1768857848] Step 1038/1212: training loss=0.41\n",
      "[1768857848] Step 1039/1212: training loss=0.30\n",
      "[1768857850] Step 1040/1212: training loss=0.74\n",
      "[1768857850] Step 1041/1212: training loss=0.65\n",
      "[1768857850] Step 1042/1212: training loss=0.78\n",
      "[1768857852] Step 1043/1212: training loss=0.65\n",
      "[1768857852] Step 1044/1212: training loss=0.60\n",
      "[1768857852] Step 1045/1212: training loss=0.63\n",
      "[1768857854] Step 1046/1212: training loss=0.62\n",
      "[1768857854] Step 1047/1212: training loss=0.50\n",
      "[1768857856] Step 1048/1212: training loss=0.78\n",
      "[1768857856] Step 1049/1212: training loss=0.43\n",
      "[1768857856] Step 1050/1212: training loss=0.44\n",
      "[1768857858] Step 1051/1212: training loss=0.45\n",
      "[1768857858] Step 1052/1212: training loss=0.51\n",
      "[1768857858] Step 1053/1212: training loss=0.66\n",
      "[1768857861] Step 1054/1212: training loss=0.37\n",
      "[1768857865] Step 1055/1212: training loss=1.10\n",
      "[1768857865] Step 1056/1212: training loss=0.73\n",
      "[1768857865] Step 1057/1212: training loss=0.58\n",
      "[1768857865] Step 1058/1212: training loss=0.83\n",
      "[1768857867] Step 1059/1212: training loss=0.82\n",
      "[1768857867] Step 1060/1212: training loss=0.71\n",
      "[1768857869] Step 1061/1212: training loss=0.51\n",
      "[1768857869] Step 1062/1212: training loss=0.48\n",
      "[1768857873] Step 1063/1212: training loss=0.76\n",
      "[1768857873] Step 1064/1212: training loss=0.56\n",
      "[1768857875] Step 1065/1212: training loss=0.41\n",
      "[1768857875] Step 1066/1212: training loss=0.74\n",
      "[1768857877] Step 1067/1212: training loss=0.56\n",
      "[1768857877] Step 1068/1212: training loss=0.57\n",
      "[1768857877] Step 1069/1212: training loss=0.50\n",
      "[1768857879] Step 1070/1212: training loss=0.67\n",
      "[1768857879] Step 1071/1212: training loss=0.59\n",
      "[1768857881] Step 1072/1212: training loss=0.49\n",
      "[1768857881] Step 1073/1212: training loss=0.88\n",
      "[1768857883] Step 1074/1212: training loss=0.66\n",
      "[1768857883] Step 1075/1212: training loss=0.65\n",
      "[1768857883] Step 1076/1212: training loss=0.66\n",
      "[1768857885] Step 1077/1212: training loss=0.85\n",
      "[1768857887] Step 1078/1212: training loss=0.72\n",
      "[1768857889] Step 1079/1212: training loss=1.23\n",
      "[1768857889] Step 1080/1212: training loss=0.80\n",
      "[1768857889] Step 1081/1212: training loss=0.74\n",
      "[1768857891] Step 1082/1212: training loss=0.53\n",
      "[1768857891] Step 1083/1212: training loss=0.61\n",
      "[1768857891] Step 1084/1212: training loss=0.69\n",
      "[1768857893] Step 1085/1212: training loss=0.72\n",
      "[1768857893] Step 1086/1212: training loss=0.61\n",
      "[1768857893] Step 1087/1212: training loss=0.90\n",
      "[1768857895] Step 1088/1212: training loss=0.83\n",
      "[1768857895] Step 1089/1212: training loss=0.56\n",
      "[1768857897] Step 1090/1212: training loss=0.61\n",
      "[1768857897] Step 1091/1212: training loss=0.51\n",
      "[1768857899] Step 1092/1212: training loss=0.74\n",
      "[1768857899] Step 1093/1212: training loss=0.78\n",
      "[1768857899] Step 1094/1212: training loss=0.74\n",
      "[1768857901] Step 1095/1212: training loss=0.34\n",
      "[1768857901] Step 1096/1212: training loss=0.58\n",
      "[1768857903] Step 1097/1212: training loss=0.41\n",
      "[1768857903] Step 1098/1212: training loss=0.70\n",
      "[1768857906] Step 1099/1212: training loss=0.46\n",
      "[1768857908] Step 1100/1212: training loss=1.37\n",
      "[1768857908] Step 1101/1212: training loss=0.29\n",
      "[1768857908] Step 1102/1212: training loss=0.84\n",
      "[1768857910] Step 1103/1212: training loss=0.43\n",
      "[1768857910] Step 1104/1212: training loss=0.66\n",
      "[1768857912] Step 1105/1212: training loss=0.57\n",
      "[1768857912] Step 1106/1212: training loss=0.41\n",
      "[1768857912] Step 1107/1212: training loss=0.58\n",
      "[1768857914] Step 1108/1212: training loss=0.85\n",
      "[1768857914] Step 1109/1212: training loss=0.48\n",
      "[1768857916] Step 1110/1212: training loss=0.48\n",
      "[1768857916] Step 1111/1212: training loss=0.33\n",
      "[1768857918] Step 1112/1212: training loss=0.42\n",
      "[1768857918] Step 1113/1212: training loss=0.53\n",
      "[1768857920] Step 1114/1212: training loss=0.92\n",
      "[1768857920] Step 1115/1212: training loss=0.66\n",
      "[1768857920] Step 1116/1212: training loss=0.60\n",
      "[1768857922] Step 1117/1212: training loss=0.43\n",
      "[1768857922] Step 1118/1212: training loss=0.51\n",
      "[1768857922] Step 1119/1212: training loss=0.52\n",
      "[1768857926] Step 1120/1212: training loss=0.82\n",
      "[1768857926] Step 1121/1212: training loss=0.53\n",
      "[1768857928] Step 1122/1212: training loss=0.80\n",
      "[1768857928] Step 1123/1212: training loss=0.39\n",
      "[1768857928] Step 1124/1212: training loss=0.54\n",
      "[1768857930] Step 1125/1212: training loss=1.02\n",
      "[1768857930] Step 1126/1212: training loss=0.33\n",
      "[1768857932] Step 1127/1212: training loss=0.68\n",
      "[1768857932] Step 1128/1212: training loss=0.85\n",
      "[1768857932] Step 1129/1212: training loss=0.48\n",
      "[1768857932] Step 1130/1212: training loss=0.38\n",
      "[1768857934] Step 1131/1212: training loss=0.65\n",
      "[1768857936] Step 1132/1212: training loss=0.74\n",
      "[1768857938] Step 1133/1212: training loss=0.42\n",
      "[1768857938] Step 1134/1212: training loss=0.67\n",
      "[1768857940] Step 1135/1212: training loss=1.06\n",
      "[1768857940] Step 1136/1212: training loss=0.75\n",
      "[1768857940] Step 1137/1212: training loss=0.42\n",
      "[1768857942] Step 1138/1212: training loss=0.51\n",
      "[1768857942] Step 1139/1212: training loss=0.67\n",
      "[1768857942] Step 1140/1212: training loss=0.60\n",
      "[1768857944] Step 1141/1212: training loss=0.76\n",
      "[1768857944] Step 1142/1212: training loss=0.37\n",
      "[1768857947] Step 1143/1212: training loss=0.50\n",
      "[1768857947] Step 1144/1212: training loss=0.82\n",
      "[1768857951] Step 1145/1212: training loss=0.67\n",
      "[1768857951] Step 1146/1212: training loss=0.85\n",
      "[1768857953] Step 1147/1212: training loss=0.91\n",
      "[1768857953] Step 1148/1212: training loss=0.42\n",
      "[1768857955] Step 1149/1212: training loss=0.65\n",
      "[1768857955] Step 1150/1212: training loss=0.58\n",
      "[1768857955] Step 1151/1212: training loss=0.57\n",
      "[1768857957] Step 1152/1212: training loss=0.83\n",
      "[1768857957] Step 1153/1212: training loss=0.63\n",
      "[1768857959] Step 1154/1212: training loss=0.51\n",
      "[1768857959] Step 1155/1212: training loss=0.63\n",
      "[1768857959] Step 1156/1212: training loss=0.53\n",
      "[1768857961] Step 1157/1212: training loss=0.70\n",
      "[1768857961] Step 1158/1212: training loss=0.70\n",
      "[1768857963] Step 1159/1212: training loss=0.39\n",
      "[1768857963] Step 1160/1212: training loss=0.46\n",
      "[1768857963] Step 1161/1212: training loss=0.52\n",
      "[1768857965] Step 1162/1212: training loss=0.57\n",
      "[1768857965] Step 1163/1212: training loss=0.69\n",
      "[1768857965] Step 1164/1212: training loss=0.50\n",
      "[1768857967] Step 1165/1212: training loss=0.64\n",
      "[1768857969] Step 1166/1212: training loss=0.49\n",
      "[1768857969] Step 1167/1212: training loss=0.69\n",
      "[1768857969] Step 1168/1212: training loss=0.36\n",
      "[1768857971] Step 1169/1212: training loss=0.67\n",
      "[1768857971] Step 1170/1212: training loss=0.75\n",
      "[1768857973] Step 1171/1212: training loss=0.32\n",
      "[1768857973] Step 1172/1212: training loss=0.24\n",
      "[1768857973] Step 1173/1212: training loss=0.39\n",
      "[1768857975] Step 1174/1212: training loss=0.38\n",
      "[1768857975] Step 1175/1212: training loss=0.75\n",
      "[1768857975] Step 1176/1212: training loss=0.56\n",
      "[1768857977] Step 1177/1212: training loss=0.57\n",
      "[1768857977] Step 1178/1212: training loss=0.46\n",
      "[1768857977] Step 1179/1212: training loss=0.50\n",
      "[1768857979] Step 1180/1212: training loss=0.43\n",
      "[1768857979] Step 1181/1212: training loss=0.83\n",
      "[1768857979] Step 1182/1212: training loss=0.68\n",
      "[1768857981] Step 1183/1212: training loss=0.57\n",
      "[1768857981] Step 1184/1212: training loss=0.54\n",
      "[1768857983] Step 1185/1212: training loss=0.43\n",
      "[1768857983] Step 1186/1212: training loss=0.58\n",
      "[1768857983] Step 1187/1212: training loss=1.58\n",
      "[1768857985] Step 1188/1212: training loss=0.68\n",
      "[1768857985] Step 1189/1212: training loss=0.67\n",
      "[1768857988] Step 1190/1212: training loss=0.73\n",
      "[1768857988] Step 1191/1212: training loss=0.43\n",
      "[1768857988] Step 1192/1212: training loss=0.66\n",
      "[1768857990] Step 1193/1212: training loss=1.37\n",
      "[1768857990] Step 1194/1212: training loss=0.65\n",
      "[1768857992] Step 1195/1212: training loss=0.43\n",
      "[1768857992] Step 1196/1212: training loss=0.87\n",
      "[1768857992] Step 1197/1212: training loss=0.32\n",
      "[1768857994] Step 1198/1212: training loss=0.65\n",
      "[1768857994] Step 1199/1212: training loss=0.80\n",
      "[1768857996] Step 1200/1212: training loss=0.87\n",
      "[1768857998] Step 1201/1212: training loss=0.80\n",
      "[1768857998] Step 1202/1212: training loss=0.50\n",
      "[1768857998] Step 1203/1212: training loss=0.15\n",
      "[1768857998] Step 1204/1212: training loss=0.49\n",
      "[1768858000] Step 1205/1212: training loss=0.91\n",
      "[1768858000] Step 1206/1212: training loss=0.58\n",
      "[1768858000] Step 1207/1212: training loss=0.66\n",
      "[1768858002] Step 1208/1212: training loss=0.87\n",
      "[1768858004] Step 1209/1212: training loss=0.35\n",
      "[1768858004] Step 1210/1212: training loss=0.62\n",
      "[1768858004] Step 1211/1212: training loss=0.71\n",
      "[1768858006] Step 1212/1212: training loss=0.56\n",
      "[1768858024] Checkpoint created at step 404\n",
      "[1768858024] Checkpoint created at step 808\n",
      "[1768858024] New fine-tuned model created\n",
      "[1768858024] Evaluating model against our usage policies\n",
      "[1768858748] Moderation checks for snapshot ft:gpt-4.1-nano-2025-04-14:tavily::Czr44ng2 passed.\n",
      "[1768858748] Usage policy evaluations completed, model is now enabled for sampling\n",
      "[1768858750] The job has successfully completed\n",
      "\n",
      "⚠ No loss data found in training events\n",
      "\n",
      "============================================================\n",
      "✓ Fine-tuning completed!\n",
      "Fine-tuned model: ft:gpt-4.1-nano-2025-04-14:tavily::Czr44ng2\n",
      "Trained tokens: 8,949,513\n",
      "Actual cost: $13.42\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ gpt-4.1-nano-2025-04-14 → ft:gpt-4.1-nano-2025-04-14:tavily::Czr44ng2\n",
      "\n",
      "\n",
      "======================================================================\n",
      "All fine-tuning jobs completed!\n",
      "======================================================================\n",
      "gpt-4.1-mini-2025-04-14\n",
      "  → ft:gpt-4.1-mini-2025-04-14:tavily::CzqO8otr\n",
      "gpt-4.1-nano-2025-04-14\n",
      "  → ft:gpt-4.1-nano-2025-04-14:tavily::Czr44ng2\n"
     ]
    }
   ],
   "source": [
    "from train.finetune import prepare_and_train\n",
    "\n",
    "# Fine-tune both models using the training data\n",
    "models_to_train = [\n",
    "    \"gpt-4.1-mini-2025-04-14\",\n",
    "    \"gpt-4.1-nano-2025-04-14\"\n",
    "]\n",
    "\n",
    "finetuned_models = {}\n",
    "\n",
    "for model in models_to_train:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Starting fine-tuning for {model}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    finetuned_model_id = prepare_and_train(\n",
    "        model=model,\n",
    "        train_json_path='data/goldstandard_train.json',\n",
    "        n_epochs=3\n",
    "    )\n",
    "    \n",
    "    finetuned_models[model] = finetuned_model_id\n",
    "    print(f\"\\n✓ {model} → {finetuned_model_id}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"All fine-tuning jobs completed!\")\n",
    "print(f\"{'='*70}\")\n",
    "for base, ft in finetuned_models.items():\n",
    "    print(f\"{base}\")\n",
    "    print(f\"  → {ft}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24339b5b",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "At this point I'll define a small benchmark of 20% of the validation set for model selection, for cost and time efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029538b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 588 items from gold standard validation set\n",
      "Using tokenizer: o200k_base\n",
      "\n",
      "Items under 250,000 tokens: 586\n",
      "Items over 250,000 tokens: 2\n",
      "\n",
      "Benchmark set: 58 items (all under token limit)\n",
      "\n",
      "✓ Benchmark set saved to data/goldstandard_validation_benchmark.json\n",
      "✓ All benchmarking examples are within the 250,000 token limit for fine-tuning\n",
      "\n",
      "Creating baseline subset with same URLs...\n",
      "Matched 58 baseline items\n",
      "✓ Baseline subset saved to data/baseline_validation_benchmark.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "PAGE_MAX_TOKENS = 250000 # Limit for >gpt-5.0 families, to ensure compatibility with the judge\n",
    "\n",
    "# Load the gold standard validation dataset\n",
    "with open('data/goldstandard_validation.json', 'r', encoding='utf-8') as f:\n",
    "    gold_standard = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(gold_standard)} items from gold standard validation set\")\n",
    "\n",
    "# Initialize tokenizer for token counting (using gpt-4.1 family)\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")  # gpt-4.1 uses same tokenizer as gpt-4o\n",
    "print(f\"Using tokenizer: {tokenizer.name}\")\n",
    "\n",
    "# Split data based on token count\n",
    "items_under_limit = []\n",
    "items_over_limit = []\n",
    "\n",
    "for item in gold_standard:\n",
    "    token_count = len(tokenizer.encode(item['markdown_content']))\n",
    "    if token_count <= PAGE_MAX_TOKENS:\n",
    "        items_under_limit.append(item)\n",
    "    else:\n",
    "        items_over_limit.append(item)\n",
    "\n",
    "print(f\"\\nItems under {PAGE_MAX_TOKENS:,} tokens: {len(items_under_limit)}\")\n",
    "print(f\"Items over {PAGE_MAX_TOKENS:,} tokens: {len(items_over_limit)}\")\n",
    "\n",
    "# Calculate target benchmarking size (10% of total validation set, ~60 items)\n",
    "target_size = len(gold_standard) // 10\n",
    "\n",
    "# Take up to 10% of total for benchmarking (only from items under limit)\n",
    "validation_data = items_under_limit[:target_size]\n",
    "\n",
    "print(f\"\\nBenchmark set: {len(validation_data)} items (all under token limit)\")\n",
    "\n",
    "# Save benchmark set\n",
    "bm_path = 'data/goldstandard_validation_benchmark.json'\n",
    "with open(bm_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(validation_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Benchmark set saved to {bm_path}\")\n",
    "print(f\"✓ All benchmarking examples are within the {PAGE_MAX_TOKENS:,} token limit for fine-tuning\")\n",
    "\n",
    "# Create baseline subset matching the same URLs\n",
    "print(\"\\nCreating baseline subset with same URLs...\")\n",
    "with open('data/baseline_1k.json', 'r', encoding='utf-8') as f:\n",
    "    baseline_full = json.load(f)\n",
    "\n",
    "# Extract URLs from validation_data\n",
    "benchmark_urls = {item['url'] for item in validation_data}\n",
    "\n",
    "# Filter baseline data to match benchmark URLs\n",
    "baseline_subset = [item for item in baseline_full['data'] if item['url'] in benchmark_urls]\n",
    "\n",
    "print(f\"Matched {len(baseline_subset)} baseline items\")\n",
    "\n",
    "# Save baseline subset\n",
    "baseline_subset_path = 'data/baseline_validation_benchmark.json'\n",
    "with open(baseline_subset_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(baseline_subset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Baseline subset saved to {baseline_subset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb13ef",
   "metadata": {},
   "source": [
    "Benchmark without Retry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6590b2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model 'gpt-5.2-2025-12-11' not recognized by tiktoken. Using 'gpt-5' tokenizer as fallback.\n",
      "\n",
      "======================================================================\n",
      "Model 1/10: Baseline\n",
      "======================================================================\n",
      "Loading baseline summaries from data/baseline_validation_benchmark.json\n",
      "✓ Filtered to 99 baseline summaries matching eval subset\n",
      "✓ Loaded 99 baseline summaries\n",
      "✓ Saved to data/inference_baseline.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/10] Judging: 100%|██████████| 99/99 [16:07<00:00,  9.78s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model 2/10: gpt-4o-mini\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/10] Inference: 100%|██████████| 99/99 [11:28<00:00,  6.96s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-4o-mini.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/10] Judging: 100%|██████████| 99/99 [14:59<00:00,  9.09s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model 3/10: gpt-4.1-2025-04-14\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/10] Inference: 100%|██████████| 99/99 [13:27<00:00,  8.15s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-4.1-2025-04-14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/10] Judging: 100%|██████████| 99/99 [16:41<00:00, 10.12s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model 4/10: gpt-4.1-mini-2025-04-14\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/10] Inference: 100%|██████████| 99/99 [12:23<00:00,  7.51s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-4.1-mini-2025-04-14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/10] Judging: 100%|██████████| 99/99 [16:46<00:00, 10.16s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model 5/10: gpt-4.1-nano-2025-04-14\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/10] Inference: 100%|██████████| 99/99 [06:30<00:00,  3.95s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-4.1-nano-2025-04-14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/10] Judging: 100%|██████████| 99/99 [16:46<00:00, 10.17s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model 6/10: ft:gpt-4.1-mini-2025-04-14:tavily::CzWAcE6p\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/10] Inference: 100%|██████████| 99/99 [12:13<00:00,  7.41s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_ft_gpt-4.1-mini-2025-04-14_tavily__CzWAcE6p.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/10] Judging: 100%|██████████| 99/99 [17:30<00:00, 10.61s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model 7/10: ft:gpt-4.1-nano-2025-04-14:tavily::CzX41hjk\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/10] Inference: 100%|██████████| 99/99 [07:16<00:00,  4.41s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_ft_gpt-4.1-nano-2025-04-14_tavily__CzX41hjk.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/10] Judging: 100%|██████████| 99/99 [17:15<00:00, 10.46s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model 8/10: gpt-5-nano\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/10] Inference: 100%|██████████| 99/99 [33:27<00:00, 20.28s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-5-nano.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/10] Judging: 100%|██████████| 99/99 [16:57<00:00, 10.28s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model 9/10: gpt-5-mini\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/10] Inference: 100%|██████████| 99/99 [27:07<00:00, 16.43s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-5-mini.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/10] Judging: 100%|██████████| 99/99 [16:48<00:00, 10.18s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Model 10/10: gpt-5.2-2025-12-11\n",
      "======================================================================\n",
      "Warning: Model 'gpt-5.2-2025-12-11' not recognized by tiktoken. Using 'gpt-5' tokenizer as fallback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/10] Inference: 100%|██████████| 99/99 [13:02<00:00,  7.90s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-5.2-2025-12-11.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/10] Judging: 100%|██████████| 99/99 [20:05<00:00, 12.18s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Benchmark Finished. Total Judge Cost: $22.9607\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_175e4_row0_col0, #T_175e4_row0_col2, #T_175e4_row0_col3, #T_175e4_row0_col4, #T_175e4_row0_col6, #T_175e4_row0_col7, #T_175e4_row4_col1, #T_175e4_row5_col5 {\n",
       "  background-color: #440154;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row0_col1 {\n",
       "  background-color: #77d153;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row0_col5, #T_175e4_row4_col2, #T_175e4_row4_col5 {\n",
       "  background-color: #bade28;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row1_col0, #T_175e4_row8_col1 {\n",
       "  background-color: #5cc863;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row1_col1 {\n",
       "  background-color: #277e8e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row1_col2 {\n",
       "  background-color: #f4e61e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row1_col3, #T_175e4_row1_col5, #T_175e4_row2_col3, #T_175e4_row7_col6, #T_175e4_row8_col2, #T_175e4_row8_col4, #T_175e4_row9_col0, #T_175e4_row9_col1, #T_175e4_row9_col2, #T_175e4_row9_col3, #T_175e4_row9_col7 {\n",
       "  background-color: #fde725;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row1_col4 {\n",
       "  background-color: #dfe318;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row1_col6 {\n",
       "  background-color: #34618d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row1_col7 {\n",
       "  background-color: #481b6d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row2_col0 {\n",
       "  background-color: #b2dd2d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row2_col1 {\n",
       "  background-color: #297b8e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row2_col2 {\n",
       "  background-color: #fbe723;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row2_col4, #T_175e4_row3_col2, #T_175e4_row8_col3 {\n",
       "  background-color: #f6e620;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row2_col5, #T_175e4_row4_col3, #T_175e4_row7_col2 {\n",
       "  background-color: #f1e51d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row2_col6 {\n",
       "  background-color: #2d718e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row2_col7 {\n",
       "  background-color: #ece51b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row3_col0 {\n",
       "  background-color: #86d549;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row3_col1 {\n",
       "  background-color: #2b758e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row3_col3, #T_175e4_row5_col2, #T_175e4_row5_col3 {\n",
       "  background-color: #f8e621;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row3_col4 {\n",
       "  background-color: #d0e11c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row3_col5 {\n",
       "  background-color: #6ccd5a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row3_col6 {\n",
       "  background-color: #30698e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row3_col7 {\n",
       "  background-color: #424186;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row4_col0 {\n",
       "  background-color: #48c16e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row4_col4 {\n",
       "  background-color: #7cd250;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row4_col6 {\n",
       "  background-color: #453581;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row4_col7 {\n",
       "  background-color: #471365;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row5_col0 {\n",
       "  background-color: #e2e418;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row5_col1 {\n",
       "  background-color: #93d741;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row5_col4 {\n",
       "  background-color: #c2df23;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row5_col6 {\n",
       "  background-color: #31688e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row5_col7 {\n",
       "  background-color: #297a8e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row6_col0 {\n",
       "  background-color: #98d83e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row6_col1 {\n",
       "  background-color: #26828e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row6_col2 {\n",
       "  background-color: #dae319;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row6_col3, #T_175e4_row7_col3 {\n",
       "  background-color: #e5e419;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row6_col4 {\n",
       "  background-color: #bddf26;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row6_col5 {\n",
       "  background-color: #482576;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row6_col6 {\n",
       "  background-color: #433d84;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row6_col7 {\n",
       "  background-color: #482979;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row7_col0 {\n",
       "  background-color: #a8db34;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row7_col1 {\n",
       "  background-color: #25858e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row7_col4 {\n",
       "  background-color: #d8e219;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row7_col5 {\n",
       "  background-color: #46327e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row7_col7 {\n",
       "  background-color: #481668;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row8_col0 {\n",
       "  background-color: #d5e21a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row8_col5 {\n",
       "  background-color: #31b57b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row8_col6 {\n",
       "  background-color: #7ad151;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row8_col7 {\n",
       "  background-color: #433e85;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row9_col4 {\n",
       "  background-color: #e7e419;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_175e4_row9_col5 {\n",
       "  background-color: #471063;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_175e4_row9_col6 {\n",
       "  background-color: #2e6e8e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_175e4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_175e4_level0_col0\" class=\"col_heading level0 col0\" >relevance</th>\n",
       "      <th id=\"T_175e4_level0_col1\" class=\"col_heading level0 col1\" >faithfulness</th>\n",
       "      <th id=\"T_175e4_level0_col2\" class=\"col_heading level0 col2\" >coherence</th>\n",
       "      <th id=\"T_175e4_level0_col3\" class=\"col_heading level0 col3\" >fluency</th>\n",
       "      <th id=\"T_175e4_level0_col4\" class=\"col_heading level0 col4\" >conciseness</th>\n",
       "      <th id=\"T_175e4_level0_col5\" class=\"col_heading level0 col5\" >length</th>\n",
       "      <th id=\"T_175e4_level0_col6\" class=\"col_heading level0 col6\" >latency</th>\n",
       "      <th id=\"T_175e4_level0_col7\" class=\"col_heading level0 col7\" >est_cost_per_1k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row0\" class=\"row_heading level0 row0\" >Baseline</th>\n",
       "      <td id=\"T_175e4_row0_col0\" class=\"data row0 col0\" >2.282828</td>\n",
       "      <td id=\"T_175e4_row0_col1\" class=\"data row0 col1\" >4.444444</td>\n",
       "      <td id=\"T_175e4_row0_col2\" class=\"data row0 col2\" >2.181818</td>\n",
       "      <td id=\"T_175e4_row0_col3\" class=\"data row0 col3\" >2.838384</td>\n",
       "      <td id=\"T_175e4_row0_col4\" class=\"data row0 col4\" >2.121212</td>\n",
       "      <td id=\"T_175e4_row0_col5\" class=\"data row0 col5\" >4.595960</td>\n",
       "      <td id=\"T_175e4_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "      <td id=\"T_175e4_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row1\" class=\"row_heading level0 row1\" >gpt-4o-mini</th>\n",
       "      <td id=\"T_175e4_row1_col0\" class=\"data row1 col0\" >4.171717</td>\n",
       "      <td id=\"T_175e4_row1_col1\" class=\"data row1 col1\" >4.010101</td>\n",
       "      <td id=\"T_175e4_row1_col2\" class=\"data row1 col2\" >4.909091</td>\n",
       "      <td id=\"T_175e4_row1_col3\" class=\"data row1 col3\" >5.000000</td>\n",
       "      <td id=\"T_175e4_row1_col4\" class=\"data row1 col4\" >4.030303</td>\n",
       "      <td id=\"T_175e4_row1_col5\" class=\"data row1 col5\" >4.797980</td>\n",
       "      <td id=\"T_175e4_row1_col6\" class=\"data row1 col6\" >5.947505</td>\n",
       "      <td id=\"T_175e4_row1_col7\" class=\"data row1 col7\" >1.554302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row2\" class=\"row_heading level0 row2\" >gpt-4.1-2025-04-14</th>\n",
       "      <td id=\"T_175e4_row2_col0\" class=\"data row2 col0\" >4.515152</td>\n",
       "      <td id=\"T_175e4_row2_col1\" class=\"data row2 col1\" >4.000000</td>\n",
       "      <td id=\"T_175e4_row2_col2\" class=\"data row2 col2\" >4.939394</td>\n",
       "      <td id=\"T_175e4_row2_col3\" class=\"data row2 col3\" >5.000000</td>\n",
       "      <td id=\"T_175e4_row2_col4\" class=\"data row2 col4\" >4.101010</td>\n",
       "      <td id=\"T_175e4_row2_col5\" class=\"data row2 col5\" >4.757576</td>\n",
       "      <td id=\"T_175e4_row2_col6\" class=\"data row2 col6\" >7.144455</td>\n",
       "      <td id=\"T_175e4_row2_col7\" class=\"data row2 col7\" >20.840869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row3\" class=\"row_heading level0 row3\" >gpt-4.1-mini-2025-04-14</th>\n",
       "      <td id=\"T_175e4_row3_col0\" class=\"data row3 col0\" >4.353535</td>\n",
       "      <td id=\"T_175e4_row3_col1\" class=\"data row3 col1\" >3.969697</td>\n",
       "      <td id=\"T_175e4_row3_col2\" class=\"data row3 col2\" >4.919192</td>\n",
       "      <td id=\"T_175e4_row3_col3\" class=\"data row3 col3\" >4.979798</td>\n",
       "      <td id=\"T_175e4_row3_col4\" class=\"data row3 col4\" >3.989899</td>\n",
       "      <td id=\"T_175e4_row3_col5\" class=\"data row3 col5\" >4.353535</td>\n",
       "      <td id=\"T_175e4_row3_col6\" class=\"data row3 col6\" >6.499374</td>\n",
       "      <td id=\"T_175e4_row3_col7\" class=\"data row3 col7\" >4.196521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row4\" class=\"row_heading level0 row4\" >gpt-4.1-nano-2025-04-14</th>\n",
       "      <td id=\"T_175e4_row4_col0\" class=\"data row4 col0\" >4.070707</td>\n",
       "      <td id=\"T_175e4_row4_col1\" class=\"data row4 col1\" >3.515152</td>\n",
       "      <td id=\"T_175e4_row4_col2\" class=\"data row4 col2\" >4.676768</td>\n",
       "      <td id=\"T_175e4_row4_col3\" class=\"data row4 col3\" >4.949495</td>\n",
       "      <td id=\"T_175e4_row4_col4\" class=\"data row4 col4\" >3.737374</td>\n",
       "      <td id=\"T_175e4_row4_col5\" class=\"data row4 col5\" >4.595960</td>\n",
       "      <td id=\"T_175e4_row4_col6\" class=\"data row4 col6\" >2.941758</td>\n",
       "      <td id=\"T_175e4_row4_col7\" class=\"data row4 col7\" >1.041138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row5\" class=\"row_heading level0 row5\" >ft:gpt-4.1-mini-2025-04-14:tavily::CzWAcE6p</th>\n",
       "      <td id=\"T_175e4_row5_col0\" class=\"data row5 col0\" >4.696970</td>\n",
       "      <td id=\"T_175e4_row5_col1\" class=\"data row5 col1\" >4.494949</td>\n",
       "      <td id=\"T_175e4_row5_col2\" class=\"data row5 col2\" >4.929293</td>\n",
       "      <td id=\"T_175e4_row5_col3\" class=\"data row5 col3\" >4.979798</td>\n",
       "      <td id=\"T_175e4_row5_col4\" class=\"data row5 col4\" >3.949495</td>\n",
       "      <td id=\"T_175e4_row5_col5\" class=\"data row5 col5\" >2.818182</td>\n",
       "      <td id=\"T_175e4_row5_col6\" class=\"data row5 col6\" >6.404545</td>\n",
       "      <td id=\"T_175e4_row5_col7\" class=\"data row5 col7\" >8.798570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row6\" class=\"row_heading level0 row6\" >ft:gpt-4.1-nano-2025-04-14:tavily::CzX41hjk</th>\n",
       "      <td id=\"T_175e4_row6_col0\" class=\"data row6 col0\" >4.414141</td>\n",
       "      <td id=\"T_175e4_row6_col1\" class=\"data row6 col1\" >4.030303</td>\n",
       "      <td id=\"T_175e4_row6_col2\" class=\"data row6 col2\" >4.797980</td>\n",
       "      <td id=\"T_175e4_row6_col3\" class=\"data row6 col3\" >4.909091</td>\n",
       "      <td id=\"T_175e4_row6_col4\" class=\"data row6 col4\" >3.929293</td>\n",
       "      <td id=\"T_175e4_row6_col5\" class=\"data row6 col5\" >3.020202</td>\n",
       "      <td id=\"T_175e4_row6_col6\" class=\"data row6 col6\" >3.404394</td>\n",
       "      <td id=\"T_175e4_row6_col7\" class=\"data row6 col7\" >2.456604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row7\" class=\"row_heading level0 row7\" >gpt-5-nano</th>\n",
       "      <td id=\"T_175e4_row7_col0\" class=\"data row7 col0\" >4.474747</td>\n",
       "      <td id=\"T_175e4_row7_col1\" class=\"data row7 col1\" >4.050505</td>\n",
       "      <td id=\"T_175e4_row7_col2\" class=\"data row7 col2\" >4.898990</td>\n",
       "      <td id=\"T_175e4_row7_col3\" class=\"data row7 col3\" >4.909091</td>\n",
       "      <td id=\"T_175e4_row7_col4\" class=\"data row7 col4\" >4.010101</td>\n",
       "      <td id=\"T_175e4_row7_col5\" class=\"data row7 col5\" >3.101010</td>\n",
       "      <td id=\"T_175e4_row7_col6\" class=\"data row7 col6\" >19.274374</td>\n",
       "      <td id=\"T_175e4_row7_col7\" class=\"data row7 col7\" >1.248511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row8\" class=\"row_heading level0 row8\" >gpt-5-mini</th>\n",
       "      <td id=\"T_175e4_row8_col0\" class=\"data row8 col0\" >4.646465</td>\n",
       "      <td id=\"T_175e4_row8_col1\" class=\"data row8 col1\" >4.393939</td>\n",
       "      <td id=\"T_175e4_row8_col2\" class=\"data row8 col2\" >4.959596</td>\n",
       "      <td id=\"T_175e4_row8_col3\" class=\"data row8 col3\" >4.969697</td>\n",
       "      <td id=\"T_175e4_row8_col4\" class=\"data row8 col4\" >4.131313</td>\n",
       "      <td id=\"T_175e4_row8_col5\" class=\"data row8 col5\" >4.111111</td>\n",
       "      <td id=\"T_175e4_row8_col6\" class=\"data row8 col6\" >15.427586</td>\n",
       "      <td id=\"T_175e4_row8_col7\" class=\"data row8 col7\" >3.941081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_175e4_level0_row9\" class=\"row_heading level0 row9\" >gpt-5.2-2025-12-11</th>\n",
       "      <td id=\"T_175e4_row9_col0\" class=\"data row9 col0\" >4.808081</td>\n",
       "      <td id=\"T_175e4_row9_col1\" class=\"data row9 col1\" >4.686869</td>\n",
       "      <td id=\"T_175e4_row9_col2\" class=\"data row9 col2\" >4.959596</td>\n",
       "      <td id=\"T_175e4_row9_col3\" class=\"data row9 col3\" >5.000000</td>\n",
       "      <td id=\"T_175e4_row9_col4\" class=\"data row9 col4\" >4.060606</td>\n",
       "      <td id=\"T_175e4_row9_col5\" class=\"data row9 col5\" >2.898990</td>\n",
       "      <td id=\"T_175e4_row9_col6\" class=\"data row9 col6\" >6.895172</td>\n",
       "      <td id=\"T_175e4_row9_col7\" class=\"data row9 col7\" >21.495727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1be4058fb80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation.benchmark import BenchmarkingSuite\n",
    "from agents.config import RATES\n",
    "\n",
    "# 1. Define models to test (from your RATES keys)\n",
    "test_models = ['Baseline'] + [model for model in RATES.keys()]\n",
    "\n",
    "# 2. Prepare subset (validation subset already defined)\n",
    "subset_data = validation_data\n",
    "\n",
    "# 3. Run flow\n",
    "suite = BenchmarkingSuite(rates=RATES)\n",
    "report_df = suite.run_benchmark(subset_data, test_models)\n",
    "\n",
    "# 4. Display stylized table\n",
    "report_df.set_index('model').style.background_gradient(cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ce9c4",
   "metadata": {},
   "source": [
    "Benchmark with Retry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "940e58b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model 'gpt-5.2-2025-12-11' not recognized by tiktoken. Using 'o200k_base' tokenizer (family mapping ^gpt-5).\n",
      "\n",
      "======================================================================\n",
      "Model 1/12: gpt-4o-mini\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/12] Inference: 100%|██████████| 58/58 [09:26<00:00,  9.77s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-4o-mini.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/12] Judging: 100%|██████████| 58/58 [09:54<00:00, 10.26s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_gpt-4o-mini.json\n",
      "\n",
      "======================================================================\n",
      "Model 2/12: gpt-4.1-2025-04-14\n",
      "======================================================================\n",
      "Warning: Model 'gpt-4.1-2025-04-14' not recognized by tiktoken. Using 'o200k_base' tokenizer (family mapping ^gpt-4\\.1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/12] Inference: 100%|██████████| 58/58 [08:00<00:00,  8.28s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-4.1-2025-04-14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/12] Judging: 100%|██████████| 58/58 [10:24<00:00, 10.77s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_gpt-4.1-2025-04-14.json\n",
      "\n",
      "======================================================================\n",
      "Model 3/12: gpt-4.1-mini-2025-04-14\n",
      "======================================================================\n",
      "Warning: Model 'gpt-4.1-mini-2025-04-14' not recognized by tiktoken. Using 'o200k_base' tokenizer (family mapping ^gpt-4\\.1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/12] Inference: 100%|██████████| 58/58 [09:58<00:00, 10.32s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-4.1-mini-2025-04-14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/12] Judging: 100%|██████████| 58/58 [10:27<00:00, 10.82s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_gpt-4.1-mini-2025-04-14.json\n",
      "\n",
      "======================================================================\n",
      "Model 4/12: gpt-4.1-nano-2025-04-14\n",
      "======================================================================\n",
      "Warning: Model 'gpt-4.1-nano-2025-04-14' not recognized by tiktoken. Using 'o200k_base' tokenizer (family mapping ^gpt-4\\.1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/12] Inference: 100%|██████████| 58/58 [05:22<00:00,  5.57s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-4.1-nano-2025-04-14.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/12] Judging: 100%|██████████| 58/58 [09:52<00:00, 10.21s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_gpt-4.1-nano-2025-04-14.json\n",
      "\n",
      "======================================================================\n",
      "Model 5/12: ft:gpt-4.1-mini-2025-04-14:tavily::CzWAcE6p\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/12] Inference: 100%|██████████| 58/58 [15:13<00:00, 15.76s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_ft_gpt-4.1-mini-2025-04-14_tavily__CzWAcE6p.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/12] Judging: 100%|██████████| 58/58 [09:38<00:00,  9.97s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_ft_gpt-4.1-mini-2025-04-14_tavily__CzWAcE6p.json\n",
      "\n",
      "======================================================================\n",
      "Model 6/12: ft:gpt-4.1-nano-2025-04-14:tavily::CzX41hjk\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/12] Inference: 100%|██████████| 58/58 [05:42<00:00,  5.91s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_ft_gpt-4.1-nano-2025-04-14_tavily__CzX41hjk.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6/12] Judging: 100%|██████████| 58/58 [10:46<00:00, 11.15s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_ft_gpt-4.1-nano-2025-04-14_tavily__CzX41hjk.json\n",
      "\n",
      "======================================================================\n",
      "Model 7/12: ft:gpt-4.1-mini-2025-04-14:tavily::CzqO8otr\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/12] Inference: 100%|██████████| 58/58 [14:08<00:00, 14.62s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_ft_gpt-4.1-mini-2025-04-14_tavily__CzqO8otr.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7/12] Judging: 100%|██████████| 58/58 [10:39<00:00, 11.02s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_ft_gpt-4.1-mini-2025-04-14_tavily__CzqO8otr.json\n",
      "\n",
      "======================================================================\n",
      "Model 8/12: ft:gpt-4.1-nano-2025-04-14:tavily::Czr44ng2\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/12] Inference: 100%|██████████| 58/58 [06:19<00:00,  6.55s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_ft_gpt-4.1-nano-2025-04-14_tavily__Czr44ng2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8/12] Judging: 100%|██████████| 58/58 [11:49<00:00, 12.24s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_ft_gpt-4.1-nano-2025-04-14_tavily__Czr44ng2.json\n",
      "\n",
      "======================================================================\n",
      "Model 9/12: gpt-5-nano\n",
      "======================================================================\n",
      "Warning: Model 'gpt-5-nano' not recognized by tiktoken. Using 'o200k_base' tokenizer (family mapping ^gpt-5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/12] Inference: 100%|██████████| 58/58 [46:38<00:00, 48.25s/sample] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-5-nano.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9/12] Judging: 100%|██████████| 58/58 [09:00<00:00,  9.32s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_gpt-5-nano.json\n",
      "\n",
      "======================================================================\n",
      "Model 10/12: gpt-5-mini\n",
      "======================================================================\n",
      "Warning: Model 'gpt-5-mini' not recognized by tiktoken. Using 'o200k_base' tokenizer (family mapping ^gpt-5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/12] Inference: 100%|██████████| 58/58 [26:03<00:00, 26.96s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-5-mini.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/12] Judging: 100%|██████████| 58/58 [09:37<00:00,  9.95s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_gpt-5-mini.json\n",
      "\n",
      "======================================================================\n",
      "Model 11/12: gpt-5.2-2025-12-11\n",
      "======================================================================\n",
      "Warning: Model 'gpt-5.2-2025-12-11' not recognized by tiktoken. Using 'o200k_base' tokenizer (family mapping ^gpt-5).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/12] Inference: 100%|██████████| 58/58 [14:46<00:00, 15.28s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to data/inference_gpt-5.2-2025-12-11.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/12] Judging: 100%|██████████| 58/58 [09:27<00:00,  9.78s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_gpt-5.2-2025-12-11.json\n",
      "\n",
      "======================================================================\n",
      "Model 12/12: Baseline\n",
      "======================================================================\n",
      "Loading baseline summaries from data/baseline_validation_benchmark.json\n",
      "✓ Filtered to 58 baseline summaries matching eval subset\n",
      "✓ Loaded 58 baseline summaries\n",
      "✓ Saved to data/inference_baseline.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12/12] Judging: 100%|██████████| 58/58 [08:33<00:00,  8.85s/sample]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Judge results saved to data/judge_Baseline.json\n",
      "\n",
      "======================================================================\n",
      "Benchmark Finished. Total Judge Cost: $20.1136\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_71f3d_row0_col0 {\n",
       "  background-color: #7ad151;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row0_col1, #T_71f3d_row1_col1 {\n",
       "  background-color: #297b8e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row0_col2 {\n",
       "  background-color: #eae51a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row0_col3, #T_71f3d_row1_col4, #T_71f3d_row1_col5, #T_71f3d_row6_col0, #T_71f3d_row8_col6, #T_71f3d_row9_col4, #T_71f3d_row10_col1, #T_71f3d_row10_col2, #T_71f3d_row10_col7, #T_71f3d_row10_col8 {\n",
       "  background-color: #fde725;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row0_col4 {\n",
       "  background-color: #cde11d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row0_col5, #T_71f3d_row4_col0, #T_71f3d_row6_col4 {\n",
       "  background-color: #e5e419;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row0_col6 {\n",
       "  background-color: #423f85;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row0_col7, #T_71f3d_row3_col2 {\n",
       "  background-color: #aadc32;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row0_col8 {\n",
       "  background-color: #471365;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row1_col0, #T_71f3d_row2_col5, #T_71f3d_row5_col0, #T_71f3d_row5_col4 {\n",
       "  background-color: #bddf26;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row1_col2, #T_71f3d_row4_col2, #T_71f3d_row6_col7 {\n",
       "  background-color: #f4e61e;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row1_col3, #T_71f3d_row2_col3, #T_71f3d_row9_col2, #T_71f3d_row9_col3, #T_71f3d_row10_col0, #T_71f3d_row10_col3 {\n",
       "  background-color: #fbe723;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row1_col6 {\n",
       "  background-color: #453581;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row1_col7 {\n",
       "  background-color: #c8e020;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row1_col8 {\n",
       "  background-color: #25ab82;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row2_col0, #T_71f3d_row2_col7, #T_71f3d_row8_col0 {\n",
       "  background-color: #b2dd2d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row2_col1 {\n",
       "  background-color: #32658e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row2_col2, #T_71f3d_row6_col2 {\n",
       "  background-color: #f8e621;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row2_col4, #T_71f3d_row7_col2 {\n",
       "  background-color: #c2df23;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row2_col6 {\n",
       "  background-color: #414287;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row2_col8, #T_71f3d_row7_col5 {\n",
       "  background-color: #46337f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row3_col0 {\n",
       "  background-color: #86d549;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row3_col1, #T_71f3d_row4_col5, #T_71f3d_row11_col0, #T_71f3d_row11_col2, #T_71f3d_row11_col3, #T_71f3d_row11_col4, #T_71f3d_row11_col6, #T_71f3d_row11_col7, #T_71f3d_row11_col8 {\n",
       "  background-color: #440154;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row3_col3 {\n",
       "  background-color: #dde318;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row3_col4 {\n",
       "  background-color: #95d840;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row3_col5, #T_71f3d_row4_col7, #T_71f3d_row8_col4 {\n",
       "  background-color: #dfe318;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row3_col6, #T_71f3d_row7_col8 {\n",
       "  background-color: #482374;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row3_col7 {\n",
       "  background-color: #6ccd5a;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row3_col8 {\n",
       "  background-color: #470d60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row4_col1, #T_71f3d_row6_col1 {\n",
       "  background-color: #9dd93b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row4_col3, #T_71f3d_row6_col3 {\n",
       "  background-color: #f6e620;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row4_col4, #T_71f3d_row7_col0 {\n",
       "  background-color: #a5db36;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row4_col6 {\n",
       "  background-color: #33628d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row4_col8 {\n",
       "  background-color: #287c8e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row5_col1 {\n",
       "  background-color: #2a778e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row5_col2 {\n",
       "  background-color: #c5e021;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row5_col3 {\n",
       "  background-color: #dae319;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row5_col5 {\n",
       "  background-color: #39558c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row5_col6 {\n",
       "  background-color: #482576;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row5_col7 {\n",
       "  background-color: #a2da37;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row5_col8 {\n",
       "  background-color: #482173;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row6_col5 {\n",
       "  background-color: #20928c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row6_col6 {\n",
       "  background-color: #365c8d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row6_col8 {\n",
       "  background-color: #2d708e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row7_col1 {\n",
       "  background-color: #3d4e8a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row7_col3, #T_71f3d_row9_col0 {\n",
       "  background-color: #d0e11c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row7_col4, #T_71f3d_row8_col7 {\n",
       "  background-color: #b8de29;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row7_col6 {\n",
       "  background-color: #472a7a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row7_col7 {\n",
       "  background-color: #8bd646;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row8_col1 {\n",
       "  background-color: #25858e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row8_col2 {\n",
       "  background-color: #e2e418;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row8_col3 {\n",
       "  background-color: #e7e419;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row8_col5 {\n",
       "  background-color: #1fa088;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row8_col8 {\n",
       "  background-color: #482475;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row9_col1 {\n",
       "  background-color: #89d548;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row9_col5 {\n",
       "  background-color: #5ec962;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row9_col6 {\n",
       "  background-color: #1e9c89;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row9_col7 {\n",
       "  background-color: #efe51c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row9_col8 {\n",
       "  background-color: #404588;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row10_col4 {\n",
       "  background-color: #d8e219;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_71f3d_row10_col5 {\n",
       "  background-color: #23898e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row10_col6 {\n",
       "  background-color: #34608d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row11_col1 {\n",
       "  background-color: #3fbc73;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_71f3d_row11_col5 {\n",
       "  background-color: #8ed645;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_71f3d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_71f3d_level0_col0\" class=\"col_heading level0 col0\" >relevance</th>\n",
       "      <th id=\"T_71f3d_level0_col1\" class=\"col_heading level0 col1\" >faithfulness</th>\n",
       "      <th id=\"T_71f3d_level0_col2\" class=\"col_heading level0 col2\" >coherence</th>\n",
       "      <th id=\"T_71f3d_level0_col3\" class=\"col_heading level0 col3\" >fluency</th>\n",
       "      <th id=\"T_71f3d_level0_col4\" class=\"col_heading level0 col4\" >conciseness</th>\n",
       "      <th id=\"T_71f3d_level0_col5\" class=\"col_heading level0 col5\" >length</th>\n",
       "      <th id=\"T_71f3d_level0_col6\" class=\"col_heading level0 col6\" >latency</th>\n",
       "      <th id=\"T_71f3d_level0_col7\" class=\"col_heading level0 col7\" >quality</th>\n",
       "      <th id=\"T_71f3d_level0_col8\" class=\"col_heading level0 col8\" >est_cost_per_1k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row0\" class=\"row_heading level0 row0\" >gpt-4o-mini</th>\n",
       "      <td id=\"T_71f3d_row0_col0\" class=\"data row0 col0\" >4.120690</td>\n",
       "      <td id=\"T_71f3d_row0_col1\" class=\"data row0 col1\" >3.896552</td>\n",
       "      <td id=\"T_71f3d_row0_col2\" class=\"data row0 col2\" >4.862069</td>\n",
       "      <td id=\"T_71f3d_row0_col3\" class=\"data row0 col3\" >5.000000</td>\n",
       "      <td id=\"T_71f3d_row0_col4\" class=\"data row0 col4\" >3.965517</td>\n",
       "      <td id=\"T_71f3d_row0_col5\" class=\"data row0 col5\" >4.949310</td>\n",
       "      <td id=\"T_71f3d_row0_col6\" class=\"data row0 col6\" >8.759172</td>\n",
       "      <td id=\"T_71f3d_row0_col7\" class=\"data row0 col7\" >4.368966</td>\n",
       "      <td id=\"T_71f3d_row0_col8\" class=\"data row0 col8\" >1.235529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row1\" class=\"row_heading level0 row1\" >gpt-4.1-2025-04-14</th>\n",
       "      <td id=\"T_71f3d_row1_col0\" class=\"data row1 col0\" >4.379310</td>\n",
       "      <td id=\"T_71f3d_row1_col1\" class=\"data row1 col1\" >3.896552</td>\n",
       "      <td id=\"T_71f3d_row1_col2\" class=\"data row1 col2\" >4.913793</td>\n",
       "      <td id=\"T_71f3d_row1_col3\" class=\"data row1 col3\" >4.982759</td>\n",
       "      <td id=\"T_71f3d_row1_col4\" class=\"data row1 col4\" >4.120690</td>\n",
       "      <td id=\"T_71f3d_row1_col5\" class=\"data row1 col5\" >5.000000</td>\n",
       "      <td id=\"T_71f3d_row1_col6\" class=\"data row1 col6\" >7.273259</td>\n",
       "      <td id=\"T_71f3d_row1_col7\" class=\"data row1 col7\" >4.458621</td>\n",
       "      <td id=\"T_71f3d_row1_col8\" class=\"data row1 col8\" >15.243828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row2\" class=\"row_heading level0 row2\" >gpt-4.1-mini-2025-04-14</th>\n",
       "      <td id=\"T_71f3d_row2_col0\" class=\"data row2 col0\" >4.344828</td>\n",
       "      <td id=\"T_71f3d_row2_col1\" class=\"data row2 col1\" >3.793103</td>\n",
       "      <td id=\"T_71f3d_row2_col2\" class=\"data row2 col2\" >4.931034</td>\n",
       "      <td id=\"T_71f3d_row2_col3\" class=\"data row2 col3\" >4.982759</td>\n",
       "      <td id=\"T_71f3d_row2_col4\" class=\"data row2 col4\" >3.931034</td>\n",
       "      <td id=\"T_71f3d_row2_col5\" class=\"data row2 col5\" >4.880690</td>\n",
       "      <td id=\"T_71f3d_row2_col6\" class=\"data row2 col6\" >9.312966</td>\n",
       "      <td id=\"T_71f3d_row2_col7\" class=\"data row2 col7\" >4.396552</td>\n",
       "      <td id=\"T_71f3d_row2_col8\" class=\"data row2 col8\" >3.628462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row3\" class=\"row_heading level0 row3\" >gpt-4.1-nano-2025-04-14</th>\n",
       "      <td id=\"T_71f3d_row3_col0\" class=\"data row3 col0\" >4.172414</td>\n",
       "      <td id=\"T_71f3d_row3_col1\" class=\"data row3 col1\" >3.413793</td>\n",
       "      <td id=\"T_71f3d_row3_col2\" class=\"data row3 col2\" >4.586207</td>\n",
       "      <td id=\"T_71f3d_row3_col3\" class=\"data row3 col3\" >4.879310</td>\n",
       "      <td id=\"T_71f3d_row3_col4\" class=\"data row3 col4\" >3.793103</td>\n",
       "      <td id=\"T_71f3d_row3_col5\" class=\"data row3 col5\" >4.940345</td>\n",
       "      <td id=\"T_71f3d_row3_col6\" class=\"data row3 col6\" >4.561276</td>\n",
       "      <td id=\"T_71f3d_row3_col7\" class=\"data row3 col7\" >4.168966</td>\n",
       "      <td id=\"T_71f3d_row3_col8\" class=\"data row3 col8\" >0.812433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row4\" class=\"row_heading level0 row4\" >ft:gpt-4.1-mini-2025-04-14:tavily::CzWAcE6p</th>\n",
       "      <td id=\"T_71f3d_row4_col0\" class=\"data row4 col0\" >4.534483</td>\n",
       "      <td id=\"T_71f3d_row4_col1\" class=\"data row4 col1\" >4.413793</td>\n",
       "      <td id=\"T_71f3d_row4_col2\" class=\"data row4 col2\" >4.913793</td>\n",
       "      <td id=\"T_71f3d_row4_col3\" class=\"data row4 col3\" >4.965517</td>\n",
       "      <td id=\"T_71f3d_row4_col4\" class=\"data row4 col4\" >3.844828</td>\n",
       "      <td id=\"T_71f3d_row4_col5\" class=\"data row4 col5\" >3.810690</td>\n",
       "      <td id=\"T_71f3d_row4_col6\" class=\"data row4 col6\" >14.750345</td>\n",
       "      <td id=\"T_71f3d_row4_col7\" class=\"data row4 col7\" >4.534483</td>\n",
       "      <td id=\"T_71f3d_row4_col8\" class=\"data row4 col8\" >10.300276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row5\" class=\"row_heading level0 row5\" >ft:gpt-4.1-nano-2025-04-14:tavily::CzX41hjk</th>\n",
       "      <td id=\"T_71f3d_row5_col0\" class=\"data row5 col0\" >4.379310</td>\n",
       "      <td id=\"T_71f3d_row5_col1\" class=\"data row5 col1\" >3.879310</td>\n",
       "      <td id=\"T_71f3d_row5_col2\" class=\"data row5 col2\" >4.706897</td>\n",
       "      <td id=\"T_71f3d_row5_col3\" class=\"data row5 col3\" >4.862069</td>\n",
       "      <td id=\"T_71f3d_row5_col4\" class=\"data row5 col4\" >3.913793</td>\n",
       "      <td id=\"T_71f3d_row5_col5\" class=\"data row5 col5\" >4.122241</td>\n",
       "      <td id=\"T_71f3d_row5_col6\" class=\"data row5 col6\" >4.899845</td>\n",
       "      <td id=\"T_71f3d_row5_col7\" class=\"data row5 col7\" >4.348276</td>\n",
       "      <td id=\"T_71f3d_row5_col8\" class=\"data row5 col8\" >2.303928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row6\" class=\"row_heading level0 row6\" >ft:gpt-4.1-mini-2025-04-14:tavily::CzqO8otr</th>\n",
       "      <td id=\"T_71f3d_row6_col0\" class=\"data row6 col0\" >4.637931</td>\n",
       "      <td id=\"T_71f3d_row6_col1\" class=\"data row6 col1\" >4.413793</td>\n",
       "      <td id=\"T_71f3d_row6_col2\" class=\"data row6 col2\" >4.931034</td>\n",
       "      <td id=\"T_71f3d_row6_col3\" class=\"data row6 col3\" >4.965517</td>\n",
       "      <td id=\"T_71f3d_row6_col4\" class=\"data row6 col4\" >4.034483</td>\n",
       "      <td id=\"T_71f3d_row6_col5\" class=\"data row6 col5\" >4.410000</td>\n",
       "      <td id=\"T_71f3d_row6_col6\" class=\"data row6 col6\" >13.615707</td>\n",
       "      <td id=\"T_71f3d_row6_col7\" class=\"data row6 col7\" >4.596552</td>\n",
       "      <td id=\"T_71f3d_row6_col8\" class=\"data row6 col8\" >9.053697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row7\" class=\"row_heading level0 row7\" >ft:gpt-4.1-nano-2025-04-14:tavily::Czr44ng2</th>\n",
       "      <td id=\"T_71f3d_row7_col0\" class=\"data row7 col0\" >4.293103</td>\n",
       "      <td id=\"T_71f3d_row7_col1\" class=\"data row7 col1\" >3.689655</td>\n",
       "      <td id=\"T_71f3d_row7_col2\" class=\"data row7 col2\" >4.689655</td>\n",
       "      <td id=\"T_71f3d_row7_col3\" class=\"data row7 col3\" >4.827586</td>\n",
       "      <td id=\"T_71f3d_row7_col4\" class=\"data row7 col4\" >3.896552</td>\n",
       "      <td id=\"T_71f3d_row7_col5\" class=\"data row7 col5\" >3.983793</td>\n",
       "      <td id=\"T_71f3d_row7_col6\" class=\"data row7 col6\" >5.545379</td>\n",
       "      <td id=\"T_71f3d_row7_col7\" class=\"data row7 col7\" >4.279310</td>\n",
       "      <td id=\"T_71f3d_row7_col8\" class=\"data row7 col8\" >2.369459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row8\" class=\"row_heading level0 row8\" >gpt-5-nano</th>\n",
       "      <td id=\"T_71f3d_row8_col0\" class=\"data row8 col0\" >4.344828</td>\n",
       "      <td id=\"T_71f3d_row8_col1\" class=\"data row8 col1\" >3.948276</td>\n",
       "      <td id=\"T_71f3d_row8_col2\" class=\"data row8 col2\" >4.827586</td>\n",
       "      <td id=\"T_71f3d_row8_col3\" class=\"data row8 col3\" >4.913793</td>\n",
       "      <td id=\"T_71f3d_row8_col4\" class=\"data row8 col4\" >4.017241</td>\n",
       "      <td id=\"T_71f3d_row8_col5\" class=\"data row8 col5\" >4.480000</td>\n",
       "      <td id=\"T_71f3d_row8_col6\" class=\"data row8 col6\" >47.240328</td>\n",
       "      <td id=\"T_71f3d_row8_col7\" class=\"data row8 col7\" >4.410345</td>\n",
       "      <td id=\"T_71f3d_row8_col8\" class=\"data row8 col8\" >2.431193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row9\" class=\"row_heading level0 row9\" >gpt-5-mini</th>\n",
       "      <td id=\"T_71f3d_row9_col0\" class=\"data row9 col0\" >4.448276</td>\n",
       "      <td id=\"T_71f3d_row9_col1\" class=\"data row9 col1\" >4.379310</td>\n",
       "      <td id=\"T_71f3d_row9_col2\" class=\"data row9 col2\" >4.948276</td>\n",
       "      <td id=\"T_71f3d_row9_col3\" class=\"data row9 col3\" >4.982759</td>\n",
       "      <td id=\"T_71f3d_row9_col4\" class=\"data row9 col4\" >4.120690</td>\n",
       "      <td id=\"T_71f3d_row9_col5\" class=\"data row9 col5\" >4.706897</td>\n",
       "      <td id=\"T_71f3d_row9_col6\" class=\"data row9 col6\" >25.952172</td>\n",
       "      <td id=\"T_71f3d_row9_col7\" class=\"data row9 col7\" >4.575862</td>\n",
       "      <td id=\"T_71f3d_row9_col8\" class=\"data row9 col8\" >5.075957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row10\" class=\"row_heading level0 row10\" >gpt-5.2-2025-12-11</th>\n",
       "      <td id=\"T_71f3d_row10_col0\" class=\"data row10 col0\" >4.620690</td>\n",
       "      <td id=\"T_71f3d_row10_col1\" class=\"data row10 col1\" >4.586207</td>\n",
       "      <td id=\"T_71f3d_row10_col2\" class=\"data row10 col2\" >4.965517</td>\n",
       "      <td id=\"T_71f3d_row10_col3\" class=\"data row10 col3\" >4.982759</td>\n",
       "      <td id=\"T_71f3d_row10_col4\" class=\"data row10 col4\" >4.000000</td>\n",
       "      <td id=\"T_71f3d_row10_col5\" class=\"data row10 col5\" >4.371724</td>\n",
       "      <td id=\"T_71f3d_row10_col6\" class=\"data row10 col6\" >14.271293</td>\n",
       "      <td id=\"T_71f3d_row10_col7\" class=\"data row10 col7\" >4.631034</td>\n",
       "      <td id=\"T_71f3d_row10_col8\" class=\"data row10 col8\" >24.873776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71f3d_level0_row11\" class=\"row_heading level0 row11\" >Baseline</th>\n",
       "      <td id=\"T_71f3d_row11_col0\" class=\"data row11 col0\" >2.051724</td>\n",
       "      <td id=\"T_71f3d_row11_col1\" class=\"data row11 col1\" >4.224138</td>\n",
       "      <td id=\"T_71f3d_row11_col2\" class=\"data row11 col2\" >1.965517</td>\n",
       "      <td id=\"T_71f3d_row11_col3\" class=\"data row11 col3\" >2.637931</td>\n",
       "      <td id=\"T_71f3d_row11_col4\" class=\"data row11 col4\" >2.068966</td>\n",
       "      <td id=\"T_71f3d_row11_col5\" class=\"data row11 col5\" >4.797586</td>\n",
       "      <td id=\"T_71f3d_row11_col6\" class=\"data row11 col6\" >0.000000</td>\n",
       "      <td id=\"T_71f3d_row11_col7\" class=\"data row11 col7\" >2.589655</td>\n",
       "      <td id=\"T_71f3d_row11_col8\" class=\"data row11 col8\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22ce3257d00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import agents.config\n",
    "import agents.llm\n",
    "import agents.summarizer\n",
    "import evaluation.benchmark\n",
    "\n",
    "# Reload modules in dependency order\n",
    "importlib.reload(agents.config)\n",
    "importlib.reload(agents.llm)\n",
    "importlib.reload(agents.summarizer)\n",
    "importlib.reload(evaluation.benchmark)\n",
    "\n",
    "from evaluation.benchmark import BenchmarkingSuite\n",
    "from agents.config import RATES\n",
    "\n",
    "# 1. Define models to test (from your RATES keys)\n",
    "test_models = [model for model in RATES.keys()] + ['Baseline']\n",
    "\n",
    "# 2. Prepare subset (validation subset already defined)\n",
    "subset_data = validation_data\n",
    "\n",
    "# 3. Run flow\n",
    "suite = BenchmarkingSuite(rates=RATES)\n",
    "report_df = suite.run_benchmark(subset_data, test_models)\n",
    "\n",
    "# 4. Display stylized table\n",
    "report_df.set_index('model').style.background_gradient(cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
