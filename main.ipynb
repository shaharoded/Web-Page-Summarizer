{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b72490b",
   "metadata": {},
   "source": [
    "# Web Page Summarization Evaluation System\n",
    "\n",
    "This notebook evaluates the quality of web page summaries generated by different LLM engines using an LLM-as-a-judge approach with G-Eval methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f18cb5",
   "metadata": {},
   "source": [
    "## 1. Creating a Gold Standard\n",
    "\n",
    "This section creates a gold standard dataset using the GPT-5.2 model to generate high-quality summaries. The gold standard will serve as a reference for evaluating other summarization engines and for distillation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cb2b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 items from baseline dataset\n",
      "Warning: Model 'gpt-5.2-2025-12-11' not recognized by tiktoken. Using 'gpt-5' tokenizer as fallback.\n",
      "Generating gold standard summaries with GPT-5.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:   2%|▏         | 22/1000 [00:22<14:58,  1.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 324199 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  13%|█▎        | 132/1000 [01:58<13:05,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 680772 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  18%|█▊        | 183/1000 [02:45<09:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 454064 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  44%|████▍     | 439/1000 [06:43<08:07,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 457769 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  50%|████▉     | 496/1000 [07:39<09:09,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 352591 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  52%|█████▏    | 518/1000 [07:57<04:52,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 310075 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  65%|██████▍   | 646/1000 [09:54<04:44,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 677936 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:  79%|███████▉  | 789/1000 [11:57<01:29,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Request failed with error: Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 374068 tokens. Please reduce the length of the messages.', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 1000/1000 [15:18<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Gold standard dataset saved to data/goldstandard_1k.json\n",
      "✓ Total items processed: 992\n",
      "⚠ Skipped 8 items due to token limits:\n",
      "  - https://pmc.ncbi.nlm.nih.gov/articles/PMC7271218/\n",
      "  - https://www.jsog.or.jp/activity/pdf/gl_fujinka_2023.pdf\n",
      "  - https://weatherspark.com/h/y/557/2024/Historical-Weather-during-2024-in-San-Francisco-California-United-States\n",
      "  - https://servicehub.ucdavis.edu/servicehub?id=ucd_kb_article&sys_id=cf60ebc293f1e69083cc38797bba1020\n",
      "  - https://s5.static.brasilescola.uol.com.br/vestibular/2022/12/resultado-cederj-2023.pdf\n",
      "  - https://colab.research.google.com/github/hc9903/deepke/blob/master/isa.ipynb\n",
      "  - https://www.insp.mx/resources/images/stories/INSP/Docs/Transparencia/EDICION%202011%20MEDICAMENTOS%20-%20link.pdf\n",
      "  - https://s2.static.brasilescola.uol.com.br/vestibular/2024/01/resultado-cederj-2024.pdf\n",
      "✓ Total estimated cost for gold standard generation (based on tokens and rates): $30.9979\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from agents.summarizer import Summarizer\n",
    "from tqdm.asyncio import tqdm as atqdm\n",
    "\n",
    "# Load baseline data\n",
    "with open('data/baseline_1k.json', 'r', encoding='utf-8') as f:\n",
    "    baseline_data = json.load(f)\n",
    "\n",
    "# Extract the data list from the top-level structure\n",
    "baseline_data = baseline_data['data']\n",
    "print(f\"Loaded {len(baseline_data)} items from baseline dataset\")\n",
    "\n",
    "# Initialize the gold standard summarizer with GPT-5.2\n",
    "gold_summarizer = Summarizer(model=\"gpt-5.2-2025-12-11\")\n",
    "\n",
    "# Initialize variables to track total cost and skipped items\n",
    "total_cost = 0.0\n",
    "gold_standard_data = []\n",
    "skipped_items = []\n",
    "\n",
    "async def summarize_item(item):\n",
    "    \"\"\"Async wrapper to summarize a single item\"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    # Run the synchronous summarize in a thread pool\n",
    "    summary, cost = await loop.run_in_executor(\n",
    "        None, \n",
    "        lambda: gold_summarizer.summarize(item['markdown_content'], get_cost=True)\n",
    "    )\n",
    "    \n",
    "    # Check if the request was skipped due to token limits\n",
    "    if summary is None:\n",
    "        return None, item['url']\n",
    "    \n",
    "    return {\n",
    "        'url': item['url'],\n",
    "        'markdown_content': item['markdown_content'],\n",
    "        'summary': summary\n",
    "    }, cost\n",
    "\n",
    "async def generate_summaries():\n",
    "    \"\"\"Generate all summaries concurrently\"\"\"\n",
    "    global total_cost, gold_standard_data, skipped_items\n",
    "    \n",
    "    # Create tasks for all items (limit concurrency to avoid rate limits)\n",
    "    max_concurrent = 10  # Adjust based on your API rate limits\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    async def summarize_with_semaphore(item):\n",
    "        async with semaphore:\n",
    "            return await summarize_item(item)\n",
    "    \n",
    "    # Process all items concurrently with progress bar\n",
    "    print(\"Generating gold standard summaries with GPT-5.2...\")\n",
    "    tasks = [summarize_with_semaphore(item) for item in baseline_data]\n",
    "    results = await atqdm.gather(*tasks, desc=\"Processing items\")\n",
    "    \n",
    "    # Collect results and accumulate costs\n",
    "    for result in results:\n",
    "        if result[0] is None:\n",
    "            # Item was skipped due to token limits\n",
    "            skipped_items.append(result[1])\n",
    "        else:\n",
    "            gold_item, cost = result\n",
    "            gold_standard_data.append(gold_item)\n",
    "            total_cost += cost\n",
    "\n",
    "# Run the async function\n",
    "await generate_summaries()\n",
    "\n",
    "# Save the gold standard dataset\n",
    "output_path = 'data/goldstandard_1k.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(gold_standard_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Gold standard dataset saved to {output_path}\")\n",
    "print(f\"✓ Total items processed: {len(gold_standard_data)}\")\n",
    "if skipped_items:\n",
    "    print(f\"⚠ Skipped {len(skipped_items)} items due to token limits:\")\n",
    "    for url in skipped_items:\n",
    "        print(f\"  - {url}\")\n",
    "print(f\"✓ Total estimated cost for gold standard generation (based on tokens and rates): ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d9f9f",
   "metadata": {},
   "source": [
    "For benchmarking, ignoring the long requests is fine. Production will need to handle inference that can handle these requests by cleaning / splitting the requests.\n",
    "\n",
    "At this point I'll split this gold standard to train-validation subsets, so that all evaluations will be done on the same web-pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05675f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 992 items from gold standard\n",
      "Using tokenizer: o200k_base\n",
      "\n",
      "Items under 64,000 tokens: 948\n",
      "Items over 64,000 tokens: 44\n",
      "\n",
      "Train set: 496 items (all under token limit)\n",
      "Validation set: 496 items (452 under limit, 44 over limit)\n",
      "\n",
      "✓ Train set saved to data/goldstandard_train.json\n",
      "✓ Validation set saved to data/goldstandard_validation.json\n",
      "✓ All training examples are within the 64,000 token limit for fine-tuning\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "PAGE_MAX_TOKENS = 64000\n",
    "\n",
    "# Load the gold standard dataset\n",
    "with open('data/goldstandard_1k.json', 'r', encoding='utf-8') as f:\n",
    "    gold_standard = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(gold_standard)} items from gold standard\")\n",
    "\n",
    "# Initialize tokenizer for token counting (using gpt-4.1 family)\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")  # gpt-4.1 uses same tokenizer as gpt-4o\n",
    "print(f\"Using tokenizer: {tokenizer.name}\")\n",
    "\n",
    "# Split data based on token count\n",
    "items_under_limit = []\n",
    "items_over_limit = []\n",
    "\n",
    "for item in gold_standard:\n",
    "    token_count = len(tokenizer.encode(item['markdown_content']))\n",
    "    if token_count <= PAGE_MAX_TOKENS:\n",
    "        items_under_limit.append(item)\n",
    "    else:\n",
    "        items_over_limit.append(item)\n",
    "\n",
    "print(f\"\\nItems under {PAGE_MAX_TOKENS:,} tokens: {len(items_under_limit)}\")\n",
    "print(f\"Items over {PAGE_MAX_TOKENS:,} tokens: {len(items_over_limit)}\")\n",
    "\n",
    "# Calculate target training size (50% of total gold standard)\n",
    "target_train_size = len(gold_standard) // 2\n",
    "\n",
    "# Take up to 50% of total for training (only from items under limit)\n",
    "train_data = items_under_limit[:target_train_size]\n",
    "\n",
    "# Everything else goes to validation\n",
    "validation_data = items_under_limit[target_train_size:] + items_over_limit\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_data)} items (all under token limit)\")\n",
    "print(f\"Validation set: {len(validation_data)} items ({len(items_under_limit[target_train_size:])} under limit, {len(items_over_limit)} over limit)\")\n",
    "\n",
    "# Save train set (for distillation)\n",
    "train_path = 'data/goldstandard_train.json'\n",
    "with open(train_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save validation set (for evaluation)\n",
    "validation_path = 'data/goldstandard_validation.json'\n",
    "with open(validation_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(validation_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Train set saved to {train_path}\")\n",
    "print(f\"✓ Validation set saved to {validation_path}\")\n",
    "print(f\"✓ All training examples are within the {PAGE_MAX_TOKENS:,} token limit for fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa54ed38",
   "metadata": {},
   "source": [
    "## 2. Distilling the Gold Standard into a Smaller Model\n",
    "\n",
    "This part will attampt to distil the intelligence of the GPT-5.2 model into smaller variants of the GPT-4.1 family, in order to create faster-cheaper solutions of similar performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b4415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Starting fine-tuning for gpt-4.1-mini-2025-04-14\n",
      "======================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: gpt-4.1-mini-2025-04-14\n",
      "Training samples: 496\n",
      "Total tokens: 4,987,780\n",
      "Epochs: 3\n",
      "Estimated training cost: $74.82\n",
      "Training file: data\\train_gpt-4.1-mini-2025-04-14.jsonl\n",
      "============================================================\n",
      "\n",
      "Fine-tuning job submitted: ftjob-GVcXQzcPFNU9KFyOIYQ1r7xK\n",
      "Monitor at: https://platform.openai.com/finetune/ftjob-GVcXQzcPFNU9KFyOIYQ1r7xK\n",
      "\n",
      "[1768775760] Created fine-tuning job: ftjob-GVcXQzcPFNU9KFyOIYQ1r7xK\n",
      "[1768775760] Validating training file: file-JjnTJVi1VoX4zQpeAdRhL9\n",
      "[1768775983] Files validated, moving job to queued state\n",
      "[1768775986] Fine-tuning job started\n",
      "[1768776107] Step 1/1488: training loss=1.07\n",
      "[1768776109] Step 2/1488: training loss=1.35\n",
      "[1768776111] Step 3/1488: training loss=2.69\n",
      "[1768776113] Step 4/1488: training loss=1.95\n",
      "[1768776115] Step 5/1488: training loss=1.75\n",
      "[1768776117] Step 6/1488: training loss=1.14\n",
      "[1768776119] Step 7/1488: training loss=1.55\n",
      "[1768776122] Step 8/1488: training loss=1.86\n",
      "[1768776122] Step 9/1488: training loss=1.12\n",
      "[1768776124] Step 10/1488: training loss=1.39\n",
      "[1768776124] Step 11/1488: training loss=2.34\n",
      "[1768776128] Step 12/1488: training loss=1.37\n",
      "[1768776130] Step 13/1488: training loss=1.22\n",
      "[1768776130] Step 14/1488: training loss=0.98\n",
      "[1768776132] Step 15/1488: training loss=1.28\n",
      "[1768776132] Step 16/1488: training loss=0.57\n",
      "[1768776136] Step 17/1488: training loss=1.47\n",
      "[1768776136] Step 18/1488: training loss=0.97\n",
      "[1768776138] Step 19/1488: training loss=1.20\n",
      "[1768776140] Step 20/1488: training loss=0.80\n",
      "[1768776140] Step 21/1488: training loss=0.59\n",
      "[1768776142] Step 22/1488: training loss=0.71\n",
      "[1768776144] Step 23/1488: training loss=1.00\n",
      "[1768776146] Step 24/1488: training loss=0.84\n",
      "[1768776146] Step 25/1488: training loss=1.13\n",
      "[1768776148] Step 26/1488: training loss=2.27\n",
      "[1768776148] Step 27/1488: training loss=0.98\n",
      "[1768776150] Step 28/1488: training loss=1.10\n",
      "[1768776150] Step 29/1488: training loss=0.78\n",
      "[1768776150] Step 30/1488: training loss=0.91\n",
      "[1768776152] Step 31/1488: training loss=1.04\n",
      "[1768776154] Step 32/1488: training loss=1.36\n",
      "[1768776156] Step 33/1488: training loss=2.38\n",
      "[1768776158] Step 34/1488: training loss=0.78\n",
      "[1768776158] Step 35/1488: training loss=0.71\n",
      "[1768776160] Step 36/1488: training loss=2.42\n",
      "[1768776162] Step 37/1488: training loss=0.72\n",
      "[1768776165] Step 38/1488: training loss=0.81\n",
      "[1768776165] Step 39/1488: training loss=0.98\n",
      "[1768776169] Step 40/1488: training loss=1.24\n",
      "[1768776171] Step 41/1488: training loss=0.89\n",
      "[1768776173] Step 42/1488: training loss=0.69\n",
      "[1768776173] Step 43/1488: training loss=0.61\n",
      "[1768776175] Step 44/1488: training loss=0.67\n",
      "[1768776177] Step 45/1488: training loss=0.66\n",
      "[1768776177] Step 46/1488: training loss=0.76\n",
      "[1768776179] Step 47/1488: training loss=2.96\n",
      "[1768776181] Step 48/1488: training loss=0.67\n",
      "[1768776181] Step 49/1488: training loss=0.81\n",
      "[1768776183] Step 50/1488: training loss=0.85\n",
      "[1768776185] Step 51/1488: training loss=0.59\n",
      "[1768776187] Step 52/1488: training loss=1.58\n",
      "[1768776189] Step 53/1488: training loss=0.59\n",
      "[1768776191] Step 54/1488: training loss=0.74\n",
      "[1768776191] Step 55/1488: training loss=2.32\n",
      "[1768776193] Step 56/1488: training loss=2.21\n",
      "[1768776193] Step 57/1488: training loss=0.65\n",
      "[1768776193] Step 58/1488: training loss=1.95\n",
      "[1768776195] Step 59/1488: training loss=1.67\n",
      "[1768776195] Step 60/1488: training loss=0.98\n",
      "[1768776195] Step 61/1488: training loss=0.55\n",
      "[1768776197] Step 62/1488: training loss=1.65\n",
      "[1768776199] Step 63/1488: training loss=0.35\n",
      "[1768776199] Step 64/1488: training loss=2.23\n",
      "[1768776202] Step 65/1488: training loss=0.86\n",
      "[1768776204] Step 66/1488: training loss=0.69\n",
      "[1768776204] Step 67/1488: training loss=0.37\n",
      "[1768776204] Step 68/1488: training loss=2.56\n",
      "[1768776206] Step 69/1488: training loss=1.94\n",
      "[1768776206] Step 70/1488: training loss=2.95\n",
      "[1768776208] Step 71/1488: training loss=0.67\n",
      "[1768776210] Step 72/1488: training loss=0.74\n",
      "[1768776210] Step 73/1488: training loss=0.80\n",
      "[1768776212] Step 74/1488: training loss=0.53\n",
      "[1768776212] Step 75/1488: training loss=0.96\n",
      "[1768776214] Step 76/1488: training loss=0.76\n",
      "[1768776214] Step 77/1488: training loss=0.65\n",
      "[1768776214] Step 78/1488: training loss=0.65\n",
      "[1768776216] Step 79/1488: training loss=0.97\n",
      "[1768776216] Step 80/1488: training loss=0.58\n",
      "[1768776218] Step 81/1488: training loss=0.43\n",
      "[1768776218] Step 82/1488: training loss=0.74\n",
      "[1768776220] Step 83/1488: training loss=0.57\n",
      "[1768776222] Step 84/1488: training loss=0.90\n",
      "[1768776224] Step 85/1488: training loss=3.36\n",
      "[1768776224] Step 86/1488: training loss=0.62\n",
      "[1768776224] Step 87/1488: training loss=0.51\n",
      "[1768776226] Step 88/1488: training loss=0.51\n",
      "[1768776226] Step 89/1488: training loss=2.26\n",
      "[1768776228] Step 90/1488: training loss=0.48\n",
      "[1768776228] Step 91/1488: training loss=0.56\n",
      "[1768776230] Step 92/1488: training loss=0.57\n",
      "[1768776232] Step 93/1488: training loss=0.60\n",
      "[1768776232] Step 94/1488: training loss=0.63\n",
      "[1768776234] Step 95/1488: training loss=0.45\n",
      "[1768776234] Step 96/1488: training loss=2.27\n",
      "[1768776236] Step 97/1488: training loss=0.77\n",
      "[1768776238] Step 98/1488: training loss=2.05\n",
      "[1768776238] Step 99/1488: training loss=2.19\n",
      "[1768776238] Step 100/1488: training loss=0.63\n",
      "[1768776241] Step 101/1488: training loss=0.30\n",
      "[1768776241] Step 102/1488: training loss=0.51\n",
      "[1768776243] Step 103/1488: training loss=2.48\n",
      "[1768776243] Step 104/1488: training loss=0.66\n",
      "[1768776245] Step 105/1488: training loss=0.45\n",
      "[1768776245] Step 106/1488: training loss=0.66\n",
      "[1768776247] Step 107/1488: training loss=0.55\n",
      "[1768776249] Step 108/1488: training loss=0.81\n",
      "[1768776249] Step 109/1488: training loss=1.63\n",
      "[1768776251] Step 110/1488: training loss=0.57\n",
      "[1768776253] Step 111/1488: training loss=0.55\n",
      "[1768776255] Step 112/1488: training loss=0.45\n",
      "[1768776255] Step 113/1488: training loss=0.69\n",
      "[1768776257] Step 114/1488: training loss=0.74\n",
      "[1768776259] Step 115/1488: training loss=0.36\n",
      "[1768776259] Step 116/1488: training loss=2.53\n",
      "[1768776261] Step 117/1488: training loss=2.17\n",
      "[1768776261] Step 118/1488: training loss=0.75\n",
      "[1768776263] Step 119/1488: training loss=0.38\n",
      "[1768776263] Step 120/1488: training loss=0.95\n",
      "[1768776265] Step 121/1488: training loss=0.58\n",
      "[1768776267] Step 122/1488: training loss=0.94\n",
      "[1768776267] Step 123/1488: training loss=0.81\n",
      "[1768776269] Step 124/1488: training loss=0.62\n",
      "[1768776269] Step 125/1488: training loss=0.85\n",
      "[1768776271] Step 126/1488: training loss=0.66\n",
      "[1768776271] Step 127/1488: training loss=2.26\n",
      "[1768776273] Step 128/1488: training loss=0.65\n",
      "[1768776273] Step 129/1488: training loss=0.69\n",
      "[1768776273] Step 130/1488: training loss=0.52\n",
      "[1768776275] Step 131/1488: training loss=0.63\n",
      "[1768776277] Step 132/1488: training loss=0.43\n",
      "[1768776279] Step 133/1488: training loss=0.57\n",
      "[1768776279] Step 134/1488: training loss=0.70\n",
      "[1768776281] Step 135/1488: training loss=0.66\n",
      "[1768776281] Step 136/1488: training loss=0.49\n",
      "[1768776284] Step 137/1488: training loss=0.78\n",
      "[1768776284] Step 138/1488: training loss=0.86\n",
      "[1768776286] Step 139/1488: training loss=0.35\n",
      "[1768776288] Step 140/1488: training loss=2.65\n",
      "[1768776288] Step 141/1488: training loss=0.70\n",
      "[1768776290] Step 142/1488: training loss=0.65\n",
      "[1768776290] Step 143/1488: training loss=0.36\n",
      "[1768776292] Step 144/1488: training loss=2.41\n",
      "[1768776292] Step 145/1488: training loss=0.76\n",
      "[1768776292] Step 146/1488: training loss=2.50\n",
      "[1768776294] Step 147/1488: training loss=0.65\n",
      "[1768776296] Step 148/1488: training loss=0.38\n",
      "[1768776298] Step 149/1488: training loss=0.44\n",
      "[1768776298] Step 150/1488: training loss=0.70\n",
      "[1768776300] Step 151/1488: training loss=0.71\n",
      "[1768776300] Step 152/1488: training loss=2.53\n",
      "[1768776302] Step 153/1488: training loss=0.63\n",
      "[1768776302] Step 154/1488: training loss=0.56\n",
      "[1768776302] Step 155/1488: training loss=2.06\n",
      "[1768776304] Step 156/1488: training loss=0.80\n",
      "[1768776304] Step 157/1488: training loss=2.68\n",
      "[1768776304] Step 158/1488: training loss=0.87\n",
      "[1768776306] Step 159/1488: training loss=1.79\n",
      "[1768776308] Step 160/1488: training loss=0.88\n",
      "[1768776310] Step 161/1488: training loss=0.88\n",
      "[1768776310] Step 162/1488: training loss=0.89\n",
      "[1768776312] Step 163/1488: training loss=2.34\n",
      "[1768776314] Step 164/1488: training loss=0.75\n",
      "[1768776314] Step 165/1488: training loss=0.41\n",
      "[1768776314] Step 166/1488: training loss=0.76\n",
      "[1768776317] Step 167/1488: training loss=3.14\n",
      "[1768776319] Step 168/1488: training loss=0.71\n",
      "[1768776321] Step 169/1488: training loss=0.69\n",
      "[1768776321] Step 170/1488: training loss=2.91\n",
      "[1768776323] Step 171/1488: training loss=0.66\n",
      "[1768776325] Step 172/1488: training loss=0.81\n",
      "[1768776325] Step 173/1488: training loss=0.76\n",
      "[1768776327] Step 174/1488: training loss=2.05\n",
      "[1768776329] Step 175/1488: training loss=0.72\n",
      "[1768776329] Step 176/1488: training loss=0.31\n",
      "[1768776331] Step 177/1488: training loss=0.81\n",
      "[1768776333] Step 178/1488: training loss=0.65\n",
      "[1768776333] Step 179/1488: training loss=2.21\n",
      "[1768776333] Step 180/1488: training loss=0.85\n",
      "[1768776335] Step 181/1488: training loss=0.34\n",
      "[1768776337] Step 182/1488: training loss=0.72\n",
      "[1768776337] Step 183/1488: training loss=0.95\n",
      "[1768776339] Step 184/1488: training loss=2.63\n",
      "[1768776339] Step 185/1488: training loss=2.38\n",
      "[1768776339] Step 186/1488: training loss=0.43\n",
      "[1768776341] Step 187/1488: training loss=0.64\n",
      "[1768776341] Step 188/1488: training loss=0.46\n",
      "[1768776341] Step 189/1488: training loss=2.68\n",
      "[1768776343] Step 190/1488: training loss=0.55\n",
      "[1768776343] Step 191/1488: training loss=2.01\n",
      "[1768776345] Step 192/1488: training loss=2.05\n",
      "[1768776347] Step 193/1488: training loss=0.84\n",
      "[1768776347] Step 194/1488: training loss=2.48\n",
      "[1768776349] Step 195/1488: training loss=0.55\n",
      "[1768776349] Step 196/1488: training loss=0.67\n",
      "[1768776352] Step 197/1488: training loss=0.98\n",
      "[1768776354] Step 198/1488: training loss=0.87\n",
      "[1768776354] Step 199/1488: training loss=0.71\n",
      "[1768776356] Step 200/1488: training loss=1.84\n",
      "[1768776358] Step 201/1488: training loss=0.38\n",
      "[1768776358] Step 202/1488: training loss=2.45\n",
      "[1768776360] Step 203/1488: training loss=0.72\n",
      "[1768776364] Step 204/1488: training loss=0.68\n",
      "[1768776364] Step 205/1488: training loss=0.92\n",
      "[1768776366] Step 206/1488: training loss=0.68\n",
      "[1768776368] Step 207/1488: training loss=0.74\n",
      "[1768776370] Step 208/1488: training loss=0.81\n",
      "[1768776372] Step 209/1488: training loss=0.64\n",
      "[1768776372] Step 210/1488: training loss=0.61\n",
      "[1768776372] Step 211/1488: training loss=2.78\n",
      "[1768776374] Step 212/1488: training loss=1.70\n",
      "[1768776374] Step 213/1488: training loss=1.99\n",
      "[1768776376] Step 214/1488: training loss=0.57\n",
      "[1768776378] Step 215/1488: training loss=0.69\n",
      "[1768776378] Step 216/1488: training loss=0.70\n",
      "[1768776378] Step 217/1488: training loss=1.97\n",
      "[1768776380] Step 218/1488: training loss=0.84\n",
      "[1768776382] Step 219/1488: training loss=0.56\n",
      "[1768776384] Step 220/1488: training loss=0.61\n",
      "[1768776386] Step 221/1488: training loss=2.28\n",
      "[1768776386] Step 222/1488: training loss=0.77\n",
      "[1768776389] Step 223/1488: training loss=0.57\n",
      "[1768776391] Step 224/1488: training loss=0.60\n",
      "[1768776391] Step 225/1488: training loss=2.41\n",
      "[1768776391] Step 226/1488: training loss=0.66\n",
      "[1768776393] Step 227/1488: training loss=1.82\n",
      "[1768776395] Step 228/1488: training loss=0.73\n",
      "[1768776395] Step 229/1488: training loss=2.22\n",
      "[1768776397] Step 230/1488: training loss=0.60\n",
      "[1768776397] Step 231/1488: training loss=0.75\n",
      "[1768776399] Step 232/1488: training loss=0.74\n",
      "[1768776399] Step 233/1488: training loss=0.42\n",
      "[1768776401] Step 234/1488: training loss=0.52\n",
      "[1768776401] Step 235/1488: training loss=0.59\n",
      "[1768776403] Step 236/1488: training loss=2.21\n",
      "[1768776405] Step 237/1488: training loss=0.62\n",
      "[1768776405] Step 238/1488: training loss=0.60\n",
      "[1768776407] Step 239/1488: training loss=0.68\n",
      "[1768776407] Step 240/1488: training loss=0.88\n",
      "[1768776409] Step 241/1488: training loss=0.63\n",
      "[1768776409] Step 242/1488: training loss=0.41\n",
      "[1768776411] Step 243/1488: training loss=0.56\n",
      "[1768776411] Step 244/1488: training loss=0.73\n",
      "[1768776413] Step 245/1488: training loss=1.62\n",
      "[1768776413] Step 246/1488: training loss=0.62\n",
      "[1768776415] Step 247/1488: training loss=0.59\n",
      "[1768776415] Step 248/1488: training loss=1.93\n",
      "[1768776419] Step 249/1488: training loss=0.63\n",
      "[1768776419] Step 250/1488: training loss=2.81\n",
      "[1768776421] Step 251/1488: training loss=0.69\n",
      "[1768776421] Step 252/1488: training loss=0.46\n",
      "[1768776421] Step 253/1488: training loss=0.62\n",
      "[1768776423] Step 254/1488: training loss=0.90\n",
      "[1768776425] Step 255/1488: training loss=0.58\n",
      "[1768776425] Step 256/1488: training loss=0.61\n",
      "[1768776427] Step 257/1488: training loss=0.65\n",
      "[1768776430] Step 258/1488: training loss=0.45\n",
      "[1768776432] Step 259/1488: training loss=0.89\n",
      "[1768776436] Step 260/1488: training loss=0.55\n",
      "[1768776436] Step 261/1488: training loss=0.75\n",
      "[1768776438] Step 262/1488: training loss=2.58\n",
      "[1768776440] Step 263/1488: training loss=0.74\n",
      "[1768776440] Step 264/1488: training loss=2.35\n",
      "[1768776442] Step 265/1488: training loss=0.44\n",
      "[1768776442] Step 266/1488: training loss=0.72\n",
      "[1768776444] Step 267/1488: training loss=0.47\n",
      "[1768776444] Step 268/1488: training loss=1.68\n",
      "[1768776444] Step 269/1488: training loss=0.68\n",
      "[1768776446] Step 270/1488: training loss=0.61\n",
      "[1768776446] Step 271/1488: training loss=0.65\n",
      "[1768776448] Step 272/1488: training loss=0.35\n",
      "[1768776450] Step 273/1488: training loss=0.86\n",
      "[1768776450] Step 274/1488: training loss=0.84\n",
      "[1768776450] Step 275/1488: training loss=1.70\n",
      "[1768776452] Step 276/1488: training loss=1.50\n",
      "[1768776454] Step 277/1488: training loss=0.56\n",
      "[1768776454] Step 278/1488: training loss=2.15\n",
      "[1768776456] Step 279/1488: training loss=0.65\n",
      "[1768776458] Step 280/1488: training loss=0.57\n",
      "[1768776458] Step 281/1488: training loss=0.60\n",
      "[1768776458] Step 282/1488: training loss=0.79\n",
      "[1768776462] Step 283/1488: training loss=0.93\n",
      "[1768776464] Step 284/1488: training loss=1.09\n",
      "[1768776464] Step 285/1488: training loss=1.97\n",
      "[1768776466] Step 286/1488: training loss=0.84\n",
      "[1768776468] Step 287/1488: training loss=2.83\n",
      "[1768776468] Step 288/1488: training loss=0.47\n",
      "[1768776468] Step 289/1488: training loss=2.04\n",
      "[1768776471] Step 290/1488: training loss=0.42\n",
      "[1768776471] Step 291/1488: training loss=3.05\n",
      "[1768776473] Step 292/1488: training loss=1.96\n",
      "[1768776473] Step 293/1488: training loss=1.54\n",
      "[1768776475] Step 294/1488: training loss=0.46\n",
      "[1768776477] Step 295/1488: training loss=1.96\n",
      "[1768776479] Step 296/1488: training loss=0.74\n",
      "[1768776479] Step 297/1488: training loss=0.83\n",
      "[1768776481] Step 298/1488: training loss=0.43\n",
      "[1768776483] Step 299/1488: training loss=0.82\n",
      "[1768776483] Step 300/1488: training loss=2.07\n",
      "[1768776485] Step 301/1488: training loss=0.70\n",
      "[1768776485] Step 302/1488: training loss=0.81\n",
      "[1768776487] Step 303/1488: training loss=0.58\n",
      "[1768776487] Step 304/1488: training loss=2.47\n",
      "[1768776489] Step 305/1488: training loss=0.68\n",
      "[1768776489] Step 306/1488: training loss=0.65\n",
      "[1768776491] Step 307/1488: training loss=0.59\n",
      "[1768776491] Step 308/1488: training loss=0.84\n",
      "[1768776493] Step 309/1488: training loss=0.54\n",
      "[1768776493] Step 310/1488: training loss=0.50\n",
      "[1768776495] Step 311/1488: training loss=0.56\n",
      "[1768776495] Step 312/1488: training loss=0.36\n",
      "[1768776497] Step 313/1488: training loss=0.74\n",
      "[1768776499] Step 314/1488: training loss=0.77\n",
      "[1768776501] Step 315/1488: training loss=0.77\n",
      "[1768776503] Step 316/1488: training loss=0.56\n",
      "[1768776503] Step 317/1488: training loss=0.66\n",
      "[1768776505] Step 318/1488: training loss=2.46\n",
      "[1768776507] Step 319/1488: training loss=0.60\n",
      "[1768776509] Step 320/1488: training loss=0.69\n",
      "[1768776510] Step 321/1488: training loss=2.20\n",
      "[1768776512] Step 322/1488: training loss=0.83\n",
      "[1768776514] Step 323/1488: training loss=0.76\n",
      "[1768776514] Step 324/1488: training loss=1.78\n",
      "[1768776516] Step 325/1488: training loss=0.58\n",
      "[1768776516] Step 326/1488: training loss=0.50\n",
      "[1768776518] Step 327/1488: training loss=0.58\n",
      "[1768776520] Step 328/1488: training loss=0.62\n",
      "[1768776520] Step 329/1488: training loss=1.46\n",
      "[1768776524] Step 330/1488: training loss=0.76\n",
      "[1768776524] Step 331/1488: training loss=0.53\n",
      "[1768776524] Step 332/1488: training loss=3.22\n",
      "[1768776526] Step 333/1488: training loss=0.50\n",
      "[1768776526] Step 334/1488: training loss=0.40\n",
      "[1768776528] Step 335/1488: training loss=0.59\n",
      "[1768776528] Step 336/1488: training loss=0.63\n",
      "[1768776530] Step 337/1488: training loss=0.49\n",
      "[1768776532] Step 338/1488: training loss=0.67\n",
      "[1768776534] Step 339/1488: training loss=0.63\n",
      "[1768776536] Step 340/1488: training loss=0.66\n",
      "[1768776536] Step 341/1488: training loss=0.52\n",
      "[1768776536] Step 342/1488: training loss=0.92\n",
      "[1768776538] Step 343/1488: training loss=0.63\n",
      "[1768776540] Step 344/1488: training loss=0.57\n",
      "[1768776542] Step 345/1488: training loss=0.62\n",
      "[1768776542] Step 346/1488: training loss=0.68\n",
      "[1768776544] Step 347/1488: training loss=0.55\n",
      "[1768776546] Step 348/1488: training loss=0.47\n",
      "[1768776546] Step 349/1488: training loss=0.57\n",
      "[1768776548] Step 350/1488: training loss=0.60\n",
      "[1768776548] Step 351/1488: training loss=2.08\n",
      "[1768776551] Step 352/1488: training loss=0.39\n",
      "[1768776551] Step 353/1488: training loss=0.61\n",
      "[1768776553] Step 354/1488: training loss=0.66\n",
      "[1768776553] Step 355/1488: training loss=0.85\n",
      "[1768776555] Step 356/1488: training loss=2.31\n",
      "[1768776557] Step 357/1488: training loss=0.57\n",
      "[1768776557] Step 358/1488: training loss=0.47\n",
      "[1768776559] Step 359/1488: training loss=0.62\n",
      "[1768776561] Step 360/1488: training loss=0.87\n",
      "[1768776563] Step 361/1488: training loss=0.56\n",
      "[1768776563] Step 362/1488: training loss=0.69\n",
      "[1768776565] Step 363/1488: training loss=0.56\n",
      "[1768776567] Step 364/1488: training loss=0.69\n",
      "[1768776567] Step 365/1488: training loss=0.44\n",
      "[1768776567] Step 366/1488: training loss=0.66\n",
      "[1768776571] Step 367/1488: training loss=0.69\n",
      "[1768776571] Step 368/1488: training loss=0.49\n",
      "[1768776573] Step 369/1488: training loss=0.88\n",
      "[1768776575] Step 370/1488: training loss=0.59\n",
      "[1768776575] Step 371/1488: training loss=1.85\n",
      "[1768776577] Step 372/1488: training loss=0.81\n",
      "[1768776579] Step 373/1488: training loss=0.91\n",
      "[1768776579] Step 374/1488: training loss=0.73\n",
      "[1768776581] Step 375/1488: training loss=0.68\n",
      "[1768776583] Step 376/1488: training loss=0.59\n",
      "[1768776583] Step 377/1488: training loss=0.40\n",
      "[1768776585] Step 378/1488: training loss=0.48\n",
      "[1768776585] Step 379/1488: training loss=0.64\n",
      "[1768776587] Step 380/1488: training loss=1.49\n",
      "[1768776587] Step 381/1488: training loss=0.48\n",
      "[1768776589] Step 382/1488: training loss=0.56\n",
      "[1768776592] Step 383/1488: training loss=0.82\n",
      "[1768776594] Step 384/1488: training loss=0.76\n",
      "[1768776596] Step 385/1488: training loss=0.66\n",
      "[1768776596] Step 386/1488: training loss=0.67\n",
      "[1768776598] Step 387/1488: training loss=0.69\n",
      "[1768776598] Step 388/1488: training loss=1.94\n",
      "[1768776598] Step 389/1488: training loss=0.56\n",
      "[1768776600] Step 390/1488: training loss=0.43\n",
      "[1768776602] Step 391/1488: training loss=0.43\n",
      "[1768776602] Step 392/1488: training loss=0.65\n",
      "[1768776604] Step 393/1488: training loss=0.61\n",
      "[1768776606] Step 394/1488: training loss=0.73\n",
      "[1768776606] Step 395/1488: training loss=0.96\n",
      "[1768776608] Step 396/1488: training loss=0.87\n",
      "[1768776608] Step 397/1488: training loss=2.72\n",
      "[1768776612] Step 398/1488: training loss=0.73\n",
      "[1768776612] Step 399/1488: training loss=0.62\n",
      "[1768776614] Step 400/1488: training loss=0.61\n",
      "[1768776614] Step 401/1488: training loss=1.59\n",
      "[1768776616] Step 402/1488: training loss=0.35\n",
      "[1768776618] Step 403/1488: training loss=0.73\n",
      "[1768776618] Step 404/1488: training loss=0.28\n",
      "[1768776620] Step 405/1488: training loss=0.69\n",
      "[1768776620] Step 406/1488: training loss=0.58\n",
      "[1768776622] Step 407/1488: training loss=2.36\n",
      "[1768776622] Step 408/1488: training loss=0.61\n",
      "[1768776622] Step 409/1488: training loss=0.16\n",
      "[1768776624] Step 410/1488: training loss=0.60\n",
      "[1768776626] Step 411/1488: training loss=0.67\n",
      "[1768776626] Step 412/1488: training loss=2.67\n",
      "[1768776628] Step 413/1488: training loss=0.77\n",
      "[1768776628] Step 414/1488: training loss=0.71\n",
      "[1768776631] Step 415/1488: training loss=2.44\n",
      "[1768776631] Step 416/1488: training loss=0.52\n",
      "[1768776631] Step 417/1488: training loss=0.32\n",
      "[1768776633] Step 418/1488: training loss=0.45\n",
      "[1768776635] Step 419/1488: training loss=0.25\n",
      "[1768776635] Step 420/1488: training loss=0.29\n",
      "[1768776637] Step 421/1488: training loss=0.56\n",
      "[1768776639] Step 422/1488: training loss=0.25\n",
      "[1768776639] Step 423/1488: training loss=0.46\n",
      "[1768776639] Step 424/1488: training loss=2.00\n",
      "[1768776641] Step 425/1488: training loss=0.50\n",
      "[1768776641] Step 426/1488: training loss=0.64\n",
      "[1768776641] Step 427/1488: training loss=3.21\n",
      "[1768776643] Step 428/1488: training loss=2.21\n",
      "[1768776645] Step 429/1488: training loss=0.61\n",
      "[1768776645] Step 430/1488: training loss=0.55\n",
      "[1768776647] Step 431/1488: training loss=0.94\n",
      "[1768776647] Step 432/1488: training loss=0.46\n",
      "[1768776649] Step 433/1488: training loss=0.54\n",
      "[1768776649] Step 434/1488: training loss=2.25\n",
      "[1768776651] Step 435/1488: training loss=0.69\n",
      "[1768776653] Step 436/1488: training loss=0.87\n",
      "[1768776653] Step 437/1488: training loss=0.55\n",
      "[1768776655] Step 438/1488: training loss=0.68\n",
      "[1768776655] Step 439/1488: training loss=1.94\n",
      "[1768776657] Step 440/1488: training loss=0.48\n",
      "[1768776659] Step 441/1488: training loss=0.41\n",
      "[1768776661] Step 442/1488: training loss=0.63\n",
      "[1768776661] Step 443/1488: training loss=0.88\n",
      "[1768776663] Step 444/1488: training loss=0.88\n",
      "[1768776665] Step 445/1488: training loss=0.66\n",
      "[1768776665] Step 446/1488: training loss=0.51\n",
      "[1768776667] Step 447/1488: training loss=0.58\n",
      "[1768776667] Step 448/1488: training loss=2.41\n",
      "[1768776669] Step 449/1488: training loss=0.67\n",
      "[1768776669] Step 450/1488: training loss=2.47\n",
      "[1768776672] Step 451/1488: training loss=2.10\n",
      "[1768776672] Step 452/1488: training loss=0.66\n",
      "[1768776674] Step 453/1488: training loss=0.62\n",
      "[1768776674] Step 454/1488: training loss=0.45\n",
      "[1768776676] Step 455/1488: training loss=0.83\n",
      "[1768776678] Step 456/1488: training loss=0.75\n",
      "[1768776680] Step 457/1488: training loss=1.04\n",
      "[1768776680] Step 458/1488: training loss=0.55\n",
      "[1768776680] Step 459/1488: training loss=0.69\n",
      "[1768776684] Step 460/1488: training loss=1.39\n",
      "[1768776684] Step 461/1488: training loss=2.63\n",
      "[1768776684] Step 462/1488: training loss=0.72\n",
      "[1768776688] Step 463/1488: training loss=0.82\n",
      "[1768776690] Step 464/1488: training loss=0.95\n",
      "[1768776690] Step 465/1488: training loss=1.81\n",
      "[1768776692] Step 466/1488: training loss=0.74\n",
      "[1768776694] Step 467/1488: training loss=0.59\n",
      "[1768776694] Step 468/1488: training loss=0.55\n",
      "[1768776696] Step 469/1488: training loss=0.97\n",
      "[1768776698] Step 470/1488: training loss=0.72\n",
      "[1768776700] Step 471/1488: training loss=3.18\n",
      "[1768776702] Step 472/1488: training loss=0.72\n",
      "[1768776704] Step 473/1488: training loss=0.68\n",
      "[1768776706] Step 474/1488: training loss=0.84\n",
      "[1768776706] Step 475/1488: training loss=0.50\n",
      "[1768776708] Step 476/1488: training loss=0.40\n",
      "[1768776708] Step 477/1488: training loss=0.71\n",
      "[1768776710] Step 478/1488: training loss=0.42\n",
      "[1768776710] Step 479/1488: training loss=0.66\n",
      "[1768776710] Step 480/1488: training loss=2.47\n",
      "[1768776713] Step 481/1488: training loss=0.65\n",
      "[1768776715] Step 482/1488: training loss=0.54\n",
      "[1768776717] Step 483/1488: training loss=0.78\n",
      "[1768776717] Step 484/1488: training loss=0.77\n",
      "[1768776719] Step 485/1488: training loss=2.52\n",
      "[1768776719] Step 486/1488: training loss=0.55\n",
      "[1768776721] Step 487/1488: training loss=0.82\n",
      "[1768776723] Step 488/1488: training loss=0.56\n",
      "[1768776725] Step 489/1488: training loss=0.61\n",
      "[1768776725] Step 490/1488: training loss=0.80\n",
      "[1768776727] Step 491/1488: training loss=0.34\n",
      "[1768776731] Step 492/1488: training loss=0.64\n",
      "[1768776731] Step 493/1488: training loss=0.71\n",
      "[1768776733] Step 494/1488: training loss=0.59\n",
      "[1768776733] Step 495/1488: training loss=2.72\n",
      "[1768776735] Step 496/1488: training loss=0.69\n",
      "[1768776739] Step 497/1488: training loss=0.41\n",
      "[1768776739] Step 498/1488: training loss=0.44\n",
      "[1768776739] Step 499/1488: training loss=2.79\n",
      "[1768776739] Step 500/1488: training loss=0.47\n",
      "[1768776741] Step 501/1488: training loss=0.34\n",
      "[1768776741] Step 502/1488: training loss=0.34\n",
      "[1768776743] Step 503/1488: training loss=2.42\n",
      "[1768776743] Step 504/1488: training loss=0.17\n",
      "[1768776745] Step 505/1488: training loss=0.54\n",
      "[1768776748] Step 506/1488: training loss=0.40\n",
      "[1768776748] Step 507/1488: training loss=0.47\n",
      "[1768776748] Step 508/1488: training loss=0.27\n",
      "[1768776750] Step 509/1488: training loss=0.45\n",
      "[1768776750] Step 510/1488: training loss=0.39\n",
      "[1768776752] Step 511/1488: training loss=1.75\n",
      "[1768776752] Step 512/1488: training loss=0.31\n",
      "[1768776754] Step 513/1488: training loss=0.56\n",
      "[1768776754] Step 514/1488: training loss=0.32\n",
      "[1768776756] Step 515/1488: training loss=0.67\n",
      "[1768776756] Step 516/1488: training loss=0.42\n",
      "[1768776758] Step 517/1488: training loss=0.40\n",
      "[1768776758] Step 518/1488: training loss=0.32\n",
      "[1768776760] Step 519/1488: training loss=0.33\n",
      "[1768776760] Step 520/1488: training loss=0.33\n",
      "[1768776762] Step 521/1488: training loss=0.42\n",
      "[1768776762] Step 522/1488: training loss=0.29\n",
      "[1768776764] Step 523/1488: training loss=1.33\n",
      "[1768776764] Step 524/1488: training loss=0.32\n",
      "[1768776766] Step 525/1488: training loss=1.95\n",
      "[1768776766] Step 526/1488: training loss=0.20\n",
      "[1768776768] Step 527/1488: training loss=0.32\n",
      "[1768776768] Step 528/1488: training loss=0.44\n",
      "[1768776768] Step 529/1488: training loss=0.21\n",
      "[1768776770] Step 530/1488: training loss=0.25\n",
      "[1768776770] Step 531/1488: training loss=0.35\n",
      "[1768776770] Step 532/1488: training loss=0.26\n",
      "[1768776772] Step 533/1488: training loss=2.27\n",
      "[1768776772] Step 534/1488: training loss=0.51\n",
      "[1768776774] Step 535/1488: training loss=0.24\n",
      "[1768776774] Step 536/1488: training loss=0.33\n",
      "[1768776774] Step 537/1488: training loss=0.28\n",
      "[1768776776] Step 538/1488: training loss=0.31\n",
      "[1768776776] Step 539/1488: training loss=0.52\n",
      "[1768776780] Step 540/1488: training loss=0.40\n",
      "[1768776780] Step 541/1488: training loss=0.16\n",
      "[1768776780] Step 542/1488: training loss=0.53\n",
      "[1768776782] Step 543/1488: training loss=0.53\n",
      "[1768776782] Step 544/1488: training loss=2.80\n",
      "[1768776784] Step 545/1488: training loss=0.45\n",
      "[1768776784] Step 546/1488: training loss=0.75\n",
      "[1768776787] Step 547/1488: training loss=1.00\n",
      "[1768776787] Step 548/1488: training loss=2.26\n",
      "[1768776789] Step 549/1488: training loss=0.79\n",
      "[1768776789] Step 550/1488: training loss=0.42\n",
      "[1768776791] Step 551/1488: training loss=0.33\n",
      "[1768776791] Step 552/1488: training loss=0.25\n",
      "[1768776791] Step 553/1488: training loss=0.35\n",
      "[1768776793] Step 554/1488: training loss=0.51\n",
      "[1768776793] Step 555/1488: training loss=0.59\n",
      "[1768776795] Step 556/1488: training loss=0.24\n",
      "[1768776795] Step 557/1488: training loss=0.53\n",
      "[1768776795] Step 558/1488: training loss=0.27\n",
      "[1768776797] Step 559/1488: training loss=0.20\n",
      "[1768776799] Step 560/1488: training loss=0.26\n",
      "[1768776799] Step 561/1488: training loss=0.75\n",
      "[1768776801] Step 562/1488: training loss=0.14\n",
      "[1768776801] Step 563/1488: training loss=0.48\n",
      "[1768776803] Step 564/1488: training loss=0.26\n",
      "[1768776803] Step 565/1488: training loss=2.13\n",
      "[1768776803] Step 566/1488: training loss=0.53\n",
      "[1768776805] Step 567/1488: training loss=0.22\n",
      "[1768776807] Step 568/1488: training loss=0.44\n",
      "[1768776807] Step 569/1488: training loss=1.68\n",
      "[1768776807] Step 570/1488: training loss=1.46\n",
      "[1768776809] Step 571/1488: training loss=1.93\n",
      "[1768776809] Step 572/1488: training loss=0.31\n",
      "[1768776811] Step 573/1488: training loss=0.72\n",
      "[1768776811] Step 574/1488: training loss=2.45\n",
      "[1768776811] Step 575/1488: training loss=0.20\n",
      "[1768776813] Step 576/1488: training loss=0.38\n",
      "[1768776815] Step 577/1488: training loss=0.33\n",
      "[1768776815] Step 578/1488: training loss=0.30\n",
      "[1768776817] Step 579/1488: training loss=2.12\n",
      "[1768776817] Step 580/1488: training loss=0.26\n",
      "[1768776819] Step 581/1488: training loss=0.42\n",
      "[1768776819] Step 582/1488: training loss=0.36\n",
      "[1768776819] Step 583/1488: training loss=0.37\n",
      "[1768776821] Step 584/1488: training loss=0.52\n",
      "[1768776821] Step 585/1488: training loss=0.33\n",
      "[1768776823] Step 586/1488: training loss=1.73\n",
      "[1768776823] Step 587/1488: training loss=1.20\n",
      "[1768776826] Step 588/1488: training loss=1.10\n",
      "[1768776826] Step 589/1488: training loss=0.37\n",
      "[1768776828] Step 590/1488: training loss=0.41\n",
      "[1768776830] Step 591/1488: training loss=0.33\n",
      "[1768776830] Step 592/1488: training loss=0.38\n",
      "[1768776830] Step 593/1488: training loss=0.29\n",
      "[1768776832] Step 594/1488: training loss=0.37\n",
      "[1768776834] Step 595/1488: training loss=1.76\n",
      "[1768776834] Step 596/1488: training loss=0.41\n",
      "[1768776836] Step 597/1488: training loss=0.56\n",
      "[1768776836] Step 598/1488: training loss=0.50\n",
      "[1768776838] Step 599/1488: training loss=0.40\n",
      "[1768776838] Step 600/1488: training loss=0.46\n",
      "[1768776840] Step 601/1488: training loss=0.23\n",
      "[1768776840] Step 602/1488: training loss=0.38\n",
      "[1768776842] Step 603/1488: training loss=0.28\n",
      "[1768776842] Step 604/1488: training loss=0.29\n",
      "[1768776842] Step 605/1488: training loss=0.38\n",
      "[1768776844] Step 606/1488: training loss=0.52\n",
      "[1768776844] Step 607/1488: training loss=0.31\n",
      "[1768776846] Step 608/1488: training loss=0.17\n",
      "[1768776846] Step 609/1488: training loss=0.37\n",
      "[1768776848] Step 610/1488: training loss=2.61\n",
      "[1768776848] Step 611/1488: training loss=0.23\n",
      "[1768776850] Step 612/1488: training loss=0.44\n",
      "[1768776850] Step 613/1488: training loss=2.65\n",
      "[1768776852] Step 614/1488: training loss=1.87\n",
      "[1768776852] Step 615/1488: training loss=0.28\n",
      "[1768776854] Step 616/1488: training loss=0.41\n",
      "[1768776854] Step 617/1488: training loss=0.18\n",
      "[1768776854] Step 618/1488: training loss=0.27\n",
      "[1768776856] Step 619/1488: training loss=0.35\n",
      "[1768776856] Step 620/1488: training loss=0.35\n",
      "[1768776858] Step 621/1488: training loss=0.47\n",
      "[1768776858] Step 622/1488: training loss=0.29\n",
      "[1768776861] Step 623/1488: training loss=0.54\n",
      "[1768776861] Step 624/1488: training loss=0.30\n",
      "[1768776861] Step 625/1488: training loss=0.19\n",
      "[1768776863] Step 626/1488: training loss=0.24\n",
      "[1768776863] Step 627/1488: training loss=0.33\n",
      "[1768776865] Step 628/1488: training loss=0.13\n",
      "[1768776865] Step 629/1488: training loss=0.20\n",
      "[1768776867] Step 630/1488: training loss=1.99\n",
      "[1768776867] Step 631/1488: training loss=0.20\n",
      "[1768776867] Step 632/1488: training loss=0.33\n",
      "[1768776869] Step 633/1488: training loss=0.56\n",
      "[1768776871] Step 634/1488: training loss=1.21\n",
      "[1768776871] Step 635/1488: training loss=0.21\n",
      "[1768776871] Step 636/1488: training loss=1.19\n",
      "[1768776873] Step 637/1488: training loss=1.42\n",
      "[1768776873] Step 638/1488: training loss=2.36\n",
      "[1768776875] Step 639/1488: training loss=0.73\n",
      "[1768776875] Step 640/1488: training loss=0.27\n",
      "[1768776877] Step 641/1488: training loss=0.49\n",
      "[1768776877] Step 642/1488: training loss=1.78\n",
      "[1768776877] Step 643/1488: training loss=0.29\n",
      "[1768776879] Step 644/1488: training loss=0.53\n",
      "[1768776881] Step 645/1488: training loss=0.28\n",
      "[1768776881] Step 646/1488: training loss=0.35\n",
      "[1768776883] Step 647/1488: training loss=0.37\n",
      "[1768776883] Step 648/1488: training loss=0.38\n",
      "[1768776885] Step 649/1488: training loss=2.00\n",
      "[1768776885] Step 650/1488: training loss=0.25\n",
      "[1768776887] Step 651/1488: training loss=1.67\n",
      "[1768776887] Step 652/1488: training loss=0.58\n",
      "[1768776889] Step 653/1488: training loss=1.74\n",
      "[1768776889] Step 654/1488: training loss=0.26\n",
      "[1768776889] Step 655/1488: training loss=0.44\n",
      "[1768776891] Step 656/1488: training loss=0.66\n",
      "[1768776891] Step 657/1488: training loss=0.19\n",
      "[1768776893] Step 658/1488: training loss=0.27\n",
      "[1768776893] Step 659/1488: training loss=0.66\n",
      "[1768776893] Step 660/1488: training loss=0.32\n",
      "[1768776895] Step 661/1488: training loss=0.43\n",
      "[1768776895] Step 662/1488: training loss=0.49\n",
      "[1768776897] Step 663/1488: training loss=0.50\n",
      "[1768776898] Step 664/1488: training loss=0.17\n",
      "[1768776898] Step 665/1488: training loss=0.33\n",
      "[1768776900] Step 666/1488: training loss=0.65\n",
      "[1768776902] Step 667/1488: training loss=0.44\n",
      "[1768776902] Step 668/1488: training loss=1.56\n",
      "[1768776902] Step 669/1488: training loss=0.37\n",
      "[1768776904] Step 670/1488: training loss=0.42\n",
      "[1768776904] Step 671/1488: training loss=0.35\n",
      "[1768776906] Step 672/1488: training loss=2.20\n",
      "[1768776906] Step 673/1488: training loss=0.43\n",
      "[1768776906] Step 674/1488: training loss=0.17\n",
      "[1768776908] Step 675/1488: training loss=1.56\n",
      "[1768776908] Step 676/1488: training loss=0.27\n",
      "[1768776910] Step 677/1488: training loss=1.59\n",
      "[1768776910] Step 678/1488: training loss=0.24\n",
      "[1768776912] Step 679/1488: training loss=0.15\n",
      "[1768776912] Step 680/1488: training loss=0.29\n",
      "[1768776912] Step 681/1488: training loss=0.34\n",
      "[1768776914] Step 682/1488: training loss=0.57\n",
      "[1768776914] Step 683/1488: training loss=0.31\n",
      "[1768776916] Step 684/1488: training loss=0.23\n",
      "[1768776916] Step 685/1488: training loss=0.21\n",
      "[1768776918] Step 686/1488: training loss=0.46\n",
      "[1768776918] Step 687/1488: training loss=0.20\n",
      "[1768776920] Step 688/1488: training loss=0.40\n",
      "[1768776920] Step 689/1488: training loss=0.47\n",
      "[1768776920] Step 690/1488: training loss=1.56\n",
      "[1768776922] Step 691/1488: training loss=0.26\n",
      "[1768776924] Step 692/1488: training loss=0.11\n",
      "[1768776924] Step 693/1488: training loss=1.43\n",
      "[1768776924] Step 694/1488: training loss=2.15\n",
      "[1768776926] Step 695/1488: training loss=0.26\n",
      "[1768776926] Step 696/1488: training loss=0.05\n",
      "[1768776926] Step 697/1488: training loss=0.42\n",
      "[1768776928] Step 698/1488: training loss=0.15\n",
      "[1768776928] Step 699/1488: training loss=0.50\n",
      "[1768776928] Step 700/1488: training loss=0.24\n",
      "[1768776930] Step 701/1488: training loss=0.32\n",
      "[1768776930] Step 702/1488: training loss=1.97\n",
      "[1768776932] Step 703/1488: training loss=0.31\n",
      "[1768776932] Step 704/1488: training loss=0.41\n",
      "[1768776934] Step 705/1488: training loss=0.40\n",
      "[1768776934] Step 706/1488: training loss=1.28\n",
      "[1768776936] Step 707/1488: training loss=0.28\n",
      "[1768776936] Step 708/1488: training loss=0.38\n",
      "[1768776939] Step 709/1488: training loss=0.42\n",
      "[1768776939] Step 710/1488: training loss=0.55\n",
      "[1768776941] Step 711/1488: training loss=0.33\n",
      "[1768776941] Step 712/1488: training loss=0.64\n",
      "[1768776943] Step 713/1488: training loss=0.27\n",
      "[1768776943] Step 714/1488: training loss=0.43\n",
      "[1768776945] Step 715/1488: training loss=0.42\n",
      "[1768776945] Step 716/1488: training loss=2.64\n",
      "[1768776945] Step 717/1488: training loss=0.56\n",
      "[1768776947] Step 718/1488: training loss=0.50\n",
      "[1768776947] Step 719/1488: training loss=0.46\n",
      "[1768776949] Step 720/1488: training loss=1.57\n",
      "[1768776949] Step 721/1488: training loss=0.32\n",
      "[1768776951] Step 722/1488: training loss=2.14\n",
      "[1768776951] Step 723/1488: training loss=1.21\n",
      "[1768776951] Step 724/1488: training loss=0.53\n",
      "[1768776953] Step 725/1488: training loss=0.51\n",
      "[1768776953] Step 726/1488: training loss=0.46\n",
      "[1768776955] Step 727/1488: training loss=2.06\n",
      "[1768776955] Step 728/1488: training loss=0.68\n",
      "[1768776957] Step 729/1488: training loss=0.33\n",
      "[1768776959] Step 730/1488: training loss=0.32\n",
      "[1768776959] Step 731/1488: training loss=0.42\n",
      "[1768776959] Step 732/1488: training loss=1.53\n",
      "[1768776961] Step 733/1488: training loss=2.07\n",
      "[1768776961] Step 734/1488: training loss=0.35\n",
      "[1768776963] Step 735/1488: training loss=0.42\n",
      "[1768776963] Step 736/1488: training loss=0.44\n",
      "[1768776965] Step 737/1488: training loss=0.33\n",
      "[1768776965] Step 738/1488: training loss=0.49\n",
      "[1768776965] Step 739/1488: training loss=0.30\n",
      "[1768776967] Step 740/1488: training loss=0.34\n",
      "[1768776967] Step 741/1488: training loss=0.27\n",
      "[1768776967] Step 742/1488: training loss=0.25\n",
      "[1768776969] Step 743/1488: training loss=0.71\n",
      "[1768776982] Step 744/1488: training loss=1.50\n",
      "[1768776982] Step 749/1488: training loss=1.91\n",
      "[1768776982] Step 750/1488: training loss=2.10\n",
      "[1768776982] Step 751/1488: training loss=2.91\n",
      "[1768776982] Step 752/1488: training loss=0.30\n",
      "[1768776982] Step 753/1488: training loss=0.15\n",
      "[1768776982] Step 754/1488: training loss=2.34\n",
      "[1768776982] Step 755/1488: training loss=1.69\n",
      "[1768776982] Step 756/1488: training loss=0.33\n",
      "[1768776982] Step 757/1488: training loss=0.39\n",
      "[1768776984] Step 758/1488: training loss=0.19\n",
      "[1768776984] Step 759/1488: training loss=0.43\n",
      "[1768776986] Step 760/1488: training loss=1.76\n",
      "[1768776986] Step 761/1488: training loss=0.13\n",
      "[1768776986] Step 762/1488: training loss=0.42\n",
      "[1768776988] Step 763/1488: training loss=0.21\n",
      "[1768776990] Step 764/1488: training loss=2.06\n",
      "[1768776990] Step 765/1488: training loss=1.99\n",
      "[1768776992] Step 766/1488: training loss=0.24\n",
      "[1768776992] Step 767/1488: training loss=0.62\n",
      "[1768776994] Step 768/1488: training loss=0.72\n",
      "[1768776994] Step 769/1488: training loss=1.80\n",
      "[1768776997] Step 770/1488: training loss=0.30\n",
      "[1768776997] Step 771/1488: training loss=0.48\n",
      "[1768776997] Step 772/1488: training loss=0.37\n",
      "[1768776999] Step 773/1488: training loss=1.71\n",
      "[1768776999] Step 774/1488: training loss=1.32\n",
      "[1768777001] Step 775/1488: training loss=2.07\n",
      "[1768777001] Step 776/1488: training loss=0.31\n",
      "[1768777003] Step 777/1488: training loss=0.40\n",
      "[1768777003] Step 778/1488: training loss=2.11\n",
      "[1768777003] Step 779/1488: training loss=0.70\n",
      "[1768777005] Step 780/1488: training loss=1.96\n",
      "[1768777005] Step 781/1488: training loss=0.32\n",
      "[1768777021] Step 782/1488: training loss=0.56\n",
      "[1768777023] Step 783/1488: training loss=1.35\n",
      "[1768777023] Step 784/1488: training loss=1.96\n",
      "[1768777025] Step 785/1488: training loss=2.15\n",
      "[1768777025] Step 786/1488: training loss=0.30\n",
      "[1768777027] Step 787/1488: training loss=0.47\n",
      "[1768777027] Step 788/1488: training loss=0.95\n",
      "[1768777029] Step 789/1488: training loss=1.63\n",
      "[1768777029] Step 790/1488: training loss=0.55\n",
      "[1768777031] Step 791/1488: training loss=0.25\n",
      "[1768777031] Step 792/1488: training loss=0.21\n",
      "[1768777033] Step 793/1488: training loss=0.41\n",
      "[1768777033] Step 794/1488: training loss=0.24\n",
      "[1768777035] Step 795/1488: training loss=0.48\n",
      "[1768777035] Step 796/1488: training loss=0.69\n",
      "[1768777037] Step 797/1488: training loss=0.37\n",
      "[1768777037] Step 798/1488: training loss=0.33\n",
      "[1768777039] Step 799/1488: training loss=2.02\n",
      "[1768777039] Step 800/1488: training loss=0.29\n",
      "[1768777039] Step 801/1488: training loss=0.36\n",
      "[1768777041] Step 802/1488: training loss=0.55\n",
      "[1768777041] Step 803/1488: training loss=0.29\n",
      "[1768777044] Step 804/1488: training loss=0.39\n",
      "[1768777044] Step 805/1488: training loss=0.27\n",
      "[1768777046] Step 806/1488: training loss=0.41\n",
      "[1768777046] Step 807/1488: training loss=1.62\n",
      "[1768777048] Step 808/1488: training loss=0.64\n",
      "[1768777048] Step 809/1488: training loss=2.33\n",
      "[1768777048] Step 810/1488: training loss=0.51\n",
      "[1768777050] Step 811/1488: training loss=1.93\n",
      "[1768777050] Step 812/1488: training loss=0.42\n",
      "[1768777050] Step 813/1488: training loss=0.43\n",
      "[1768777052] Step 814/1488: training loss=0.23\n",
      "[1768777054] Step 815/1488: training loss=0.44\n",
      "[1768777054] Step 816/1488: training loss=0.41\n",
      "[1768777056] Step 817/1488: training loss=1.87\n",
      "[1768777056] Step 818/1488: training loss=0.33\n",
      "[1768777056] Step 819/1488: training loss=0.34\n",
      "[1768777058] Step 820/1488: training loss=0.32\n",
      "[1768777058] Step 821/1488: training loss=0.29\n",
      "[1768777060] Step 822/1488: training loss=2.07\n",
      "[1768777060] Step 823/1488: training loss=0.35\n",
      "[1768777060] Step 824/1488: training loss=0.36\n",
      "[1768777062] Step 825/1488: training loss=0.62\n",
      "[1768777062] Step 826/1488: training loss=0.16\n",
      "[1768777064] Step 827/1488: training loss=0.31\n",
      "[1768777064] Step 828/1488: training loss=2.04\n",
      "[1768777066] Step 829/1488: training loss=0.57\n",
      "[1768777066] Step 830/1488: training loss=1.43\n",
      "[1768777068] Step 831/1488: training loss=0.32\n",
      "[1768777068] Step 832/1488: training loss=0.23\n",
      "[1768777070] Step 833/1488: training loss=0.38\n",
      "[1768777070] Step 834/1488: training loss=0.47\n",
      "[1768777070] Step 835/1488: training loss=0.23\n",
      "[1768777072] Step 836/1488: training loss=0.31\n",
      "[1768777072] Step 837/1488: training loss=0.42\n",
      "[1768777074] Step 838/1488: training loss=0.35\n",
      "[1768777076] Step 839/1488: training loss=0.59\n",
      "[1768777076] Step 840/1488: training loss=0.22\n",
      "[1768777076] Step 841/1488: training loss=0.25\n",
      "[1768777078] Step 842/1488: training loss=1.00\n",
      "[1768777080] Step 843/1488: training loss=0.60\n",
      "[1768777080] Step 844/1488: training loss=0.79\n",
      "[1768777082] Step 845/1488: training loss=0.41\n",
      "[1768777082] Step 846/1488: training loss=0.61\n",
      "[1768777082] Step 847/1488: training loss=0.27\n",
      "[1768777084] Step 848/1488: training loss=1.29\n",
      "[1768777084] Step 849/1488: training loss=0.27\n",
      "[1768777087] Step 850/1488: training loss=0.46\n",
      "[1768777087] Step 851/1488: training loss=0.25\n",
      "[1768777087] Step 852/1488: training loss=0.47\n",
      "[1768777089] Step 853/1488: training loss=0.28\n",
      "[1768777091] Step 854/1488: training loss=0.17\n",
      "[1768777091] Step 855/1488: training loss=0.40\n",
      "[1768777093] Step 856/1488: training loss=0.39\n",
      "[1768777093] Step 857/1488: training loss=2.05\n",
      "[1768777095] Step 858/1488: training loss=0.40\n",
      "[1768777095] Step 859/1488: training loss=2.40\n",
      "[1768777097] Step 860/1488: training loss=1.79\n",
      "[1768777097] Step 861/1488: training loss=1.59\n",
      "[1768777097] Step 862/1488: training loss=0.34\n",
      "[1768777099] Step 863/1488: training loss=1.79\n",
      "[1768777099] Step 864/1488: training loss=0.41\n",
      "[1768777099] Step 865/1488: training loss=0.37\n",
      "[1768777101] Step 866/1488: training loss=0.57\n",
      "[1768777101] Step 867/1488: training loss=0.31\n",
      "[1768777103] Step 868/1488: training loss=0.48\n",
      "[1768777103] Step 869/1488: training loss=0.35\n",
      "[1768777105] Step 870/1488: training loss=2.10\n",
      "[1768777105] Step 871/1488: training loss=0.52\n",
      "[1768777105] Step 872/1488: training loss=0.51\n",
      "[1768777107] Step 873/1488: training loss=0.27\n",
      "[1768777109] Step 874/1488: training loss=1.40\n",
      "[1768777109] Step 875/1488: training loss=1.80\n",
      "[1768777111] Step 876/1488: training loss=0.33\n",
      "[1768777111] Step 877/1488: training loss=0.28\n",
      "[1768777111] Step 878/1488: training loss=0.38\n",
      "[1768777113] Step 879/1488: training loss=0.50\n",
      "[1768777113] Step 880/1488: training loss=0.20\n",
      "[1768777115] Step 881/1488: training loss=0.56\n",
      "[1768777117] Step 882/1488: training loss=0.29\n",
      "[1768777117] Step 883/1488: training loss=1.97\n",
      "[1768777117] Step 884/1488: training loss=0.83\n",
      "[1768777119] Step 885/1488: training loss=0.20\n",
      "[1768777119] Step 886/1488: training loss=0.46\n",
      "[1768777121] Step 887/1488: training loss=0.35\n",
      "[1768777121] Step 888/1488: training loss=0.39\n",
      "[1768777121] Step 889/1488: training loss=0.30\n",
      "[1768777123] Step 890/1488: training loss=0.60\n",
      "[1768777123] Step 891/1488: training loss=0.32\n",
      "[1768777125] Step 892/1488: training loss=0.55\n",
      "[1768777125] Step 893/1488: training loss=1.13\n",
      "[1768777125] Step 894/1488: training loss=0.36\n",
      "[1768777127] Step 895/1488: training loss=2.36\n",
      "[1768777127] Step 896/1488: training loss=2.06\n",
      "[1768777130] Step 897/1488: training loss=2.23\n",
      "[1768777130] Step 898/1488: training loss=1.20\n",
      "[1768777132] Step 899/1488: training loss=0.26\n",
      "[1768777132] Step 900/1488: training loss=0.38\n",
      "[1768777134] Step 901/1488: training loss=0.31\n",
      "[1768777134] Step 902/1488: training loss=0.39\n",
      "[1768777136] Step 903/1488: training loss=0.35\n",
      "[1768777136] Step 904/1488: training loss=0.45\n",
      "[1768777136] Step 905/1488: training loss=2.30\n",
      "[1768777138] Step 906/1488: training loss=0.49\n",
      "[1768777140] Step 907/1488: training loss=0.42\n",
      "[1768777140] Step 908/1488: training loss=0.42\n",
      "[1768777142] Step 909/1488: training loss=0.48\n",
      "[1768777142] Step 910/1488: training loss=0.48\n",
      "[1768777146] Step 911/1488: training loss=0.48\n",
      "[1768777146] Step 912/1488: training loss=0.49\n",
      "[1768777146] Step 913/1488: training loss=1.68\n",
      "[1768777148] Step 914/1488: training loss=0.16\n",
      "[1768777148] Step 915/1488: training loss=0.52\n",
      "[1768777150] Step 916/1488: training loss=0.20\n",
      "[1768777150] Step 917/1488: training loss=0.39\n",
      "[1768777150] Step 918/1488: training loss=1.67\n",
      "[1768777152] Step 919/1488: training loss=0.54\n",
      "[1768777154] Step 920/1488: training loss=0.48\n",
      "[1768777154] Step 921/1488: training loss=0.36\n",
      "[1768777156] Step 922/1488: training loss=1.43\n",
      "[1768777156] Step 923/1488: training loss=0.50\n",
      "[1768777158] Step 924/1488: training loss=0.60\n",
      "[1768777158] Step 925/1488: training loss=0.36\n",
      "[1768777158] Step 926/1488: training loss=0.39\n",
      "[1768777160] Step 927/1488: training loss=0.44\n",
      "[1768777160] Step 928/1488: training loss=2.69\n",
      "[1768777162] Step 929/1488: training loss=0.52\n",
      "[1768777162] Step 930/1488: training loss=0.47\n",
      "[1768777164] Step 931/1488: training loss=0.44\n",
      "[1768777166] Step 932/1488: training loss=0.27\n",
      "[1768777166] Step 933/1488: training loss=0.36\n",
      "[1768777168] Step 934/1488: training loss=0.24\n",
      "[1768777168] Step 935/1488: training loss=0.25\n",
      "[1768777170] Step 936/1488: training loss=0.47\n",
      "[1768777170] Step 937/1488: training loss=0.34\n",
      "[1768777170] Step 938/1488: training loss=0.42\n",
      "[1768777173] Step 939/1488: training loss=0.52\n",
      "[1768777173] Step 940/1488: training loss=0.35\n",
      "[1768777175] Step 941/1488: training loss=0.45\n",
      "[1768777175] Step 942/1488: training loss=0.53\n",
      "[1768777177] Step 943/1488: training loss=0.38\n",
      "[1768777177] Step 944/1488: training loss=0.24\n",
      "[1768777179] Step 945/1488: training loss=0.51\n",
      "[1768777179] Step 946/1488: training loss=0.48\n",
      "[1768777181] Step 947/1488: training loss=0.53\n",
      "[1768777181] Step 948/1488: training loss=2.76\n",
      "[1768777183] Step 949/1488: training loss=0.37\n",
      "[1768777183] Step 950/1488: training loss=0.26\n",
      "[1768777183] Step 951/1488: training loss=0.30\n",
      "[1768777185] Step 952/1488: training loss=0.52\n",
      "[1768777185] Step 953/1488: training loss=0.52\n",
      "[1768777187] Step 954/1488: training loss=2.13\n",
      "[1768777187] Step 955/1488: training loss=0.34\n",
      "[1768777189] Step 956/1488: training loss=0.06\n",
      "[1768777189] Step 957/1488: training loss=2.03\n",
      "[1768777189] Step 958/1488: training loss=0.33\n",
      "[1768777191] Step 959/1488: training loss=0.36\n",
      "[1768777191] Step 960/1488: training loss=0.38\n",
      "[1768777193] Step 961/1488: training loss=0.32\n",
      "[1768777193] Step 962/1488: training loss=0.41\n",
      "[1768777195] Step 963/1488: training loss=2.09\n",
      "[1768777197] Step 964/1488: training loss=0.12\n",
      "[1768777197] Step 965/1488: training loss=2.08\n",
      "[1768777197] Step 966/1488: training loss=0.36\n",
      "[1768777199] Step 967/1488: training loss=0.32\n",
      "[1768777199] Step 968/1488: training loss=1.98\n",
      "[1768777201] Step 969/1488: training loss=0.15\n",
      "[1768777201] Step 970/1488: training loss=1.32\n",
      "[1768777201] Step 971/1488: training loss=0.33\n",
      "[1768777203] Step 972/1488: training loss=1.62\n",
      "[1768777203] Step 973/1488: training loss=0.30\n",
      "[1768777203] Step 974/1488: training loss=0.29\n",
      "[1768777205] Step 975/1488: training loss=0.23\n",
      "[1768777205] Step 976/1488: training loss=0.44\n",
      "[1768777207] Step 977/1488: training loss=0.34\n",
      "[1768777207] Step 978/1488: training loss=1.30\n",
      "[1768777209] Step 979/1488: training loss=0.37\n",
      "[1768777209] Step 980/1488: training loss=2.31\n",
      "[1768777211] Step 981/1488: training loss=0.75\n",
      "[1768777211] Step 982/1488: training loss=0.44\n",
      "[1768777213] Step 983/1488: training loss=0.36\n",
      "[1768777213] Step 984/1488: training loss=1.58\n",
      "[1768777216] Step 985/1488: training loss=0.36\n",
      "[1768777216] Step 986/1488: training loss=0.49\n",
      "[1768777218] Step 987/1488: training loss=0.39\n",
      "[1768777218] Step 988/1488: training loss=0.35\n",
      "[1768777218] Step 989/1488: training loss=0.36\n",
      "[1768777220] Step 990/1488: training loss=0.32\n",
      "[1768777220] Step 991/1488: training loss=0.22\n",
      "[1768777222] Step 992/1488: training loss=0.41\n",
      "[1768777224] Step 993/1488: training loss=0.28\n",
      "[1768777224] Step 994/1488: training loss=0.14\n",
      "[1768777226] Step 995/1488: training loss=0.31\n",
      "[1768777226] Step 996/1488: training loss=0.20\n",
      "[1768777226] Step 997/1488: training loss=0.18\n",
      "[1768777228] Step 998/1488: training loss=2.03\n",
      "[1768777228] Step 999/1488: training loss=1.10\n",
      "[1768777230] Step 1000/1488: training loss=2.49\n",
      "[1768777230] Step 1001/1488: training loss=0.18\n",
      "[1768777230] Step 1002/1488: training loss=0.05\n",
      "[1768777232] Step 1003/1488: training loss=1.36\n",
      "[1768777232] Step 1004/1488: training loss=1.15\n",
      "[1768777234] Step 1005/1488: training loss=0.32\n",
      "[1768777234] Step 1006/1488: training loss=1.36\n",
      "[1768777234] Step 1007/1488: training loss=1.87\n",
      "[1768777236] Step 1008/1488: training loss=0.24\n",
      "[1768777236] Step 1009/1488: training loss=0.10\n",
      "[1768777238] Step 1010/1488: training loss=0.17\n",
      "[1768777240] Step 1011/1488: training loss=0.33\n",
      "[1768777240] Step 1012/1488: training loss=0.12\n",
      "[1768777240] Step 1013/1488: training loss=2.03\n",
      "[1768777242] Step 1014/1488: training loss=0.20\n",
      "[1768777245] Step 1015/1488: training loss=0.10\n",
      "[1768777245] Step 1016/1488: training loss=0.10\n",
      "[1768777245] Step 1017/1488: training loss=0.08\n",
      "[1768777247] Step 1018/1488: training loss=1.47\n",
      "[1768777247] Step 1019/1488: training loss=0.34\n",
      "[1768777247] Step 1020/1488: training loss=1.82\n",
      "[1768777251] Step 1021/1488: training loss=0.10\n",
      "[1768777251] Step 1022/1488: training loss=0.28\n",
      "[1768777251] Step 1023/1488: training loss=0.16\n",
      "[1768777253] Step 1024/1488: training loss=1.74\n",
      "[1768777253] Step 1025/1488: training loss=0.19\n",
      "[1768777255] Step 1026/1488: training loss=0.21\n",
      "[1768777255] Step 1027/1488: training loss=0.24\n",
      "[1768777257] Step 1028/1488: training loss=0.19\n",
      "[1768777257] Step 1029/1488: training loss=0.20\n",
      "[1768777259] Step 1030/1488: training loss=0.19\n",
      "[1768777259] Step 1031/1488: training loss=0.22\n",
      "[1768777259] Step 1032/1488: training loss=0.11\n",
      "[1768777261] Step 1033/1488: training loss=0.21\n",
      "[1768777261] Step 1034/1488: training loss=0.14\n",
      "[1768777263] Step 1035/1488: training loss=0.09\n",
      "[1768777263] Step 1036/1488: training loss=2.01\n",
      "[1768777265] Step 1037/1488: training loss=0.32\n",
      "[1768777265] Step 1038/1488: training loss=0.06\n",
      "[1768777267] Step 1039/1488: training loss=0.24\n",
      "[1768777267] Step 1040/1488: training loss=0.19\n",
      "[1768777269] Step 1041/1488: training loss=0.14\n",
      "[1768777269] Step 1042/1488: training loss=0.02\n",
      "[1768777271] Step 1043/1488: training loss=0.15\n",
      "[1768777271] Step 1044/1488: training loss=0.12\n",
      "[1768777273] Step 1045/1488: training loss=0.12\n",
      "[1768777275] Step 1046/1488: training loss=0.21\n",
      "[1768777275] Step 1047/1488: training loss=0.19\n",
      "[1768777277] Step 1048/1488: training loss=0.20\n",
      "[1768777277] Step 1049/1488: training loss=1.32\n",
      "[1768777277] Step 1050/1488: training loss=1.80\n",
      "[1768777279] Step 1051/1488: training loss=0.39\n",
      "[1768777279] Step 1052/1488: training loss=0.15\n",
      "[1768777281] Step 1053/1488: training loss=0.13\n",
      "[1768777281] Step 1054/1488: training loss=0.12\n",
      "[1768777281] Step 1055/1488: training loss=0.45\n",
      "[1768777283] Step 1056/1488: training loss=0.26\n",
      "[1768777283] Step 1057/1488: training loss=0.20\n",
      "[1768777285] Step 1058/1488: training loss=0.10\n",
      "[1768777285] Step 1059/1488: training loss=0.37\n",
      "[1768777288] Step 1060/1488: training loss=0.15\n",
      "[1768777288] Step 1061/1488: training loss=0.11\n",
      "[1768777288] Step 1062/1488: training loss=0.20\n",
      "[1768777290] Step 1063/1488: training loss=0.25\n",
      "[1768777290] Step 1064/1488: training loss=1.57\n",
      "[1768777292] Step 1065/1488: training loss=0.07\n",
      "[1768777292] Step 1066/1488: training loss=0.24\n",
      "[1768777292] Step 1067/1488: training loss=0.32\n",
      "[1768777294] Step 1068/1488: training loss=0.17\n",
      "[1768777296] Step 1069/1488: training loss=0.31\n",
      "[1768777296] Step 1070/1488: training loss=0.12\n",
      "[1768777296] Step 1071/1488: training loss=0.06\n",
      "[1768777298] Step 1072/1488: training loss=0.40\n",
      "[1768777298] Step 1073/1488: training loss=0.10\n",
      "[1768777300] Step 1074/1488: training loss=0.13\n",
      "[1768777300] Step 1075/1488: training loss=0.09\n",
      "[1768777300] Step 1076/1488: training loss=0.91\n",
      "[1768777302] Step 1077/1488: training loss=1.58\n",
      "[1768777302] Step 1078/1488: training loss=0.10\n",
      "[1768777304] Step 1079/1488: training loss=2.12\n",
      "[1768777304] Step 1080/1488: training loss=0.97\n",
      "[1768777304] Step 1081/1488: training loss=1.66\n",
      "[1768777306] Step 1082/1488: training loss=1.40\n",
      "[1768777306] Step 1083/1488: training loss=0.25\n",
      "[1768777308] Step 1084/1488: training loss=0.16\n",
      "[1768777308] Step 1085/1488: training loss=0.21\n",
      "[1768777310] Step 1086/1488: training loss=0.21\n",
      "[1768777310] Step 1087/1488: training loss=0.04\n",
      "[1768777310] Step 1088/1488: training loss=0.16\n",
      "[1768777312] Step 1089/1488: training loss=0.92\n",
      "[1768777312] Step 1090/1488: training loss=0.44\n",
      "[1768777314] Step 1091/1488: training loss=0.07\n",
      "[1768777314] Step 1092/1488: training loss=0.17\n",
      "[1768777314] Step 1093/1488: training loss=0.27\n",
      "[1768777316] Step 1094/1488: training loss=0.46\n",
      "[1768777316] Step 1095/1488: training loss=1.16\n",
      "[1768777318] Step 1096/1488: training loss=0.14\n",
      "[1768777318] Step 1097/1488: training loss=0.11\n",
      "[1768777318] Step 1098/1488: training loss=1.09\n",
      "[1768777320] Step 1099/1488: training loss=0.12\n",
      "[1768777320] Step 1100/1488: training loss=1.13\n",
      "[1768777322] Step 1101/1488: training loss=0.12\n",
      "[1768777322] Step 1102/1488: training loss=0.32\n",
      "[1768777324] Step 1103/1488: training loss=0.15\n",
      "[1768777324] Step 1104/1488: training loss=0.15\n",
      "[1768777324] Step 1105/1488: training loss=1.63\n",
      "[1768777326] Step 1106/1488: training loss=0.13\n",
      "[1768777328] Step 1107/1488: training loss=0.14\n",
      "[1768777328] Step 1108/1488: training loss=0.17\n",
      "[1768777331] Step 1109/1488: training loss=0.21\n",
      "[1768777331] Step 1110/1488: training loss=0.04\n",
      "[1768777331] Step 1111/1488: training loss=1.68\n",
      "[1768777333] Step 1112/1488: training loss=0.05\n",
      "[1768777333] Step 1113/1488: training loss=1.64\n",
      "[1768777335] Step 1114/1488: training loss=0.19\n",
      "[1768777335] Step 1115/1488: training loss=0.26\n",
      "[1768777335] Step 1116/1488: training loss=0.27\n",
      "[1768777337] Step 1117/1488: training loss=0.13\n",
      "[1768777337] Step 1118/1488: training loss=0.16\n",
      "[1768777339] Step 1119/1488: training loss=0.25\n",
      "[1768777339] Step 1120/1488: training loss=0.02\n",
      "[1768777339] Step 1121/1488: training loss=0.07\n",
      "[1768777341] Step 1122/1488: training loss=0.07\n",
      "[1768777341] Step 1123/1488: training loss=1.19\n",
      "[1768777341] Step 1124/1488: training loss=0.21\n",
      "[1768777343] Step 1125/1488: training loss=0.13\n",
      "[1768777343] Step 1126/1488: training loss=0.25\n",
      "[1768777345] Step 1127/1488: training loss=0.13\n",
      "[1768777345] Step 1128/1488: training loss=0.15\n",
      "[1768777347] Step 1129/1488: training loss=1.36\n",
      "[1768777347] Step 1130/1488: training loss=0.23\n",
      "[1768777349] Step 1131/1488: training loss=0.15\n",
      "[1768777349] Step 1132/1488: training loss=0.24\n",
      "[1768777351] Step 1133/1488: training loss=0.20\n",
      "[1768777351] Step 1134/1488: training loss=0.30\n",
      "[1768777354] Step 1135/1488: training loss=0.21\n",
      "[1768777354] Step 1136/1488: training loss=0.18\n",
      "[1768777354] Step 1137/1488: training loss=0.19\n",
      "[1768777356] Step 1138/1488: training loss=1.09\n",
      "[1768777356] Step 1139/1488: training loss=0.15\n",
      "[1768777356] Step 1140/1488: training loss=1.03\n",
      "[1768777358] Step 1141/1488: training loss=1.60\n",
      "[1768777358] Step 1142/1488: training loss=1.82\n",
      "[1768777360] Step 1143/1488: training loss=0.08\n",
      "[1768777360] Step 1144/1488: training loss=0.30\n",
      "[1768777360] Step 1145/1488: training loss=2.34\n",
      "[1768777362] Step 1146/1488: training loss=0.11\n",
      "[1768777362] Step 1147/1488: training loss=1.80\n",
      "[1768777364] Step 1148/1488: training loss=0.03\n",
      "[1768777364] Step 1149/1488: training loss=0.08\n",
      "[1768777366] Step 1150/1488: training loss=0.10\n",
      "[1768777366] Step 1151/1488: training loss=0.16\n",
      "[1768777368] Step 1152/1488: training loss=0.18\n",
      "[1768777368] Step 1153/1488: training loss=0.26\n",
      "[1768777370] Step 1154/1488: training loss=0.33\n",
      "[1768777370] Step 1155/1488: training loss=0.18\n",
      "[1768777372] Step 1156/1488: training loss=0.16\n",
      "[1768777372] Step 1157/1488: training loss=1.83\n",
      "[1768777374] Step 1158/1488: training loss=2.47\n",
      "[1768777374] Step 1159/1488: training loss=0.24\n",
      "[1768777376] Step 1160/1488: training loss=0.29\n",
      "[1768777376] Step 1161/1488: training loss=0.09\n",
      "[1768777378] Step 1162/1488: training loss=0.16\n",
      "[1768777378] Step 1163/1488: training loss=0.20\n",
      "[1768777380] Step 1164/1488: training loss=0.18\n",
      "[1768777380] Step 1165/1488: training loss=0.28\n",
      "[1768777382] Step 1166/1488: training loss=0.12\n",
      "[1768777384] Step 1167/1488: training loss=0.22\n",
      "[1768777386] Step 1168/1488: training loss=0.32\n",
      "[1768777386] Step 1169/1488: training loss=0.19\n",
      "[1768777386] Step 1170/1488: training loss=1.35\n",
      "[1768777388] Step 1171/1488: training loss=0.06\n",
      "[1768777388] Step 1172/1488: training loss=0.22\n",
      "[1768777390] Step 1173/1488: training loss=0.08\n",
      "[1768777390] Step 1174/1488: training loss=2.09\n",
      "[1768777390] Step 1175/1488: training loss=1.33\n",
      "[1768777392] Step 1176/1488: training loss=0.20\n",
      "[1768777392] Step 1177/1488: training loss=0.20\n",
      "[1768777395] Step 1178/1488: training loss=0.18\n",
      "[1768777395] Step 1179/1488: training loss=0.27\n",
      "[1768777395] Step 1180/1488: training loss=1.65\n",
      "[1768777397] Step 1181/1488: training loss=0.05\n",
      "[1768777397] Step 1182/1488: training loss=0.12\n",
      "[1768777399] Step 1183/1488: training loss=0.31\n",
      "[1768777399] Step 1184/1488: training loss=0.21\n",
      "[1768777401] Step 1185/1488: training loss=0.31\n",
      "[1768777401] Step 1186/1488: training loss=0.12\n",
      "[1768777403] Step 1187/1488: training loss=0.20\n",
      "[1768777403] Step 1188/1488: training loss=1.25\n",
      "[1768777403] Step 1189/1488: training loss=0.16\n",
      "[1768777405] Step 1190/1488: training loss=0.20\n",
      "[1768777407] Step 1191/1488: training loss=0.17\n",
      "[1768777407] Step 1192/1488: training loss=0.55\n",
      "[1768777409] Step 1193/1488: training loss=0.32\n",
      "[1768777411] Step 1194/1488: training loss=0.12\n",
      "[1768777411] Step 1195/1488: training loss=0.19\n",
      "[1768777413] Step 1196/1488: training loss=0.03\n",
      "[1768777413] Step 1197/1488: training loss=0.14\n",
      "[1768777415] Step 1198/1488: training loss=0.31\n",
      "[1768777415] Step 1199/1488: training loss=0.21\n",
      "[1768777417] Step 1200/1488: training loss=0.15\n",
      "[1768777417] Step 1201/1488: training loss=0.07\n",
      "[1768777419] Step 1202/1488: training loss=0.09\n",
      "[1768777419] Step 1203/1488: training loss=0.15\n",
      "[1768777419] Step 1204/1488: training loss=0.09\n",
      "[1768777421] Step 1205/1488: training loss=0.05\n",
      "[1768777421] Step 1206/1488: training loss=1.82\n",
      "[1768777423] Step 1207/1488: training loss=0.18\n",
      "[1768777425] Step 1208/1488: training loss=0.05\n",
      "[1768777425] Step 1209/1488: training loss=0.22\n",
      "[1768777425] Step 1210/1488: training loss=0.22\n",
      "[1768777427] Step 1211/1488: training loss=0.16\n",
      "[1768777427] Step 1212/1488: training loss=0.26\n",
      "[1768777429] Step 1213/1488: training loss=0.30\n",
      "[1768777429] Step 1214/1488: training loss=1.92\n",
      "[1768777429] Step 1215/1488: training loss=2.40\n",
      "[1768777431] Step 1216/1488: training loss=0.35\n",
      "[1768777431] Step 1217/1488: training loss=0.29\n",
      "[1768777433] Step 1218/1488: training loss=2.34\n",
      "[1768777435] Step 1219/1488: training loss=0.12\n",
      "[1768777435] Step 1220/1488: training loss=1.49\n",
      "[1768777435] Step 1221/1488: training loss=1.56\n",
      "[1768777437] Step 1222/1488: training loss=0.16\n",
      "[1768777437] Step 1223/1488: training loss=0.96\n",
      "[1768777440] Step 1224/1488: training loss=0.31\n",
      "[1768777440] Step 1225/1488: training loss=0.19\n",
      "[1768777442] Step 1226/1488: training loss=0.22\n",
      "[1768777442] Step 1227/1488: training loss=0.04\n",
      "[1768777444] Step 1228/1488: training loss=0.23\n",
      "[1768777444] Step 1229/1488: training loss=0.14\n",
      "[1768777446] Step 1230/1488: training loss=1.66\n",
      "[1768777446] Step 1231/1488: training loss=0.21\n",
      "[1768777448] Step 1232/1488: training loss=0.17\n",
      "[1768777448] Step 1233/1488: training loss=0.23\n",
      "[1768777450] Step 1234/1488: training loss=0.16\n",
      "[1768777450] Step 1235/1488: training loss=0.13\n",
      "[1768777452] Step 1236/1488: training loss=0.43\n",
      "[1768777454] Step 1237/1488: training loss=0.10\n",
      "[1768777456] Step 1238/1488: training loss=0.27\n",
      "[1768777456] Step 1239/1488: training loss=0.32\n",
      "[1768777456] Step 1240/1488: training loss=0.15\n",
      "[1768777458] Step 1241/1488: training loss=0.58\n",
      "[1768777460] Step 1242/1488: training loss=0.24\n",
      "[1768777460] Step 1243/1488: training loss=0.08\n",
      "[1768777460] Step 1244/1488: training loss=0.06\n",
      "[1768777462] Step 1245/1488: training loss=0.47\n",
      "[1768777464] Step 1246/1488: training loss=0.23\n",
      "[1768777464] Step 1247/1488: training loss=1.73\n",
      "[1768777464] Step 1248/1488: training loss=0.13\n",
      "[1768777466] Step 1249/1488: training loss=0.21\n",
      "[1768777466] Step 1250/1488: training loss=0.29\n",
      "[1768777468] Step 1251/1488: training loss=0.16\n",
      "[1768777468] Step 1252/1488: training loss=0.25\n",
      "[1768777470] Step 1253/1488: training loss=0.16\n",
      "[1768777470] Step 1254/1488: training loss=0.14\n",
      "[1768777470] Step 1255/1488: training loss=0.11\n",
      "[1768777472] Step 1256/1488: training loss=0.19\n",
      "[1768777472] Step 1257/1488: training loss=0.11\n",
      "[1768777474] Step 1258/1488: training loss=0.05\n",
      "[1768777474] Step 1259/1488: training loss=1.68\n",
      "[1768777476] Step 1260/1488: training loss=0.19\n",
      "[1768777476] Step 1261/1488: training loss=1.46\n",
      "[1768777478] Step 1262/1488: training loss=0.27\n",
      "[1768777478] Step 1263/1488: training loss=0.13\n",
      "[1768777481] Step 1264/1488: training loss=0.30\n",
      "[1768777481] Step 1265/1488: training loss=1.28\n",
      "[1768777483] Step 1266/1488: training loss=0.26\n",
      "[1768777483] Step 1267/1488: training loss=0.10\n",
      "[1768777485] Step 1268/1488: training loss=0.23\n",
      "[1768777485] Step 1269/1488: training loss=0.10\n",
      "[1768777487] Step 1270/1488: training loss=0.16\n",
      "[1768777489] Step 1271/1488: training loss=0.34\n",
      "[1768777489] Step 1272/1488: training loss=2.50\n",
      "[1768777489] Step 1273/1488: training loss=1.41\n",
      "[1768777491] Step 1274/1488: training loss=0.08\n",
      "[1768777493] Step 1275/1488: training loss=0.11\n",
      "[1768777493] Step 1276/1488: training loss=0.28\n",
      "[1768777495] Step 1277/1488: training loss=0.11\n",
      "[1768777495] Step 1278/1488: training loss=0.11\n",
      "[1768777497] Step 1279/1488: training loss=0.14\n",
      "[1768777497] Step 1280/1488: training loss=0.22\n",
      "[1768777499] Step 1281/1488: training loss=0.10\n",
      "[1768777499] Step 1282/1488: training loss=0.17\n",
      "[1768777499] Step 1283/1488: training loss=0.29\n",
      "[1768777501] Step 1284/1488: training loss=0.19\n",
      "[1768777501] Step 1285/1488: training loss=1.13\n",
      "[1768777503] Step 1286/1488: training loss=0.19\n",
      "[1768777503] Step 1287/1488: training loss=0.15\n",
      "[1768777503] Step 1288/1488: training loss=0.09\n",
      "[1768777505] Step 1289/1488: training loss=0.20\n",
      "[1768777505] Step 1290/1488: training loss=1.39\n",
      "[1768777507] Step 1291/1488: training loss=0.18\n",
      "[1768777507] Step 1292/1488: training loss=0.10\n",
      "[1768777509] Step 1293/1488: training loss=0.45\n",
      "[1768777509] Step 1294/1488: training loss=0.20\n",
      "[1768777511] Step 1295/1488: training loss=0.26\n",
      "[1768777511] Step 1296/1488: training loss=0.11\n",
      "[1768777513] Step 1297/1488: training loss=0.09\n",
      "[1768777513] Step 1298/1488: training loss=1.83\n",
      "[1768777515] Step 1299/1488: training loss=0.15\n",
      "[1768777516] Step 1300/1488: training loss=0.22\n",
      "[1768777516] Step 1301/1488: training loss=0.14\n",
      "[1768777518] Step 1302/1488: training loss=0.16\n",
      "[1768777518] Step 1303/1488: training loss=0.23\n",
      "[1768777518] Step 1304/1488: training loss=0.12\n",
      "[1768777520] Step 1305/1488: training loss=0.10\n",
      "[1768777522] Step 1306/1488: training loss=0.05\n",
      "[1768777522] Step 1307/1488: training loss=1.75\n",
      "[1768777522] Step 1308/1488: training loss=0.13\n",
      "[1768777524] Step 1309/1488: training loss=0.22\n",
      "[1768777524] Step 1310/1488: training loss=0.12\n",
      "[1768777526] Step 1311/1488: training loss=0.09\n",
      "[1768777526] Step 1312/1488: training loss=0.07\n",
      "[1768777528] Step 1313/1488: training loss=0.06\n",
      "[1768777528] Step 1314/1488: training loss=0.19\n",
      "[1768777528] Step 1315/1488: training loss=0.18\n",
      "[1768777530] Step 1316/1488: training loss=0.12\n",
      "[1768777530] Step 1317/1488: training loss=0.17\n",
      "[1768777532] Step 1318/1488: training loss=0.08\n",
      "[1768777532] Step 1319/1488: training loss=0.09\n",
      "[1768777532] Step 1320/1488: training loss=0.29\n",
      "[1768777534] Step 1321/1488: training loss=0.13\n",
      "[1768777536] Step 1322/1488: training loss=0.19\n",
      "[1768777536] Step 1323/1488: training loss=0.26\n",
      "[1768777536] Step 1324/1488: training loss=0.18\n",
      "[1768777538] Step 1325/1488: training loss=0.02\n",
      "[1768777538] Step 1326/1488: training loss=0.13\n",
      "[1768777540] Step 1327/1488: training loss=0.20\n",
      "[1768777540] Step 1328/1488: training loss=0.08\n",
      "[1768777542] Step 1329/1488: training loss=0.74\n",
      "[1768777542] Step 1330/1488: training loss=0.04\n",
      "[1768777544] Step 1331/1488: training loss=0.08\n",
      "[1768777544] Step 1332/1488: training loss=0.16\n",
      "[1768777546] Step 1333/1488: training loss=0.26\n",
      "[1768777546] Step 1334/1488: training loss=2.11\n",
      "[1768777546] Step 1335/1488: training loss=1.45\n",
      "[1768777548] Step 1336/1488: training loss=0.15\n",
      "[1768777550] Step 1337/1488: training loss=0.21\n",
      "[1768777550] Step 1338/1488: training loss=0.28\n",
      "[1768777550] Step 1339/1488: training loss=0.17\n",
      "[1768777552] Step 1340/1488: training loss=0.14\n",
      "[1768777554] Step 1341/1488: training loss=1.53\n",
      "[1768777554] Step 1342/1488: training loss=0.04\n",
      "[1768777554] Step 1343/1488: training loss=0.98\n",
      "[1768777557] Step 1344/1488: training loss=1.42\n",
      "[1768777557] Step 1345/1488: training loss=0.33\n",
      "[1768777559] Step 1346/1488: training loss=1.32\n",
      "[1768777559] Step 1347/1488: training loss=1.31\n",
      "[1768777559] Step 1348/1488: training loss=0.09\n",
      "[1768777561] Step 1349/1488: training loss=0.27\n",
      "[1768777563] Step 1350/1488: training loss=0.10\n",
      "[1768777563] Step 1351/1488: training loss=0.26\n",
      "[1768777563] Step 1352/1488: training loss=0.16\n",
      "[1768777565] Step 1353/1488: training loss=1.18\n",
      "[1768777565] Step 1354/1488: training loss=0.19\n",
      "[1768777567] Step 1355/1488: training loss=0.59\n",
      "[1768777567] Step 1356/1488: training loss=1.73\n",
      "[1768777569] Step 1357/1488: training loss=0.16\n",
      "[1768777569] Step 1358/1488: training loss=1.48\n",
      "[1768777571] Step 1359/1488: training loss=0.16\n",
      "[1768777571] Step 1360/1488: training loss=0.30\n",
      "[1768777573] Step 1361/1488: training loss=0.43\n",
      "[1768777573] Step 1362/1488: training loss=0.10\n",
      "[1768777575] Step 1363/1488: training loss=0.94\n",
      "[1768777575] Step 1364/1488: training loss=0.20\n",
      "[1768777575] Step 1365/1488: training loss=1.49\n",
      "[1768777577] Step 1366/1488: training loss=0.20\n",
      "[1768777577] Step 1367/1488: training loss=1.96\n",
      "[1768777579] Step 1368/1488: training loss=0.07\n",
      "[1768777579] Step 1369/1488: training loss=0.26\n",
      "[1768777581] Step 1370/1488: training loss=0.13\n",
      "[1768777581] Step 1371/1488: training loss=0.03\n",
      "[1768777581] Step 1372/1488: training loss=0.14\n",
      "[1768777583] Step 1373/1488: training loss=0.19\n",
      "[1768777583] Step 1374/1488: training loss=1.69\n",
      "[1768777585] Step 1375/1488: training loss=0.07\n",
      "[1768777585] Step 1376/1488: training loss=0.05\n",
      "[1768777585] Step 1377/1488: training loss=0.14\n",
      "[1768777587] Step 1378/1488: training loss=1.94\n",
      "[1768777587] Step 1379/1488: training loss=0.20\n",
      "[1768777587] Step 1380/1488: training loss=0.15\n",
      "[1768777589] Step 1381/1488: training loss=0.25\n",
      "[1768777589] Step 1382/1488: training loss=0.14\n",
      "[1768777591] Step 1383/1488: training loss=0.09\n",
      "[1768777591] Step 1384/1488: training loss=0.22\n",
      "[1768777593] Step 1385/1488: training loss=0.15\n",
      "[1768777593] Step 1386/1488: training loss=0.16\n",
      "[1768777595] Step 1387/1488: training loss=0.45\n",
      "[1768777596] Step 1388/1488: training loss=0.11\n",
      "[1768777598] Step 1389/1488: training loss=0.10\n",
      "[1768777598] Step 1390/1488: training loss=0.88\n",
      "[1768777600] Step 1391/1488: training loss=1.63\n",
      "[1768777600] Step 1392/1488: training loss=0.97\n",
      "[1768777602] Step 1393/1488: training loss=0.15\n",
      "[1768777602] Step 1394/1488: training loss=0.17\n",
      "[1768777604] Step 1395/1488: training loss=0.19\n",
      "[1768777604] Step 1396/1488: training loss=0.91\n",
      "[1768777606] Step 1397/1488: training loss=1.81\n",
      "[1768777606] Step 1398/1488: training loss=0.28\n",
      "[1768777608] Step 1399/1488: training loss=0.12\n",
      "[1768777608] Step 1400/1488: training loss=0.25\n",
      "[1768777610] Step 1401/1488: training loss=0.14\n",
      "[1768777610] Step 1402/1488: training loss=0.14\n",
      "[1768777610] Step 1403/1488: training loss=0.06\n",
      "[1768777612] Step 1404/1488: training loss=0.09\n",
      "[1768777612] Step 1405/1488: training loss=0.17\n",
      "[1768777614] Step 1406/1488: training loss=0.17\n",
      "[1768777614] Step 1407/1488: training loss=0.25\n",
      "[1768777616] Step 1408/1488: training loss=1.42\n",
      "[1768777616] Step 1409/1488: training loss=0.24\n",
      "[1768777618] Step 1410/1488: training loss=0.16\n",
      "[1768777618] Step 1411/1488: training loss=0.11\n",
      "[1768777618] Step 1412/1488: training loss=0.18\n",
      "[1768777620] Step 1413/1488: training loss=0.12\n",
      "[1768777620] Step 1414/1488: training loss=0.15\n",
      "[1768777622] Step 1415/1488: training loss=0.16\n",
      "[1768777622] Step 1416/1488: training loss=1.80\n",
      "[1768777622] Step 1417/1488: training loss=1.31\n",
      "[1768777624] Step 1418/1488: training loss=0.49\n",
      "[1768777626] Step 1419/1488: training loss=0.22\n",
      "[1768777628] Step 1420/1488: training loss=0.20\n",
      "[1768777628] Step 1421/1488: training loss=0.62\n",
      "[1768777630] Step 1422/1488: training loss=0.17\n",
      "[1768777630] Step 1423/1488: training loss=1.61\n",
      "[1768777632] Step 1424/1488: training loss=0.08\n",
      "[1768777632] Step 1425/1488: training loss=0.11\n",
      "[1768777632] Step 1426/1488: training loss=0.08\n",
      "[1768777634] Step 1427/1488: training loss=1.69\n",
      "[1768777636] Step 1428/1488: training loss=0.24\n",
      "[1768777636] Step 1429/1488: training loss=0.17\n",
      "[1768777638] Step 1430/1488: training loss=0.07\n",
      "[1768777639] Step 1431/1488: training loss=0.24\n",
      "[1768777639] Step 1432/1488: training loss=1.73\n",
      "[1768777641] Step 1433/1488: training loss=1.79\n",
      "[1768777641] Step 1434/1488: training loss=0.96\n",
      "[1768777643] Step 1435/1488: training loss=0.17\n",
      "[1768777643] Step 1436/1488: training loss=0.18\n",
      "[1768777645] Step 1437/1488: training loss=1.53\n",
      "[1768777645] Step 1438/1488: training loss=1.82\n",
      "[1768777647] Step 1439/1488: training loss=0.18\n",
      "[1768777647] Step 1440/1488: training loss=0.12\n",
      "[1768777647] Step 1441/1488: training loss=0.22\n",
      "[1768777649] Step 1442/1488: training loss=0.11\n",
      "[1768777649] Step 1443/1488: training loss=1.79\n",
      "[1768777651] Step 1444/1488: training loss=0.34\n",
      "[1768777651] Step 1445/1488: training loss=1.61\n",
      "[1768777651] Step 1446/1488: training loss=0.09\n",
      "[1768777653] Step 1447/1488: training loss=0.07\n",
      "[1768777653] Step 1448/1488: training loss=0.22\n",
      "[1768777655] Step 1449/1488: training loss=0.20\n",
      "[1768777655] Step 1450/1488: training loss=0.12\n",
      "[1768777655] Step 1451/1488: training loss=0.06\n",
      "[1768777657] Step 1452/1488: training loss=0.37\n",
      "[1768777657] Step 1453/1488: training loss=0.11\n",
      "[1768777657] Step 1454/1488: training loss=1.01\n",
      "[1768777660] Step 1455/1488: training loss=0.06\n",
      "[1768777660] Step 1456/1488: training loss=0.10\n",
      "[1768777662] Step 1457/1488: training loss=1.72\n",
      "[1768777662] Step 1458/1488: training loss=0.15\n",
      "[1768777664] Step 1459/1488: training loss=0.07\n",
      "[1768777664] Step 1460/1488: training loss=0.16\n",
      "[1768777666] Step 1461/1488: training loss=0.07\n",
      "[1768777666] Step 1462/1488: training loss=1.35\n",
      "[1768777666] Step 1463/1488: training loss=0.30\n",
      "[1768777668] Step 1464/1488: training loss=0.24\n",
      "[1768777668] Step 1465/1488: training loss=0.21\n",
      "[1768777670] Step 1466/1488: training loss=0.33\n",
      "[1768777670] Step 1467/1488: training loss=0.23\n",
      "[1768777672] Step 1468/1488: training loss=2.29\n",
      "[1768777672] Step 1469/1488: training loss=1.10\n",
      "[1768777674] Step 1470/1488: training loss=0.41\n",
      "[1768777674] Step 1471/1488: training loss=0.13\n",
      "[1768777676] Step 1472/1488: training loss=2.04\n",
      "[1768777676] Step 1473/1488: training loss=2.08\n",
      "[1768777678] Step 1474/1488: training loss=0.49\n",
      "[1768777678] Step 1475/1488: training loss=1.13\n",
      "[1768777680] Step 1476/1488: training loss=0.13\n",
      "[1768777682] Step 1477/1488: training loss=0.15\n",
      "[1768777682] Step 1478/1488: training loss=0.07\n",
      "[1768777684] Step 1479/1488: training loss=0.10\n",
      "[1768777684] Step 1480/1488: training loss=0.10\n",
      "[1768777686] Step 1481/1488: training loss=0.05\n",
      "[1768777686] Step 1482/1488: training loss=1.64\n",
      "[1768777688] Step 1483/1488: training loss=0.10\n",
      "[1768777688] Step 1484/1488: training loss=0.07\n",
      "[1768777688] Step 1485/1488: training loss=2.01\n",
      "[1768777690] Step 1486/1488: training loss=0.26\n",
      "[1768777690] Step 1487/1488: training loss=0.19\n",
      "[1768777692] Step 1488/1488: training loss=0.11\n",
      "[1768777706] Checkpoint created at step 496\n",
      "[1768777706] Checkpoint created at step 992\n",
      "[1768777706] New fine-tuned model created\n",
      "[1768777706] Evaluating model against our usage policies\n",
      "[1768778431] Moderation checks for snapshot ft:gpt-4.1-mini-2025-04-14:tavily::CzWAcE6p passed.\n",
      "[1768778431] Usage policy evaluations completed, model is now enabled for sampling\n",
      "[1768778433] The job has successfully completed\n",
      "\n",
      "============================================================\n",
      "✓ Fine-tuning completed!\n",
      "Fine-tuned model: ft:gpt-4.1-mini-2025-04-14:tavily::CzWAcE6p\n",
      "Trained tokens: 14,982,684\n",
      "Actual cost: $74.91\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ gpt-4.1-mini-2025-04-14 → ft:gpt-4.1-mini-2025-04-14:tavily::CzWAcE6p\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Starting fine-tuning for gpt-4.1-nano-2025-04-14\n",
      "======================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: gpt-4.1-nano-2025-04-14\n",
      "Training samples: 496\n",
      "Total tokens: 4,987,780\n",
      "Epochs: 3\n",
      "Estimated training cost: $22.45\n",
      "Training file: data\\train_gpt-4.1-nano-2025-04-14.jsonl\n",
      "============================================================\n",
      "\n",
      "Fine-tuning job submitted: ftjob-dLk8q6fj3B6GRRBGCRuQj85a\n",
      "Monitor at: https://platform.openai.com/finetune/ftjob-dLk8q6fj3B6GRRBGCRuQj85a\n",
      "\n",
      "[1768778485] Created fine-tuning job: ftjob-dLk8q6fj3B6GRRBGCRuQj85a\n",
      "[1768778485] Validating training file: file-SQ9pgYpy8rYgG2vaYucwxy\n",
      "[1768778706] Files validated, moving job to queued state\n",
      "[1768778708] Fine-tuning job started\n",
      "[1768778821] Step 1/1488: training loss=1.90\n",
      "[1768778823] Step 2/1488: training loss=2.19\n",
      "[1768778826] Step 3/1488: training loss=1.22\n",
      "[1768778828] Step 4/1488: training loss=1.75\n",
      "[1768778828] Step 5/1488: training loss=1.91\n",
      "[1768778830] Step 6/1488: training loss=1.80\n",
      "[1768778832] Step 7/1488: training loss=1.48\n",
      "[1768778834] Step 8/1488: training loss=1.96\n",
      "[1768778838] Step 9/1488: training loss=1.99\n",
      "[1768778840] Step 10/1488: training loss=1.22\n",
      "[1768778842] Step 11/1488: training loss=1.70\n",
      "[1768778844] Step 12/1488: training loss=1.75\n",
      "[1768778848] Step 13/1488: training loss=1.68\n",
      "[1768778848] Step 14/1488: training loss=1.86\n",
      "[1768778850] Step 15/1488: training loss=1.58\n",
      "[1768778852] Step 16/1488: training loss=1.98\n",
      "[1768778854] Step 17/1488: training loss=1.60\n",
      "[1768778856] Step 18/1488: training loss=1.97\n",
      "[1768778856] Step 19/1488: training loss=1.20\n",
      "[1768778858] Step 20/1488: training loss=2.08\n",
      "[1768778860] Step 21/1488: training loss=1.81\n",
      "[1768778860] Step 22/1488: training loss=1.80\n",
      "[1768778873] Step 23/1488: training loss=0.94\n",
      "[1768778875] Step 24/1488: training loss=1.60\n",
      "[1768778877] Step 25/1488: training loss=1.74\n",
      "[1768778877] Step 26/1488: training loss=1.55\n",
      "[1768778879] Step 27/1488: training loss=1.48\n",
      "[1768778881] Step 28/1488: training loss=0.92\n",
      "[1768778883] Step 29/1488: training loss=1.37\n",
      "[1768778887] Step 30/1488: training loss=1.98\n",
      "[1768778889] Step 31/1488: training loss=0.81\n",
      "[1768778889] Step 32/1488: training loss=0.87\n",
      "[1768778893] Step 33/1488: training loss=1.56\n",
      "[1768778897] Step 34/1488: training loss=1.30\n",
      "[1768778901] Step 35/1488: training loss=1.31\n",
      "[1768778901] Step 36/1488: training loss=0.73\n",
      "[1768778906] Step 37/1488: training loss=1.56\n",
      "[1768778908] Step 38/1488: training loss=1.14\n",
      "[1768778908] Step 39/1488: training loss=0.72\n",
      "[1768778910] Step 40/1488: training loss=1.13\n",
      "[1768778914] Step 41/1488: training loss=1.09\n",
      "[1768778916] Step 42/1488: training loss=0.92\n",
      "[1768778916] Step 43/1488: training loss=0.31\n",
      "[1768778920] Step 44/1488: training loss=1.66\n",
      "[1768778922] Step 45/1488: training loss=1.38\n",
      "[1768778922] Step 46/1488: training loss=1.21\n",
      "[1768778922] Step 47/1488: training loss=0.89\n",
      "[1768778928] Step 48/1488: training loss=1.13\n",
      "[1768778930] Step 49/1488: training loss=1.15\n",
      "[1768778932] Step 50/1488: training loss=1.09\n",
      "[1768778932] Step 51/1488: training loss=1.67\n",
      "[1768778936] Step 52/1488: training loss=1.11\n",
      "[1768778938] Step 53/1488: training loss=0.83\n",
      "[1768778938] Step 54/1488: training loss=1.22\n",
      "[1768778942] Step 55/1488: training loss=1.45\n",
      "[1768778947] Step 56/1488: training loss=1.59\n",
      "[1768778949] Step 57/1488: training loss=0.75\n",
      "[1768778961] Step 58/1488: training loss=1.20\n",
      "[1768778965] Step 59/1488: training loss=1.11\n",
      "[1768778967] Step 60/1488: training loss=1.34\n",
      "[1768778969] Step 61/1488: training loss=1.09\n",
      "[1768778971] Step 62/1488: training loss=1.06\n",
      "[1768778973] Step 63/1488: training loss=1.32\n",
      "[1768778973] Step 64/1488: training loss=1.32\n",
      "[1768778975] Step 65/1488: training loss=0.98\n",
      "[1768778975] Step 66/1488: training loss=0.55\n",
      "[1768778977] Step 67/1488: training loss=0.69\n",
      "[1768778979] Step 68/1488: training loss=1.26\n",
      "[1768778981] Step 69/1488: training loss=1.11\n",
      "[1768778981] Step 70/1488: training loss=1.27\n",
      "[1768778983] Step 71/1488: training loss=1.49\n",
      "[1768778985] Step 72/1488: training loss=1.06\n",
      "[1768778985] Step 73/1488: training loss=0.92\n",
      "[1768778990] Step 74/1488: training loss=1.41\n",
      "[1768778994] Step 75/1488: training loss=1.02\n",
      "[1768778996] Step 76/1488: training loss=1.43\n",
      "[1768778998] Step 77/1488: training loss=1.15\n",
      "[1768779000] Step 78/1488: training loss=0.91\n",
      "[1768779004] Step 79/1488: training loss=1.63\n",
      "[1768779006] Step 80/1488: training loss=1.10\n",
      "[1768779006] Step 81/1488: training loss=1.26\n",
      "[1768779010] Step 82/1488: training loss=1.34\n",
      "[1768779014] Step 83/1488: training loss=0.80\n",
      "[1768779016] Step 84/1488: training loss=0.68\n",
      "[1768779016] Step 85/1488: training loss=1.23\n",
      "[1768779018] Step 86/1488: training loss=1.04\n",
      "[1768779024] Step 87/1488: training loss=1.06\n",
      "[1768779028] Step 88/1488: training loss=1.65\n",
      "[1768779042] Step 89/1488: training loss=0.97\n",
      "[1768779042] Step 90/1488: training loss=1.34\n",
      "[1768779045] Step 91/1488: training loss=1.21\n",
      "[1768779045] Step 92/1488: training loss=1.07\n",
      "[1768779047] Step 93/1488: training loss=1.10\n",
      "[1768779051] Step 94/1488: training loss=0.94\n",
      "[1768779053] Step 95/1488: training loss=1.29\n",
      "[1768779055] Step 96/1488: training loss=1.22\n",
      "[1768779055] Step 97/1488: training loss=0.66\n",
      "[1768779059] Step 98/1488: training loss=1.39\n",
      "[1768779061] Step 99/1488: training loss=1.07\n",
      "[1768779061] Step 100/1488: training loss=0.73\n",
      "[1768779076] Step 101/1488: training loss=1.06\n",
      "[1768779076] Step 102/1488: training loss=1.06\n",
      "[1768779078] Step 103/1488: training loss=1.70\n",
      "[1768779080] Step 104/1488: training loss=1.08\n",
      "[1768779080] Step 105/1488: training loss=0.67\n",
      "[1768779082] Step 106/1488: training loss=0.85\n",
      "[1768779082] Step 107/1488: training loss=0.87\n",
      "[1768779082] Step 108/1488: training loss=1.20\n",
      "[1768779086] Step 109/1488: training loss=1.26\n",
      "[1768779086] Step 110/1488: training loss=1.17\n",
      "[1768779090] Step 111/1488: training loss=1.32\n",
      "[1768779092] Step 112/1488: training loss=0.88\n",
      "[1768779092] Step 113/1488: training loss=1.31\n",
      "[1768779092] Step 114/1488: training loss=0.55\n",
      "[1768779094] Step 115/1488: training loss=0.63\n",
      "[1768779098] Step 116/1488: training loss=0.92\n",
      "[1768779100] Step 117/1488: training loss=1.51\n",
      "[1768779103] Step 118/1488: training loss=1.19\n",
      "[1768779103] Step 119/1488: training loss=0.92\n",
      "[1768779105] Step 120/1488: training loss=1.07\n",
      "[1768779109] Step 121/1488: training loss=0.94\n",
      "[1768779111] Step 122/1488: training loss=1.06\n",
      "[1768779114] Step 123/1488: training loss=1.08\n",
      "[1768779116] Step 124/1488: training loss=1.25\n",
      "[1768779116] Step 125/1488: training loss=1.00\n",
      "[1768779118] Step 126/1488: training loss=1.14\n",
      "[1768779120] Step 127/1488: training loss=1.08\n",
      "[1768779124] Step 128/1488: training loss=0.87\n",
      "[1768779126] Step 129/1488: training loss=1.23\n",
      "[1768779128] Step 130/1488: training loss=1.02\n",
      "[1768779132] Step 131/1488: training loss=1.04\n",
      "[1768779134] Step 132/1488: training loss=0.91\n",
      "[1768779134] Step 133/1488: training loss=0.80\n",
      "[1768779140] Step 134/1488: training loss=1.06\n",
      "[1768779144] Step 135/1488: training loss=1.21\n",
      "[1768779144] Step 136/1488: training loss=0.89\n",
      "[1768779155] Step 137/1488: training loss=0.92\n",
      "[1768779157] Step 138/1488: training loss=0.90\n",
      "[1768779157] Step 139/1488: training loss=1.13\n",
      "[1768779159] Step 140/1488: training loss=1.12\n",
      "[1768779159] Step 141/1488: training loss=0.97\n",
      "[1768779161] Step 142/1488: training loss=0.70\n",
      "[1768779163] Step 143/1488: training loss=1.26\n",
      "[1768779165] Step 144/1488: training loss=0.74\n",
      "[1768779165] Step 145/1488: training loss=0.66\n",
      "[1768779167] Step 146/1488: training loss=0.83\n",
      "[1768779167] Step 147/1488: training loss=0.85\n",
      "[1768779171] Step 148/1488: training loss=1.43\n",
      "[1768779175] Step 149/1488: training loss=1.35\n",
      "[1768779177] Step 150/1488: training loss=0.79\n",
      "[1768779181] Step 151/1488: training loss=1.34\n",
      "[1768779191] Step 152/1488: training loss=1.82\n",
      "[1768779193] Step 153/1488: training loss=0.95\n",
      "[1768779205] Step 154/1488: training loss=1.58\n",
      "[1768779210] Step 155/1488: training loss=1.13\n",
      "[1768779214] Step 156/1488: training loss=0.99\n",
      "[1768779214] Step 157/1488: training loss=1.33\n",
      "[1768779216] Step 158/1488: training loss=1.02\n",
      "[1768779216] Step 159/1488: training loss=1.07\n",
      "[1768779220] Step 160/1488: training loss=1.29\n",
      "[1768779220] Step 161/1488: training loss=0.78\n",
      "[1768779224] Step 162/1488: training loss=1.32\n",
      "[1768779226] Step 163/1488: training loss=0.85\n",
      "[1768779228] Step 164/1488: training loss=1.19\n",
      "[1768779228] Step 165/1488: training loss=0.86\n",
      "[1768779228] Step 166/1488: training loss=0.92\n",
      "[1768779232] Step 167/1488: training loss=1.13\n",
      "[1768779234] Step 168/1488: training loss=1.38\n",
      "[1768779234] Step 169/1488: training loss=0.49\n",
      "[1768779236] Step 170/1488: training loss=0.85\n",
      "[1768779238] Step 171/1488: training loss=0.88\n",
      "[1768779238] Step 172/1488: training loss=0.73\n",
      "[1768779240] Step 173/1488: training loss=0.91\n",
      "[1768779242] Step 174/1488: training loss=1.51\n",
      "[1768779245] Step 175/1488: training loss=1.25\n",
      "[1768779245] Step 176/1488: training loss=0.89\n",
      "[1768779247] Step 177/1488: training loss=0.92\n",
      "[1768779250] Step 178/1488: training loss=0.98\n",
      "[1768779250] Step 179/1488: training loss=1.10\n",
      "[1768779252] Step 180/1488: training loss=0.82\n",
      "[1768779252] Step 181/1488: training loss=0.78\n",
      "[1768779252] Step 182/1488: training loss=0.68\n",
      "[1768779254] Step 183/1488: training loss=0.80\n",
      "[1768779258] Step 184/1488: training loss=0.82\n",
      "[1768779260] Step 185/1488: training loss=0.76\n",
      "[1768779260] Step 186/1488: training loss=0.89\n",
      "[1768779272] Step 187/1488: training loss=1.24\n",
      "[1768779274] Step 188/1488: training loss=0.86\n",
      "[1768779284] Step 189/1488: training loss=1.00\n",
      "[1768779297] Step 190/1488: training loss=1.21\n",
      "[1768779299] Step 191/1488: training loss=0.98\n",
      "[1768779299] Step 192/1488: training loss=1.01\n",
      "[1768779301] Step 193/1488: training loss=0.91\n",
      "[1768779307] Step 194/1488: training loss=0.89\n",
      "[1768779307] Step 195/1488: training loss=0.68\n",
      "[1768779319] Step 196/1488: training loss=1.39\n",
      "[1768779321] Step 197/1488: training loss=1.38\n",
      "[1768779323] Step 198/1488: training loss=0.79\n",
      "[1768779325] Step 199/1488: training loss=0.95\n",
      "[1768779325] Step 200/1488: training loss=0.45\n",
      "[1768779329] Step 201/1488: training loss=1.06\n",
      "[1768779329] Step 202/1488: training loss=0.56\n",
      "[1768779339] Step 203/1488: training loss=1.43\n",
      "[1768779341] Step 204/1488: training loss=1.02\n",
      "[1768779345] Step 205/1488: training loss=0.82\n",
      "[1768779348] Step 206/1488: training loss=1.09\n",
      "[1768779352] Step 207/1488: training loss=1.33\n",
      "[1768779352] Step 208/1488: training loss=1.35\n",
      "[1768779356] Step 209/1488: training loss=1.00\n",
      "[1768779358] Step 210/1488: training loss=0.65\n",
      "[1768779362] Step 211/1488: training loss=0.52\n",
      "[1768779362] Step 212/1488: training loss=0.88\n",
      "[1768779362] Step 213/1488: training loss=0.70\n",
      "[1768779364] Step 214/1488: training loss=0.95\n",
      "[1768779366] Step 215/1488: training loss=1.10\n",
      "[1768779368] Step 216/1488: training loss=0.92\n",
      "[1768779378] Step 217/1488: training loss=1.29\n",
      "[1768779380] Step 218/1488: training loss=0.94\n",
      "[1768779382] Step 219/1488: training loss=0.96\n",
      "[1768779384] Step 220/1488: training loss=0.84\n",
      "[1768779386] Step 221/1488: training loss=0.84\n",
      "[1768779388] Step 222/1488: training loss=0.89\n",
      "[1768779390] Step 223/1488: training loss=1.01\n",
      "[1768779390] Step 224/1488: training loss=1.40\n",
      "[1768779392] Step 225/1488: training loss=0.71\n",
      "[1768779392] Step 226/1488: training loss=1.06\n",
      "[1768779394] Step 227/1488: training loss=1.22\n",
      "[1768779394] Step 228/1488: training loss=0.98\n",
      "[1768779396] Step 229/1488: training loss=1.22\n",
      "[1768779399] Step 230/1488: training loss=0.75\n",
      "[1768779401] Step 231/1488: training loss=0.46\n",
      "[1768779411] Step 232/1488: training loss=0.89\n",
      "[1768779415] Step 233/1488: training loss=0.90\n",
      "[1768779417] Step 234/1488: training loss=0.87\n",
      "[1768779429] Step 235/1488: training loss=0.87\n",
      "[1768779431] Step 236/1488: training loss=0.89\n",
      "[1768779433] Step 237/1488: training loss=0.67\n",
      "[1768779437] Step 238/1488: training loss=0.83\n",
      "[1768779437] Step 239/1488: training loss=0.99\n",
      "[1768779439] Step 240/1488: training loss=0.98\n",
      "[1768779441] Step 241/1488: training loss=0.83\n",
      "[1768779441] Step 242/1488: training loss=0.87\n",
      "[1768779443] Step 243/1488: training loss=0.71\n",
      "[1768779448] Step 244/1488: training loss=1.30\n",
      "[1768779448] Step 245/1488: training loss=0.74\n",
      "[1768779450] Step 246/1488: training loss=1.08\n",
      "[1768779454] Step 247/1488: training loss=0.92\n",
      "[1768779456] Step 248/1488: training loss=1.23\n",
      "[1768779458] Step 249/1488: training loss=0.97\n",
      "[1768779460] Step 250/1488: training loss=0.90\n",
      "[1768779464] Step 251/1488: training loss=1.80\n",
      "[1768779466] Step 252/1488: training loss=0.92\n",
      "[1768779468] Step 253/1488: training loss=0.68\n",
      "[1768779472] Step 254/1488: training loss=0.90\n",
      "[1768779472] Step 255/1488: training loss=0.84\n",
      "[1768779478] Step 256/1488: training loss=1.17\n",
      "[1768779478] Step 257/1488: training loss=1.18\n",
      "[1768779480] Step 258/1488: training loss=0.64\n",
      "[1768779482] Step 259/1488: training loss=0.96\n",
      "[1768779484] Step 260/1488: training loss=0.92\n",
      "[1768779486] Step 261/1488: training loss=0.94\n",
      "[1768779488] Step 262/1488: training loss=0.84\n",
      "[1768779488] Step 263/1488: training loss=0.62\n",
      "[1768779492] Step 264/1488: training loss=0.84\n",
      "[1768779499] Step 265/1488: training loss=0.96\n",
      "[1768779503] Step 266/1488: training loss=1.79\n",
      "[1768779505] Step 267/1488: training loss=0.74\n",
      "[1768779507] Step 268/1488: training loss=0.43\n",
      "[1768779511] Step 269/1488: training loss=0.91\n",
      "[1768779513] Step 270/1488: training loss=0.67\n",
      "[1768779515] Step 271/1488: training loss=0.84\n",
      "[1768779519] Step 272/1488: training loss=0.84\n",
      "[1768779519] Step 273/1488: training loss=0.74\n",
      "[1768779523] Step 274/1488: training loss=1.09\n",
      "[1768779527] Step 275/1488: training loss=1.38\n",
      "[1768779529] Step 276/1488: training loss=1.15\n",
      "[1768779533] Step 277/1488: training loss=0.70\n",
      "[1768779533] Step 278/1488: training loss=0.71\n",
      "[1768779535] Step 279/1488: training loss=0.88\n",
      "[1768779537] Step 280/1488: training loss=0.86\n",
      "[1768779539] Step 281/1488: training loss=1.05\n",
      "[1768779541] Step 282/1488: training loss=1.21\n",
      "[1768779546] Step 283/1488: training loss=1.07\n",
      "[1768779550] Step 284/1488: training loss=1.00\n",
      "[1768779552] Step 285/1488: training loss=0.93\n",
      "[1768779554] Step 286/1488: training loss=0.71\n",
      "[1768779558] Step 287/1488: training loss=1.23\n",
      "[1768779560] Step 288/1488: training loss=1.02\n",
      "[1768779562] Step 289/1488: training loss=1.18\n",
      "[1768779564] Step 290/1488: training loss=0.54\n",
      "[1768779566] Step 291/1488: training loss=0.57\n",
      "[1768779568] Step 292/1488: training loss=0.89\n",
      "[1768779570] Step 293/1488: training loss=1.22\n",
      "[1768779576] Step 294/1488: training loss=0.90\n",
      "[1768779578] Step 295/1488: training loss=0.62\n",
      "[1768779580] Step 296/1488: training loss=1.03\n",
      "[1768779580] Step 297/1488: training loss=0.83\n",
      "[1768779580] Step 298/1488: training loss=0.72\n",
      "[1768779584] Step 299/1488: training loss=1.50\n",
      "[1768779586] Step 300/1488: training loss=0.73\n",
      "[1768779591] Step 301/1488: training loss=0.99\n",
      "[1768779591] Step 302/1488: training loss=0.64\n",
      "[1768779593] Step 303/1488: training loss=0.82\n",
      "[1768779595] Step 304/1488: training loss=1.22\n",
      "[1768779597] Step 305/1488: training loss=0.50\n",
      "[1768779597] Step 306/1488: training loss=0.64\n",
      "[1768779601] Step 307/1488: training loss=0.72\n",
      "[1768779601] Step 308/1488: training loss=0.64\n",
      "[1768779605] Step 309/1488: training loss=1.10\n",
      "[1768779605] Step 310/1488: training loss=0.82\n",
      "[1768779607] Step 311/1488: training loss=0.82\n",
      "[1768779609] Step 312/1488: training loss=1.55\n",
      "[1768779611] Step 313/1488: training loss=0.85\n",
      "[1768779613] Step 314/1488: training loss=0.92\n",
      "[1768779619] Step 315/1488: training loss=1.07\n",
      "[1768779621] Step 316/1488: training loss=0.93\n",
      "[1768779625] Step 317/1488: training loss=0.89\n",
      "[1768779625] Step 318/1488: training loss=0.84\n",
      "[1768779629] Step 319/1488: training loss=0.90\n",
      "[1768779629] Step 320/1488: training loss=0.64\n",
      "[1768779631] Step 321/1488: training loss=0.92\n",
      "[1768779636] Step 322/1488: training loss=1.07\n",
      "[1768779636] Step 323/1488: training loss=0.91\n",
      "[1768779640] Step 324/1488: training loss=1.14\n",
      "[1768779650] Step 325/1488: training loss=1.23\n",
      "[1768779650] Step 326/1488: training loss=1.05\n",
      "[1768779656] Step 327/1488: training loss=0.83\n",
      "[1768779658] Step 328/1488: training loss=1.00\n",
      "[1768779660] Step 329/1488: training loss=1.24\n",
      "[1768779664] Step 330/1488: training loss=0.77\n",
      "[1768779666] Step 331/1488: training loss=1.05\n",
      "[1768779668] Step 332/1488: training loss=1.32\n",
      "[1768779670] Step 333/1488: training loss=0.79\n",
      "[1768779682] Step 334/1488: training loss=1.61\n",
      "[1768779685] Step 335/1488: training loss=1.12\n",
      "[1768779687] Step 336/1488: training loss=0.91\n",
      "[1768779689] Step 337/1488: training loss=1.10\n",
      "[1768779693] Step 338/1488: training loss=0.92\n",
      "[1768779693] Step 339/1488: training loss=0.83\n",
      "[1768779695] Step 340/1488: training loss=1.02\n",
      "[1768779699] Step 341/1488: training loss=1.14\n",
      "[1768779699] Step 342/1488: training loss=0.49\n",
      "[1768779701] Step 343/1488: training loss=0.68\n",
      "[1768779703] Step 344/1488: training loss=0.69\n",
      "[1768779705] Step 345/1488: training loss=1.47\n",
      "[1768779707] Step 346/1488: training loss=0.91\n",
      "[1768779709] Step 347/1488: training loss=0.90\n",
      "[1768779711] Step 348/1488: training loss=1.16\n",
      "[1768779711] Step 349/1488: training loss=0.69\n",
      "[1768779713] Step 350/1488: training loss=0.71\n",
      "[1768779717] Step 351/1488: training loss=1.40\n",
      "[1768779719] Step 352/1488: training loss=0.89\n",
      "[1768779721] Step 353/1488: training loss=0.74\n",
      "[1768779740] Step 354/1488: training loss=0.88\n",
      "[1768779742] Step 355/1488: training loss=1.04\n",
      "[1768779745] Step 356/1488: training loss=1.08\n",
      "[1768779745] Step 357/1488: training loss=0.81\n",
      "[1768779747] Step 358/1488: training loss=0.67\n",
      "[1768779749] Step 359/1488: training loss=1.10\n",
      "[1768779751] Step 360/1488: training loss=0.92\n",
      "[1768779751] Step 361/1488: training loss=1.17\n",
      "[1768779753] Step 362/1488: training loss=0.76\n",
      "[1768779753] Step 363/1488: training loss=0.87\n",
      "[1768779755] Step 364/1488: training loss=0.84\n",
      "[1768779755] Step 365/1488: training loss=0.51\n",
      "[1768779759] Step 366/1488: training loss=1.07\n",
      "[1768779759] Step 367/1488: training loss=0.85\n",
      "[1768779761] Step 368/1488: training loss=0.47\n",
      "[1768779763] Step 369/1488: training loss=0.99\n",
      "[1768779765] Step 370/1488: training loss=0.54\n",
      "[1768779769] Step 371/1488: training loss=0.89\n",
      "[1768779775] Step 372/1488: training loss=1.03\n",
      "[1768779775] Step 373/1488: training loss=0.93\n",
      "[1768779777] Step 374/1488: training loss=0.90\n",
      "[1768779777] Step 375/1488: training loss=0.78\n",
      "[1768779783] Step 376/1488: training loss=0.75\n",
      "[1768779785] Step 377/1488: training loss=1.03\n",
      "[1768779789] Step 378/1488: training loss=0.69\n",
      "[1768779792] Step 379/1488: training loss=1.25\n",
      "[1768779794] Step 380/1488: training loss=1.37\n",
      "[1768779796] Step 381/1488: training loss=0.87\n",
      "[1768779800] Step 382/1488: training loss=0.61\n",
      "[1768779804] Step 383/1488: training loss=1.12\n",
      "[1768779808] Step 384/1488: training loss=0.72\n",
      "[1768779808] Step 385/1488: training loss=0.97\n",
      "[1768779811] Step 386/1488: training loss=0.59\n",
      "[1768779813] Step 387/1488: training loss=1.21\n",
      "[1768779815] Step 388/1488: training loss=0.67\n",
      "[1768779817] Step 389/1488: training loss=0.84\n",
      "[1768779817] Step 390/1488: training loss=1.45\n",
      "[1768779823] Step 391/1488: training loss=1.20\n",
      "[1768779823] Step 392/1488: training loss=0.63\n",
      "[1768779825] Step 393/1488: training loss=0.99\n",
      "[1768779839] Step 394/1488: training loss=0.98\n",
      "[1768779842] Step 395/1488: training loss=1.07\n",
      "[1768779846] Step 396/1488: training loss=0.82\n",
      "[1768779850] Step 397/1488: training loss=0.88\n",
      "[1768779850] Step 398/1488: training loss=0.69\n",
      "[1768779854] Step 399/1488: training loss=0.41\n",
      "[1768779854] Step 400/1488: training loss=0.87\n",
      "[1768779854] Step 401/1488: training loss=0.63\n",
      "[1768779856] Step 402/1488: training loss=0.91\n",
      "[1768779858] Step 403/1488: training loss=1.09\n",
      "[1768779872] Step 404/1488: training loss=1.26\n",
      "[1768779872] Step 405/1488: training loss=0.93\n",
      "[1768779872] Step 406/1488: training loss=0.85\n",
      "[1768779874] Step 407/1488: training loss=0.79\n",
      "[1768779880] Step 408/1488: training loss=1.15\n",
      "[1768779880] Step 409/1488: training loss=1.19\n",
      "[1768779882] Step 410/1488: training loss=0.98\n",
      "[1768779884] Step 411/1488: training loss=0.83\n",
      "[1768779884] Step 412/1488: training loss=0.85\n",
      "[1768779889] Step 413/1488: training loss=0.94\n",
      "[1768779893] Step 414/1488: training loss=1.40\n",
      "[1768779899] Step 415/1488: training loss=0.93\n",
      "[1768779901] Step 416/1488: training loss=0.72\n",
      "[1768779901] Step 417/1488: training loss=0.86\n",
      "[1768779905] Step 418/1488: training loss=0.90\n",
      "[1768779907] Step 419/1488: training loss=0.83\n",
      "[1768779909] Step 420/1488: training loss=0.72\n",
      "[1768779909] Step 421/1488: training loss=1.06\n",
      "[1768779915] Step 422/1488: training loss=1.24\n",
      "[1768779915] Step 423/1488: training loss=0.92\n",
      "[1768779915] Step 424/1488: training loss=0.65\n",
      "[1768779921] Step 425/1488: training loss=1.27\n",
      "[1768779927] Step 426/1488: training loss=0.98\n",
      "[1768779927] Step 427/1488: training loss=0.92\n",
      "[1768779929] Step 428/1488: training loss=0.76\n",
      "[1768779931] Step 429/1488: training loss=0.99\n",
      "[1768779931] Step 430/1488: training loss=1.25\n",
      "[1768779937] Step 431/1488: training loss=0.93\n",
      "[1768779939] Step 432/1488: training loss=0.49\n",
      "[1768779941] Step 433/1488: training loss=0.77\n",
      "[1768779945] Step 434/1488: training loss=0.82\n",
      "[1768779949] Step 435/1488: training loss=1.26\n",
      "[1768779953] Step 436/1488: training loss=0.69\n",
      "[1768779953] Step 437/1488: training loss=0.92\n",
      "[1768779955] Step 438/1488: training loss=0.76\n",
      "[1768779959] Step 439/1488: training loss=0.79\n",
      "[1768779963] Step 440/1488: training loss=0.60\n",
      "[1768779965] Step 441/1488: training loss=0.70\n",
      "[1768779968] Step 442/1488: training loss=1.08\n",
      "[1768779972] Step 443/1488: training loss=0.92\n",
      "[1768779976] Step 444/1488: training loss=0.66\n",
      "[1768779976] Step 445/1488: training loss=0.89\n",
      "[1768779978] Step 446/1488: training loss=0.46\n",
      "[1768779980] Step 447/1488: training loss=1.15\n",
      "[1768779982] Step 448/1488: training loss=0.92\n",
      "[1768779986] Step 449/1488: training loss=0.89\n",
      "[1768779986] Step 450/1488: training loss=0.36\n",
      "[1768779990] Step 451/1488: training loss=1.28\n",
      "[1768779992] Step 452/1488: training loss=0.91\n",
      "[1768779994] Step 453/1488: training loss=1.00\n",
      "[1768779998] Step 454/1488: training loss=1.28\n",
      "[1768780000] Step 455/1488: training loss=1.06\n",
      "[1768780012] Step 456/1488: training loss=0.94\n",
      "[1768780014] Step 457/1488: training loss=1.05\n",
      "[1768780021] Step 458/1488: training loss=1.03\n",
      "[1768780021] Step 459/1488: training loss=0.82\n",
      "[1768780025] Step 460/1488: training loss=1.25\n",
      "[1768780025] Step 461/1488: training loss=0.86\n",
      "[1768780029] Step 462/1488: training loss=0.90\n",
      "[1768780033] Step 463/1488: training loss=1.09\n",
      "[1768780035] Step 464/1488: training loss=1.37\n",
      "[1768780039] Step 465/1488: training loss=1.18\n",
      "[1768780041] Step 466/1488: training loss=0.89\n",
      "[1768780047] Step 467/1488: training loss=1.26\n",
      "[1768780047] Step 468/1488: training loss=1.11\n",
      "[1768780047] Step 469/1488: training loss=0.52\n",
      "[1768780049] Step 470/1488: training loss=0.83\n",
      "[1768780051] Step 471/1488: training loss=0.67\n",
      "[1768780053] Step 472/1488: training loss=0.83\n",
      "[1768780053] Step 473/1488: training loss=1.15\n",
      "[1768780055] Step 474/1488: training loss=1.13\n",
      "[1768780059] Step 475/1488: training loss=1.05\n",
      "[1768780061] Step 476/1488: training loss=0.92\n",
      "[1768780064] Step 477/1488: training loss=0.68\n",
      "[1768780070] Step 478/1488: training loss=1.13\n",
      "[1768780070] Step 479/1488: training loss=1.19\n",
      "[1768780084] Step 480/1488: training loss=1.28\n",
      "[1768780084] Step 481/1488: training loss=0.87\n",
      "[1768780086] Step 482/1488: training loss=0.71\n",
      "[1768780090] Step 483/1488: training loss=1.21\n",
      "[1768780090] Step 484/1488: training loss=0.57\n",
      "[1768780094] Step 485/1488: training loss=1.29\n",
      "[1768780098] Step 486/1488: training loss=1.08\n",
      "[1768780098] Step 487/1488: training loss=0.83\n",
      "[1768780098] Step 488/1488: training loss=0.79\n",
      "[1768780100] Step 489/1488: training loss=0.65\n",
      "[1768780104] Step 490/1488: training loss=0.80\n",
      "[1768780104] Step 491/1488: training loss=0.74\n",
      "[1768780108] Step 492/1488: training loss=1.19\n",
      "[1768780110] Step 493/1488: training loss=0.73\n",
      "[1768780113] Step 494/1488: training loss=0.73\n",
      "[1768780115] Step 495/1488: training loss=1.14\n",
      "[1768780115] Step 496/1488: training loss=0.91\n",
      "[1768780117] Step 497/1488: training loss=0.61\n",
      "[1768780119] Step 498/1488: training loss=0.73\n",
      "[1768780119] Step 499/1488: training loss=0.99\n",
      "[1768780121] Step 500/1488: training loss=1.09\n",
      "[1768780121] Step 501/1488: training loss=1.10\n",
      "[1768780121] Step 502/1488: training loss=0.42\n",
      "[1768780123] Step 503/1488: training loss=0.84\n",
      "[1768780125] Step 504/1488: training loss=0.83\n",
      "[1768780125] Step 505/1488: training loss=0.56\n",
      "[1768780127] Step 506/1488: training loss=0.87\n",
      "[1768780127] Step 507/1488: training loss=0.88\n",
      "[1768780127] Step 508/1488: training loss=0.65\n",
      "[1768780129] Step 509/1488: training loss=0.95\n",
      "[1768780129] Step 510/1488: training loss=0.45\n",
      "[1768780131] Step 511/1488: training loss=0.68\n",
      "[1768780131] Step 512/1488: training loss=0.72\n",
      "[1768780131] Step 513/1488: training loss=0.49\n",
      "[1768780133] Step 514/1488: training loss=0.94\n",
      "[1768780133] Step 515/1488: training loss=1.09\n",
      "[1768780135] Step 516/1488: training loss=0.75\n",
      "[1768780135] Step 517/1488: training loss=0.73\n",
      "[1768780137] Step 518/1488: training loss=1.00\n",
      "[1768780137] Step 519/1488: training loss=0.90\n",
      "[1768780139] Step 520/1488: training loss=0.99\n",
      "[1768780141] Step 521/1488: training loss=1.21\n",
      "[1768780141] Step 522/1488: training loss=1.13\n",
      "[1768780143] Step 523/1488: training loss=0.99\n",
      "[1768780143] Step 524/1488: training loss=0.99\n",
      "[1768780143] Step 525/1488: training loss=0.54\n",
      "[1768780146] Step 526/1488: training loss=0.59\n",
      "[1768780146] Step 527/1488: training loss=1.12\n",
      "[1768780148] Step 528/1488: training loss=1.24\n",
      "[1768780148] Step 529/1488: training loss=0.88\n",
      "[1768780152] Step 530/1488: training loss=0.73\n",
      "[1768780152] Step 531/1488: training loss=1.08\n",
      "[1768780154] Step 532/1488: training loss=0.84\n",
      "[1768780154] Step 533/1488: training loss=0.72\n",
      "[1768780154] Step 534/1488: training loss=0.74\n",
      "[1768780156] Step 535/1488: training loss=0.59\n",
      "[1768780156] Step 536/1488: training loss=0.73\n",
      "[1768780156] Step 537/1488: training loss=0.70\n",
      "[1768780158] Step 538/1488: training loss=0.83\n",
      "[1768780160] Step 539/1488: training loss=0.74\n",
      "[1768780160] Step 540/1488: training loss=0.64\n",
      "[1768780160] Step 541/1488: training loss=0.60\n",
      "[1768780162] Step 542/1488: training loss=0.90\n",
      "[1768780162] Step 543/1488: training loss=0.65\n",
      "[1768780164] Step 544/1488: training loss=1.46\n",
      "[1768780166] Step 545/1488: training loss=0.42\n",
      "[1768780166] Step 546/1488: training loss=0.91\n",
      "[1768780166] Step 547/1488: training loss=0.60\n",
      "[1768780168] Step 548/1488: training loss=0.91\n",
      "[1768780168] Step 549/1488: training loss=0.72\n",
      "[1768780170] Step 550/1488: training loss=1.18\n",
      "[1768780170] Step 551/1488: training loss=0.56\n",
      "[1768780172] Step 552/1488: training loss=0.65\n",
      "[1768780172] Step 553/1488: training loss=0.80\n",
      "[1768780174] Step 554/1488: training loss=1.00\n",
      "[1768780174] Step 555/1488: training loss=0.75\n",
      "[1768780176] Step 556/1488: training loss=0.88\n",
      "[1768780176] Step 557/1488: training loss=0.88\n",
      "[1768780178] Step 558/1488: training loss=0.46\n",
      "[1768780180] Step 559/1488: training loss=0.67\n",
      "[1768780180] Step 560/1488: training loss=0.67\n",
      "[1768780182] Step 561/1488: training loss=0.49\n",
      "[1768780184] Step 562/1488: training loss=1.39\n",
      "[1768780185] Step 563/1488: training loss=0.72\n",
      "[1768780187] Step 564/1488: training loss=0.76\n",
      "[1768780187] Step 565/1488: training loss=0.68\n",
      "[1768780187] Step 566/1488: training loss=0.86\n",
      "[1768780189] Step 567/1488: training loss=0.75\n",
      "[1768780189] Step 568/1488: training loss=0.83\n",
      "[1768780191] Step 569/1488: training loss=0.52\n",
      "[1768780191] Step 570/1488: training loss=0.87\n",
      "[1768780191] Step 571/1488: training loss=0.85\n",
      "[1768780193] Step 572/1488: training loss=0.64\n",
      "[1768780193] Step 573/1488: training loss=0.71\n",
      "[1768780195] Step 574/1488: training loss=1.19\n",
      "[1768780195] Step 575/1488: training loss=0.55\n",
      "[1768780197] Step 576/1488: training loss=0.89\n",
      "[1768780197] Step 577/1488: training loss=0.60\n",
      "[1768780197] Step 578/1488: training loss=0.74\n",
      "[1768780199] Step 579/1488: training loss=0.91\n",
      "[1768780199] Step 580/1488: training loss=0.66\n",
      "[1768780201] Step 581/1488: training loss=0.84\n",
      "[1768780203] Step 582/1488: training loss=1.30\n",
      "[1768780203] Step 583/1488: training loss=0.78\n",
      "[1768780205] Step 584/1488: training loss=0.90\n",
      "[1768780207] Step 585/1488: training loss=0.99\n",
      "[1768780207] Step 586/1488: training loss=0.73\n",
      "[1768780209] Step 587/1488: training loss=0.69\n",
      "[1768780211] Step 588/1488: training loss=0.95\n",
      "[1768780211] Step 589/1488: training loss=1.11\n",
      "[1768780213] Step 590/1488: training loss=1.03\n",
      "[1768780213] Step 591/1488: training loss=0.55\n",
      "[1768780216] Step 592/1488: training loss=1.10\n",
      "[1768780216] Step 593/1488: training loss=0.62\n",
      "[1768780218] Step 594/1488: training loss=0.97\n",
      "[1768780218] Step 595/1488: training loss=0.68\n",
      "[1768780220] Step 596/1488: training loss=1.09\n",
      "[1768780220] Step 597/1488: training loss=0.86\n",
      "[1768780222] Step 598/1488: training loss=1.19\n",
      "[1768780222] Step 599/1488: training loss=0.71\n",
      "[1768780222] Step 600/1488: training loss=0.93\n",
      "[1768780224] Step 601/1488: training loss=0.93\n",
      "[1768780224] Step 602/1488: training loss=0.55\n",
      "[1768780224] Step 603/1488: training loss=0.52\n",
      "[1768780226] Step 604/1488: training loss=0.73\n",
      "[1768780226] Step 605/1488: training loss=0.84\n",
      "[1768780226] Step 606/1488: training loss=0.99\n",
      "[1768780228] Step 607/1488: training loss=0.55\n",
      "[1768780228] Step 608/1488: training loss=1.24\n",
      "[1768780230] Step 609/1488: training loss=0.67\n",
      "[1768780230] Step 610/1488: training loss=0.72\n",
      "[1768780233] Step 611/1488: training loss=1.22\n",
      "[1768780233] Step 612/1488: training loss=0.75\n",
      "[1768780237] Step 613/1488: training loss=1.07\n",
      "[1768780237] Step 614/1488: training loss=0.68\n",
      "[1768780239] Step 615/1488: training loss=0.51\n",
      "[1768780239] Step 616/1488: training loss=0.79\n",
      "[1768780239] Step 617/1488: training loss=0.86\n",
      "[1768780241] Step 618/1488: training loss=0.63\n",
      "[1768780241] Step 619/1488: training loss=0.72\n",
      "[1768780241] Step 620/1488: training loss=0.54\n",
      "[1768780243] Step 621/1488: training loss=0.64\n",
      "[1768780245] Step 622/1488: training loss=0.76\n",
      "[1768780245] Step 623/1488: training loss=0.55\n",
      "[1768780245] Step 624/1488: training loss=0.84\n",
      "[1768780247] Step 625/1488: training loss=1.03\n",
      "[1768780249] Step 626/1488: training loss=1.17\n",
      "[1768780249] Step 627/1488: training loss=0.72\n",
      "[1768780251] Step 628/1488: training loss=1.07\n",
      "[1768780253] Step 629/1488: training loss=1.01\n",
      "[1768780253] Step 630/1488: training loss=0.91\n",
      "[1768780253] Step 631/1488: training loss=0.96\n",
      "[1768780255] Step 632/1488: training loss=1.03\n",
      "[1768780255] Step 633/1488: training loss=0.95\n",
      "[1768780257] Step 634/1488: training loss=0.85\n",
      "[1768780257] Step 635/1488: training loss=0.76\n",
      "[1768780259] Step 636/1488: training loss=0.83\n",
      "[1768780261] Step 637/1488: training loss=0.75\n",
      "[1768780261] Step 638/1488: training loss=0.70\n",
      "[1768780265] Step 639/1488: training loss=0.85\n",
      "[1768780265] Step 640/1488: training loss=0.80\n",
      "[1768780267] Step 641/1488: training loss=0.90\n",
      "[1768780267] Step 642/1488: training loss=0.79\n",
      "[1768780269] Step 643/1488: training loss=0.53\n",
      "[1768780269] Step 644/1488: training loss=0.63\n",
      "[1768780269] Step 645/1488: training loss=0.51\n",
      "[1768780271] Step 646/1488: training loss=0.46\n",
      "[1768780271] Step 647/1488: training loss=0.53\n",
      "[1768780273] Step 648/1488: training loss=0.86\n",
      "[1768780273] Step 649/1488: training loss=0.82\n",
      "[1768780273] Step 650/1488: training loss=1.13\n",
      "[1768780276] Step 651/1488: training loss=0.74\n",
      "[1768780276] Step 652/1488: training loss=0.41\n",
      "[1768780276] Step 653/1488: training loss=0.76\n",
      "[1768780278] Step 654/1488: training loss=1.28\n",
      "[1768780280] Step 655/1488: training loss=0.72\n",
      "[1768780282] Step 656/1488: training loss=0.68\n",
      "[1768780282] Step 657/1488: training loss=0.47\n",
      "[1768780284] Step 658/1488: training loss=0.53\n",
      "[1768780286] Step 659/1488: training loss=1.07\n",
      "[1768780286] Step 660/1488: training loss=1.36\n",
      "[1768780288] Step 661/1488: training loss=1.06\n",
      "[1768780288] Step 662/1488: training loss=0.52\n",
      "[1768780290] Step 663/1488: training loss=1.03\n",
      "[1768780290] Step 664/1488: training loss=0.90\n",
      "[1768780292] Step 665/1488: training loss=0.85\n",
      "[1768780292] Step 666/1488: training loss=1.04\n",
      "[1768780294] Step 667/1488: training loss=0.84\n",
      "[1768780294] Step 668/1488: training loss=1.09\n",
      "[1768780296] Step 669/1488: training loss=0.86\n",
      "[1768780296] Step 670/1488: training loss=0.82\n",
      "[1768780298] Step 671/1488: training loss=0.91\n",
      "[1768780298] Step 672/1488: training loss=0.57\n",
      "[1768780298] Step 673/1488: training loss=1.18\n",
      "[1768780300] Step 674/1488: training loss=1.08\n",
      "[1768780302] Step 675/1488: training loss=0.58\n",
      "[1768780302] Step 676/1488: training loss=1.11\n",
      "[1768780302] Step 677/1488: training loss=1.03\n",
      "[1768780304] Step 678/1488: training loss=0.71\n",
      "[1768780304] Step 679/1488: training loss=0.98\n",
      "[1768780306] Step 680/1488: training loss=0.70\n",
      "[1768780306] Step 681/1488: training loss=0.66\n",
      "[1768780306] Step 682/1488: training loss=0.73\n",
      "[1768780308] Step 683/1488: training loss=0.70\n",
      "[1768780308] Step 684/1488: training loss=0.72\n",
      "[1768780310] Step 685/1488: training loss=1.30\n",
      "[1768780310] Step 686/1488: training loss=1.09\n",
      "[1768780310] Step 687/1488: training loss=0.53\n",
      "[1768780312] Step 688/1488: training loss=0.93\n",
      "[1768780312] Step 689/1488: training loss=0.83\n",
      "[1768780314] Step 690/1488: training loss=0.88\n",
      "[1768780314] Step 691/1488: training loss=0.88\n",
      "[1768780314] Step 692/1488: training loss=0.65\n",
      "[1768780316] Step 693/1488: training loss=0.71\n",
      "[1768780319] Step 694/1488: training loss=1.06\n",
      "[1768780319] Step 695/1488: training loss=0.64\n",
      "[1768780319] Step 696/1488: training loss=0.80\n",
      "[1768780323] Step 697/1488: training loss=1.00\n",
      "[1768780323] Step 698/1488: training loss=1.03\n",
      "[1768780323] Step 699/1488: training loss=0.58\n",
      "[1768780325] Step 700/1488: training loss=0.76\n",
      "[1768780327] Step 701/1488: training loss=0.83\n",
      "[1768780327] Step 702/1488: training loss=0.70\n",
      "[1768780329] Step 703/1488: training loss=0.93\n",
      "[1768780329] Step 704/1488: training loss=0.64\n",
      "[1768780331] Step 705/1488: training loss=0.73\n",
      "[1768780331] Step 706/1488: training loss=0.81\n",
      "[1768780333] Step 707/1488: training loss=0.59\n",
      "[1768780333] Step 708/1488: training loss=0.80\n",
      "[1768780333] Step 709/1488: training loss=0.58\n",
      "[1768780335] Step 710/1488: training loss=0.78\n",
      "[1768780337] Step 711/1488: training loss=0.92\n",
      "[1768780337] Step 712/1488: training loss=0.75\n",
      "[1768780339] Step 713/1488: training loss=1.05\n",
      "[1768780341] Step 714/1488: training loss=0.87\n",
      "[1768780341] Step 715/1488: training loss=0.97\n",
      "[1768780343] Step 716/1488: training loss=0.73\n",
      "[1768780343] Step 717/1488: training loss=1.47\n",
      "[1768780345] Step 718/1488: training loss=1.55\n",
      "[1768780345] Step 719/1488: training loss=0.54\n",
      "[1768780347] Step 720/1488: training loss=0.64\n",
      "[1768780347] Step 721/1488: training loss=0.81\n",
      "[1768780347] Step 722/1488: training loss=0.77\n",
      "[1768780349] Step 723/1488: training loss=0.75\n",
      "[1768780349] Step 724/1488: training loss=1.10\n",
      "[1768780349] Step 725/1488: training loss=0.75\n",
      "[1768780351] Step 726/1488: training loss=0.75\n",
      "[1768780351] Step 727/1488: training loss=0.95\n",
      "[1768780353] Step 728/1488: training loss=0.74\n",
      "[1768780353] Step 729/1488: training loss=0.74\n",
      "[1768780353] Step 730/1488: training loss=0.79\n",
      "[1768780355] Step 731/1488: training loss=0.57\n",
      "[1768780357] Step 732/1488: training loss=0.89\n",
      "[1768780359] Step 733/1488: training loss=0.71\n",
      "[1768780359] Step 734/1488: training loss=0.68\n",
      "[1768780362] Step 735/1488: training loss=0.72\n",
      "[1768780362] Step 736/1488: training loss=0.87\n",
      "[1768780362] Step 737/1488: training loss=0.58\n",
      "[1768780364] Step 738/1488: training loss=0.65\n",
      "[1768780364] Step 739/1488: training loss=0.55\n",
      "[1768780366] Step 740/1488: training loss=0.72\n",
      "[1768780366] Step 741/1488: training loss=0.66\n",
      "[1768780368] Step 742/1488: training loss=1.12\n",
      "[1768780368] Step 743/1488: training loss=0.82\n",
      "[1768780368] Step 744/1488: training loss=0.47\n",
      "[1768780370] Step 745/1488: training loss=0.95\n",
      "[1768780370] Step 746/1488: training loss=0.73\n",
      "[1768780370] Step 747/1488: training loss=0.59\n",
      "[1768780372] Step 748/1488: training loss=0.81\n",
      "[1768780372] Step 749/1488: training loss=0.83\n",
      "[1768780372] Step 750/1488: training loss=1.11\n",
      "[1768780374] Step 751/1488: training loss=0.83\n",
      "[1768780374] Step 752/1488: training loss=0.53\n",
      "[1768780374] Step 753/1488: training loss=0.50\n",
      "[1768780376] Step 754/1488: training loss=0.52\n",
      "[1768780376] Step 755/1488: training loss=1.07\n",
      "[1768780376] Step 756/1488: training loss=0.77\n",
      "[1768780378] Step 757/1488: training loss=0.79\n",
      "[1768780378] Step 758/1488: training loss=0.77\n",
      "[1768780380] Step 759/1488: training loss=0.99\n",
      "[1768780380] Step 760/1488: training loss=0.92\n",
      "[1768780382] Step 761/1488: training loss=1.04\n",
      "[1768780382] Step 762/1488: training loss=0.63\n",
      "[1768780384] Step 763/1488: training loss=0.76\n",
      "[1768780384] Step 764/1488: training loss=1.05\n",
      "[1768780384] Step 765/1488: training loss=0.70\n",
      "[1768780384] Step 766/1488: training loss=1.25\n",
      "[1768780388] Step 767/1488: training loss=1.24\n",
      "[1768780388] Step 768/1488: training loss=1.16\n",
      "[1768780390] Step 769/1488: training loss=1.25\n",
      "[1768780390] Step 770/1488: training loss=0.83\n",
      "[1768780392] Step 771/1488: training loss=1.45\n",
      "[1768780394] Step 772/1488: training loss=0.91\n",
      "[1768780396] Step 773/1488: training loss=0.93\n",
      "[1768780396] Step 774/1488: training loss=1.03\n",
      "[1768780396] Step 775/1488: training loss=0.66\n",
      "[1768780398] Step 776/1488: training loss=0.89\n",
      "[1768780398] Step 777/1488: training loss=0.63\n",
      "[1768780400] Step 778/1488: training loss=0.85\n",
      "[1768780400] Step 779/1488: training loss=0.64\n",
      "[1768780400] Step 780/1488: training loss=0.28\n",
      "[1768780403] Step 781/1488: training loss=0.37\n",
      "[1768780403] Step 782/1488: training loss=0.73\n",
      "[1768780405] Step 783/1488: training loss=0.64\n",
      "[1768780409] Step 784/1488: training loss=1.31\n",
      "[1768780409] Step 785/1488: training loss=0.87\n",
      "[1768780409] Step 786/1488: training loss=1.03\n",
      "[1768780411] Step 787/1488: training loss=0.98\n",
      "[1768780415] Step 788/1488: training loss=1.14\n",
      "[1768780415] Step 789/1488: training loss=0.67\n",
      "[1768780415] Step 790/1488: training loss=0.51\n",
      "[1768780419] Step 791/1488: training loss=0.80\n",
      "[1768780421] Step 792/1488: training loss=0.83\n",
      "[1768780421] Step 793/1488: training loss=1.00\n",
      "[1768780421] Step 794/1488: training loss=0.99\n",
      "[1768780423] Step 795/1488: training loss=1.04\n",
      "[1768780423] Step 796/1488: training loss=0.63\n",
      "[1768780425] Step 797/1488: training loss=0.85\n",
      "[1768780425] Step 798/1488: training loss=0.80\n",
      "[1768780427] Step 799/1488: training loss=0.66\n",
      "[1768780427] Step 800/1488: training loss=0.71\n",
      "[1768780429] Step 801/1488: training loss=0.92\n",
      "[1768780429] Step 802/1488: training loss=0.78\n",
      "[1768780431] Step 803/1488: training loss=0.37\n",
      "[1768780431] Step 804/1488: training loss=0.86\n",
      "[1768780433] Step 805/1488: training loss=0.69\n",
      "[1768780435] Step 806/1488: training loss=0.95\n",
      "[1768780435] Step 807/1488: training loss=0.48\n",
      "[1768780435] Step 808/1488: training loss=0.73\n",
      "[1768780437] Step 809/1488: training loss=0.53\n",
      "[1768780437] Step 810/1488: training loss=0.74\n",
      "[1768780439] Step 811/1488: training loss=1.01\n",
      "[1768780439] Step 812/1488: training loss=0.82\n",
      "[1768780441] Step 813/1488: training loss=1.10\n",
      "[1768780441] Step 814/1488: training loss=0.71\n",
      "[1768780443] Step 815/1488: training loss=0.91\n",
      "[1768780443] Step 816/1488: training loss=0.77\n",
      "[1768780443] Step 817/1488: training loss=0.34\n",
      "[1768780446] Step 818/1488: training loss=1.17\n",
      "[1768780446] Step 819/1488: training loss=0.92\n",
      "[1768780446] Step 820/1488: training loss=1.04\n",
      "[1768780448] Step 821/1488: training loss=0.74\n",
      "[1768780448] Step 822/1488: training loss=0.60\n",
      "[1768780448] Step 823/1488: training loss=1.04\n",
      "[1768780450] Step 824/1488: training loss=0.74\n",
      "[1768780452] Step 825/1488: training loss=0.86\n",
      "[1768780452] Step 826/1488: training loss=0.73\n",
      "[1768780452] Step 827/1488: training loss=0.66\n",
      "[1768780456] Step 828/1488: training loss=0.76\n",
      "[1768780456] Step 829/1488: training loss=0.92\n",
      "[1768780456] Step 830/1488: training loss=0.86\n",
      "[1768780458] Step 831/1488: training loss=0.87\n",
      "[1768780458] Step 832/1488: training loss=0.47\n",
      "[1768780460] Step 833/1488: training loss=0.88\n",
      "[1768780460] Step 834/1488: training loss=0.73\n",
      "[1768780460] Step 835/1488: training loss=0.85\n",
      "[1768780462] Step 836/1488: training loss=0.36\n",
      "[1768780466] Step 837/1488: training loss=1.14\n",
      "[1768780466] Step 838/1488: training loss=0.47\n",
      "[1768780468] Step 839/1488: training loss=1.11\n",
      "[1768780468] Step 840/1488: training loss=0.53\n",
      "[1768780470] Step 841/1488: training loss=0.81\n",
      "[1768780470] Step 842/1488: training loss=0.91\n",
      "[1768780470] Step 843/1488: training loss=0.94\n",
      "[1768780472] Step 844/1488: training loss=0.73\n",
      "[1768780472] Step 845/1488: training loss=0.95\n",
      "[1768780474] Step 846/1488: training loss=0.80\n",
      "[1768780474] Step 847/1488: training loss=0.78\n",
      "[1768780476] Step 848/1488: training loss=0.77\n",
      "[1768780476] Step 849/1488: training loss=0.54\n",
      "[1768780476] Step 850/1488: training loss=0.77\n",
      "[1768780478] Step 851/1488: training loss=0.71\n",
      "[1768780478] Step 852/1488: training loss=0.77\n",
      "[1768780478] Step 853/1488: training loss=0.52\n",
      "[1768780480] Step 854/1488: training loss=1.06\n",
      "[1768780480] Step 855/1488: training loss=0.85\n",
      "[1768780482] Step 856/1488: training loss=0.89\n",
      "[1768780482] Step 857/1488: training loss=0.53\n",
      "[1768780484] Step 858/1488: training loss=0.72\n",
      "[1768780484] Step 859/1488: training loss=0.67\n",
      "[1768780486] Step 860/1488: training loss=0.53\n",
      "[1768780486] Step 861/1488: training loss=0.84\n",
      "[1768780486] Step 862/1488: training loss=0.65\n",
      "[1768780489] Step 863/1488: training loss=0.57\n",
      "[1768780489] Step 864/1488: training loss=0.79\n",
      "[1768780491] Step 865/1488: training loss=0.86\n",
      "[1768780491] Step 866/1488: training loss=0.91\n",
      "[1768780493] Step 867/1488: training loss=1.29\n",
      "[1768780495] Step 868/1488: training loss=0.97\n",
      "[1768780495] Step 869/1488: training loss=0.42\n",
      "[1768780497] Step 870/1488: training loss=0.78\n",
      "[1768780497] Step 871/1488: training loss=0.46\n",
      "[1768780497] Step 872/1488: training loss=1.42\n",
      "[1768780499] Step 873/1488: training loss=1.06\n",
      "[1768780499] Step 874/1488: training loss=0.47\n",
      "[1768780501] Step 875/1488: training loss=0.84\n",
      "[1768780501] Step 876/1488: training loss=0.74\n",
      "[1768780501] Step 877/1488: training loss=0.91\n",
      "[1768780503] Step 878/1488: training loss=0.98\n",
      "[1768780505] Step 879/1488: training loss=0.73\n",
      "[1768780505] Step 880/1488: training loss=0.51\n",
      "[1768780505] Step 881/1488: training loss=0.60\n",
      "[1768780507] Step 882/1488: training loss=1.01\n",
      "[1768780507] Step 883/1488: training loss=0.93\n",
      "[1768780509] Step 884/1488: training loss=0.71\n",
      "[1768780509] Step 885/1488: training loss=1.01\n",
      "[1768780509] Step 886/1488: training loss=0.45\n",
      "[1768780511] Step 887/1488: training loss=0.76\n",
      "[1768780513] Step 888/1488: training loss=1.06\n",
      "[1768780513] Step 889/1488: training loss=0.43\n",
      "[1768780516] Step 890/1488: training loss=1.13\n",
      "[1768780516] Step 891/1488: training loss=0.73\n",
      "[1768780518] Step 892/1488: training loss=0.83\n",
      "[1768780518] Step 893/1488: training loss=0.19\n",
      "[1768780520] Step 894/1488: training loss=0.96\n",
      "[1768780520] Step 895/1488: training loss=0.87\n",
      "[1768780522] Step 896/1488: training loss=0.53\n",
      "[1768780522] Step 897/1488: training loss=0.92\n",
      "[1768780522] Step 898/1488: training loss=0.54\n",
      "[1768780524] Step 899/1488: training loss=0.48\n",
      "[1768780524] Step 900/1488: training loss=0.43\n",
      "[1768780524] Step 901/1488: training loss=0.87\n",
      "[1768780526] Step 902/1488: training loss=0.63\n",
      "[1768780526] Step 903/1488: training loss=0.61\n",
      "[1768780526] Step 904/1488: training loss=0.71\n",
      "[1768780530] Step 905/1488: training loss=0.85\n",
      "[1768780530] Step 906/1488: training loss=1.12\n",
      "[1768780532] Step 907/1488: training loss=1.32\n",
      "[1768780532] Step 908/1488: training loss=0.59\n",
      "[1768780532] Step 909/1488: training loss=0.38\n",
      "[1768780534] Step 910/1488: training loss=0.79\n",
      "[1768780534] Step 911/1488: training loss=0.47\n",
      "[1768780536] Step 912/1488: training loss=0.63\n",
      "[1768780536] Step 913/1488: training loss=1.15\n",
      "[1768780538] Step 914/1488: training loss=1.00\n",
      "[1768780542] Step 915/1488: training loss=1.08\n",
      "[1768780542] Step 916/1488: training loss=1.03\n",
      "[1768780542] Step 917/1488: training loss=0.48\n",
      "[1768780544] Step 918/1488: training loss=0.67\n",
      "[1768780544] Step 919/1488: training loss=0.82\n",
      "[1768780546] Step 920/1488: training loss=0.52\n",
      "[1768780546] Step 921/1488: training loss=0.96\n",
      "[1768780548] Step 922/1488: training loss=0.40\n",
      "[1768780548] Step 923/1488: training loss=0.71\n",
      "[1768780550] Step 924/1488: training loss=1.12\n",
      "[1768780552] Step 925/1488: training loss=0.65\n",
      "[1768780552] Step 926/1488: training loss=0.83\n",
      "[1768780554] Step 927/1488: training loss=0.74\n",
      "[1768780554] Step 928/1488: training loss=0.80\n",
      "[1768780554] Step 929/1488: training loss=1.19\n",
      "[1768780557] Step 930/1488: training loss=0.91\n",
      "[1768780557] Step 931/1488: training loss=0.69\n",
      "[1768780559] Step 932/1488: training loss=1.11\n",
      "[1768780559] Step 933/1488: training loss=0.65\n",
      "[1768780559] Step 934/1488: training loss=0.83\n",
      "[1768780561] Step 935/1488: training loss=0.69\n",
      "[1768780561] Step 936/1488: training loss=0.84\n",
      "[1768780563] Step 937/1488: training loss=0.69\n",
      "[1768780563] Step 938/1488: training loss=0.75\n",
      "[1768780563] Step 939/1488: training loss=0.67\n",
      "[1768780565] Step 940/1488: training loss=0.74\n",
      "[1768780565] Step 941/1488: training loss=0.58\n",
      "[1768780565] Step 942/1488: training loss=0.89\n",
      "[1768780567] Step 943/1488: training loss=1.02\n",
      "[1768780567] Step 944/1488: training loss=0.90\n",
      "[1768780569] Step 945/1488: training loss=0.73\n",
      "[1768780571] Step 946/1488: training loss=1.04\n",
      "[1768780571] Step 947/1488: training loss=0.79\n",
      "[1768780573] Step 948/1488: training loss=0.84\n",
      "[1768780575] Step 949/1488: training loss=0.78\n",
      "[1768780575] Step 950/1488: training loss=1.14\n",
      "[1768780577] Step 951/1488: training loss=0.67\n",
      "[1768780579] Step 952/1488: training loss=0.89\n",
      "[1768780581] Step 953/1488: training loss=0.90\n",
      "[1768780581] Step 954/1488: training loss=0.80\n",
      "[1768780581] Step 955/1488: training loss=0.52\n",
      "[1768780583] Step 956/1488: training loss=0.48\n",
      "[1768780583] Step 957/1488: training loss=1.08\n",
      "[1768780585] Step 958/1488: training loss=0.77\n",
      "[1768780585] Step 959/1488: training loss=0.49\n",
      "[1768780585] Step 960/1488: training loss=0.71\n",
      "[1768780587] Step 961/1488: training loss=1.09\n",
      "[1768780587] Step 962/1488: training loss=0.56\n",
      "[1768780589] Step 963/1488: training loss=0.56\n",
      "[1768780589] Step 964/1488: training loss=0.34\n",
      "[1768780589] Step 965/1488: training loss=0.65\n",
      "[1768780591] Step 966/1488: training loss=0.76\n",
      "[1768780591] Step 967/1488: training loss=0.60\n",
      "[1768780593] Step 968/1488: training loss=0.86\n",
      "[1768780595] Step 969/1488: training loss=0.59\n",
      "[1768780595] Step 970/1488: training loss=0.76\n",
      "[1768780595] Step 971/1488: training loss=0.61\n",
      "[1768780597] Step 972/1488: training loss=0.35\n",
      "[1768780598] Step 973/1488: training loss=0.77\n",
      "[1768780600] Step 974/1488: training loss=0.81\n",
      "[1768780600] Step 975/1488: training loss=0.83\n",
      "[1768780600] Step 976/1488: training loss=0.69\n",
      "[1768780602] Step 977/1488: training loss=0.68\n",
      "[1768780602] Step 978/1488: training loss=0.56\n",
      "[1768780604] Step 979/1488: training loss=1.20\n",
      "[1768780604] Step 980/1488: training loss=0.89\n",
      "[1768780604] Step 981/1488: training loss=1.45\n",
      "[1768780606] Step 982/1488: training loss=0.75\n",
      "[1768780606] Step 983/1488: training loss=1.12\n",
      "[1768780608] Step 984/1488: training loss=0.85\n",
      "[1768780608] Step 985/1488: training loss=0.96\n",
      "[1768780608] Step 986/1488: training loss=0.66\n",
      "[1768780610] Step 987/1488: training loss=0.77\n",
      "[1768780612] Step 988/1488: training loss=0.30\n",
      "[1768780612] Step 989/1488: training loss=0.53\n",
      "[1768780612] Step 990/1488: training loss=0.69\n",
      "[1768780614] Step 991/1488: training loss=0.57\n",
      "[1768780614] Step 992/1488: training loss=0.72\n",
      "[1768780616] Step 993/1488: training loss=0.62\n",
      "[1768780618] Step 994/1488: training loss=0.57\n",
      "[1768780618] Step 995/1488: training loss=0.58\n",
      "[1768780618] Step 996/1488: training loss=0.73\n",
      "[1768780634] Step 997/1488: training loss=0.79\n",
      "[1768780637] Step 998/1488: training loss=0.64\n",
      "[1768780639] Step 999/1488: training loss=0.52\n",
      "[1768780639] Step 1000/1488: training loss=0.59\n",
      "[1768780641] Step 1001/1488: training loss=0.62\n",
      "[1768780641] Step 1002/1488: training loss=0.57\n",
      "[1768780641] Step 1003/1488: training loss=0.49\n",
      "[1768780641] Step 1004/1488: training loss=0.43\n",
      "[1768780643] Step 1005/1488: training loss=1.34\n",
      "[1768780645] Step 1006/1488: training loss=0.66\n",
      "[1768780645] Step 1007/1488: training loss=0.60\n",
      "[1768780647] Step 1008/1488: training loss=0.78\n",
      "[1768780647] Step 1009/1488: training loss=0.61\n",
      "[1768780649] Step 1010/1488: training loss=0.41\n",
      "[1768780649] Step 1011/1488: training loss=0.85\n",
      "[1768780651] Step 1012/1488: training loss=0.90\n",
      "[1768780651] Step 1013/1488: training loss=0.53\n",
      "[1768780653] Step 1014/1488: training loss=1.28\n",
      "[1768780653] Step 1015/1488: training loss=1.00\n",
      "[1768780653] Step 1016/1488: training loss=0.51\n",
      "[1768780655] Step 1017/1488: training loss=0.30\n",
      "[1768780655] Step 1018/1488: training loss=0.76\n",
      "[1768780657] Step 1019/1488: training loss=0.99\n",
      "[1768780657] Step 1020/1488: training loss=0.39\n",
      "[1768780659] Step 1021/1488: training loss=0.64\n",
      "[1768780661] Step 1022/1488: training loss=0.60\n",
      "[1768780661] Step 1023/1488: training loss=0.62\n",
      "[1768780661] Step 1024/1488: training loss=0.30\n",
      "[1768780663] Step 1025/1488: training loss=0.88\n",
      "[1768780663] Step 1026/1488: training loss=0.99\n",
      "[1768780663] Step 1027/1488: training loss=0.86\n",
      "[1768780665] Step 1028/1488: training loss=0.53\n",
      "[1768780665] Step 1029/1488: training loss=0.93\n",
      "[1768780665] Step 1030/1488: training loss=0.67\n",
      "[1768780667] Step 1031/1488: training loss=0.47\n",
      "[1768780667] Step 1032/1488: training loss=0.65\n",
      "[1768780671] Step 1033/1488: training loss=0.60\n",
      "[1768780671] Step 1034/1488: training loss=0.77\n",
      "[1768780673] Step 1035/1488: training loss=0.70\n",
      "[1768780673] Step 1036/1488: training loss=0.58\n",
      "[1768780673] Step 1037/1488: training loss=0.45\n",
      "[1768780675] Step 1038/1488: training loss=0.78\n",
      "[1768780677] Step 1039/1488: training loss=0.51\n",
      "[1768780678] Step 1040/1488: training loss=0.64\n",
      "[1768780680] Step 1041/1488: training loss=1.14\n",
      "[1768780680] Step 1042/1488: training loss=0.65\n",
      "[1768780682] Step 1043/1488: training loss=0.67\n",
      "[1768780684] Step 1044/1488: training loss=0.87\n",
      "[1768780686] Step 1045/1488: training loss=0.96\n",
      "[1768780688] Step 1046/1488: training loss=0.90\n",
      "[1768780690] Step 1047/1488: training loss=0.85\n",
      "[1768780690] Step 1048/1488: training loss=0.58\n",
      "[1768780692] Step 1049/1488: training loss=0.67\n",
      "[1768780692] Step 1050/1488: training loss=1.02\n",
      "[1768780692] Step 1051/1488: training loss=0.43\n",
      "[1768780694] Step 1052/1488: training loss=0.89\n",
      "[1768780694] Step 1053/1488: training loss=0.78\n",
      "[1768780696] Step 1054/1488: training loss=0.73\n",
      "[1768780696] Step 1055/1488: training loss=0.58\n",
      "[1768780698] Step 1056/1488: training loss=0.81\n",
      "[1768780698] Step 1057/1488: training loss=0.55\n",
      "[1768780700] Step 1058/1488: training loss=0.78\n",
      "[1768780700] Step 1059/1488: training loss=0.51\n",
      "[1768780702] Step 1060/1488: training loss=0.43\n",
      "[1768780702] Step 1061/1488: training loss=0.92\n",
      "[1768780702] Step 1062/1488: training loss=0.59\n",
      "[1768780704] Step 1063/1488: training loss=0.51\n",
      "[1768780704] Step 1064/1488: training loss=0.99\n",
      "[1768780706] Step 1065/1488: training loss=0.95\n",
      "[1768780706] Step 1066/1488: training loss=0.83\n",
      "[1768780708] Step 1067/1488: training loss=0.37\n",
      "[1768780708] Step 1068/1488: training loss=0.32\n",
      "[1768780708] Step 1069/1488: training loss=1.02\n",
      "[1768780710] Step 1070/1488: training loss=0.68\n",
      "[1768780710] Step 1071/1488: training loss=0.61\n",
      "[1768780710] Step 1072/1488: training loss=0.49\n",
      "[1768780712] Step 1073/1488: training loss=1.30\n",
      "[1768780714] Step 1074/1488: training loss=0.71\n",
      "[1768780714] Step 1075/1488: training loss=0.44\n",
      "[1768780714] Step 1076/1488: training loss=0.56\n",
      "[1768780717] Step 1077/1488: training loss=0.88\n",
      "[1768780717] Step 1078/1488: training loss=0.54\n",
      "[1768780719] Step 1079/1488: training loss=0.64\n",
      "[1768780719] Step 1080/1488: training loss=0.92\n",
      "[1768780719] Step 1081/1488: training loss=0.58\n",
      "[1768780721] Step 1082/1488: training loss=0.41\n",
      "[1768780721] Step 1083/1488: training loss=0.95\n",
      "[1768780725] Step 1084/1488: training loss=1.00\n",
      "[1768780725] Step 1085/1488: training loss=0.51\n",
      "[1768780725] Step 1086/1488: training loss=0.61\n",
      "[1768780727] Step 1087/1488: training loss=0.89\n",
      "[1768780729] Step 1088/1488: training loss=1.03\n",
      "[1768780729] Step 1089/1488: training loss=0.81\n",
      "[1768780729] Step 1090/1488: training loss=0.68\n",
      "[1768780731] Step 1091/1488: training loss=0.60\n",
      "[1768780731] Step 1092/1488: training loss=0.77\n",
      "[1768780731] Step 1093/1488: training loss=0.61\n",
      "[1768780733] Step 1094/1488: training loss=0.89\n",
      "[1768780735] Step 1095/1488: training loss=1.06\n",
      "[1768780737] Step 1096/1488: training loss=0.74\n",
      "[1768780737] Step 1097/1488: training loss=0.78\n",
      "[1768780739] Step 1098/1488: training loss=0.43\n",
      "[1768780739] Step 1099/1488: training loss=0.93\n",
      "[1768780739] Step 1100/1488: training loss=0.70\n",
      "[1768780741] Step 1101/1488: training loss=0.56\n",
      "[1768780741] Step 1102/1488: training loss=0.89\n",
      "[1768780741] Step 1103/1488: training loss=0.49\n",
      "[1768780743] Step 1104/1488: training loss=0.45\n",
      "[1768780743] Step 1105/1488: training loss=1.03\n",
      "[1768780745] Step 1106/1488: training loss=0.75\n",
      "[1768780745] Step 1107/1488: training loss=0.74\n",
      "[1768780747] Step 1108/1488: training loss=0.99\n",
      "[1768780747] Step 1109/1488: training loss=0.36\n",
      "[1768780750] Step 1110/1488: training loss=0.38\n",
      "[1768780750] Step 1111/1488: training loss=0.58\n",
      "[1768780750] Step 1112/1488: training loss=0.66\n",
      "[1768780752] Step 1113/1488: training loss=0.69\n",
      "[1768780752] Step 1114/1488: training loss=0.66\n",
      "[1768780752] Step 1115/1488: training loss=0.90\n",
      "[1768780754] Step 1116/1488: training loss=0.54\n",
      "[1768780754] Step 1117/1488: training loss=0.70\n",
      "[1768780754] Step 1118/1488: training loss=0.64\n",
      "[1768780756] Step 1119/1488: training loss=0.59\n",
      "[1768780758] Step 1120/1488: training loss=0.67\n",
      "[1768780758] Step 1121/1488: training loss=0.52\n",
      "[1768780758] Step 1122/1488: training loss=0.17\n",
      "[1768780760] Step 1123/1488: training loss=0.74\n",
      "[1768780760] Step 1124/1488: training loss=0.81\n",
      "[1768780760] Step 1125/1488: training loss=0.76\n",
      "[1768780762] Step 1126/1488: training loss=0.97\n",
      "[1768780762] Step 1127/1488: training loss=0.65\n",
      "[1768780764] Step 1128/1488: training loss=0.63\n",
      "[1768780764] Step 1129/1488: training loss=0.83\n",
      "[1768780766] Step 1130/1488: training loss=0.61\n",
      "[1768780766] Step 1131/1488: training loss=0.90\n",
      "[1768780768] Step 1132/1488: training loss=0.57\n",
      "[1768780768] Step 1133/1488: training loss=0.73\n",
      "[1768780770] Step 1134/1488: training loss=0.68\n",
      "[1768780772] Step 1135/1488: training loss=0.58\n",
      "[1768780772] Step 1136/1488: training loss=1.01\n",
      "[1768780774] Step 1137/1488: training loss=0.85\n",
      "[1768780774] Step 1138/1488: training loss=0.86\n",
      "[1768780776] Step 1139/1488: training loss=1.14\n",
      "[1768780776] Step 1140/1488: training loss=0.41\n",
      "[1768780776] Step 1141/1488: training loss=0.87\n",
      "[1768780778] Step 1142/1488: training loss=0.74\n",
      "[1768780780] Step 1143/1488: training loss=0.93\n",
      "[1768780780] Step 1144/1488: training loss=0.74\n",
      "[1768780782] Step 1145/1488: training loss=0.79\n",
      "[1768780782] Step 1146/1488: training loss=1.16\n",
      "[1768780784] Step 1147/1488: training loss=0.50\n",
      "[1768780784] Step 1148/1488: training loss=0.36\n",
      "[1768780784] Step 1149/1488: training loss=0.65\n",
      "[1768780787] Step 1150/1488: training loss=0.65\n",
      "[1768780787] Step 1151/1488: training loss=0.79\n",
      "[1768780791] Step 1152/1488: training loss=0.76\n",
      "[1768780793] Step 1153/1488: training loss=1.31\n",
      "[1768780795] Step 1154/1488: training loss=0.59\n",
      "[1768780795] Step 1155/1488: training loss=0.61\n",
      "[1768780797] Step 1156/1488: training loss=0.45\n",
      "[1768780797] Step 1157/1488: training loss=0.70\n",
      "[1768780797] Step 1158/1488: training loss=0.41\n",
      "[1768780799] Step 1159/1488: training loss=0.62\n",
      "[1768780801] Step 1160/1488: training loss=0.62\n",
      "[1768780801] Step 1161/1488: training loss=0.91\n",
      "[1768780801] Step 1162/1488: training loss=0.68\n",
      "[1768780803] Step 1163/1488: training loss=0.16\n",
      "[1768780805] Step 1164/1488: training loss=0.58\n",
      "[1768780805] Step 1165/1488: training loss=0.48\n",
      "[1768780805] Step 1166/1488: training loss=0.76\n",
      "[1768780805] Step 1167/1488: training loss=0.43\n",
      "[1768780807] Step 1168/1488: training loss=0.45\n",
      "[1768780807] Step 1169/1488: training loss=0.61\n",
      "[1768780809] Step 1170/1488: training loss=0.96\n",
      "[1768780809] Step 1171/1488: training loss=0.66\n",
      "[1768780809] Step 1172/1488: training loss=0.87\n",
      "[1768780811] Step 1173/1488: training loss=0.65\n",
      "[1768780811] Step 1174/1488: training loss=1.03\n",
      "[1768780813] Step 1175/1488: training loss=0.63\n",
      "[1768780813] Step 1176/1488: training loss=0.56\n",
      "[1768780813] Step 1177/1488: training loss=0.44\n",
      "[1768780815] Step 1178/1488: training loss=0.34\n",
      "[1768780819] Step 1179/1488: training loss=0.96\n",
      "[1768780819] Step 1180/1488: training loss=0.25\n",
      "[1768780819] Step 1181/1488: training loss=0.31\n",
      "[1768780819] Step 1182/1488: training loss=0.42\n",
      "[1768780821] Step 1183/1488: training loss=0.41\n",
      "[1768780821] Step 1184/1488: training loss=0.44\n",
      "[1768780823] Step 1185/1488: training loss=0.72\n",
      "[1768780823] Step 1186/1488: training loss=0.45\n",
      "[1768780826] Step 1187/1488: training loss=0.90\n",
      "[1768780830] Step 1188/1488: training loss=0.59\n",
      "[1768780830] Step 1189/1488: training loss=0.77\n",
      "[1768780830] Step 1190/1488: training loss=0.95\n",
      "[1768780832] Step 1191/1488: training loss=0.29\n",
      "[1768780832] Step 1192/1488: training loss=0.89\n",
      "[1768780832] Step 1193/1488: training loss=0.60\n",
      "[1768780834] Step 1194/1488: training loss=0.80\n",
      "[1768780834] Step 1195/1488: training loss=0.41\n",
      "[1768780834] Step 1196/1488: training loss=0.33\n",
      "[1768780836] Step 1197/1488: training loss=0.82\n",
      "[1768780836] Step 1198/1488: training loss=0.55\n",
      "[1768780836] Step 1199/1488: training loss=0.60\n",
      "[1768780838] Step 1200/1488: training loss=0.51\n",
      "[1768780838] Step 1201/1488: training loss=0.72\n",
      "[1768780842] Step 1202/1488: training loss=0.94\n",
      "[1768780842] Step 1203/1488: training loss=0.53\n",
      "[1768780842] Step 1204/1488: training loss=0.94\n",
      "[1768780844] Step 1205/1488: training loss=0.77\n",
      "[1768780844] Step 1206/1488: training loss=1.06\n",
      "[1768780844] Step 1207/1488: training loss=0.37\n",
      "[1768780846] Step 1208/1488: training loss=0.63\n",
      "[1768780846] Step 1209/1488: training loss=0.57\n",
      "[1768780848] Step 1210/1488: training loss=0.66\n",
      "[1768780848] Step 1211/1488: training loss=1.00\n",
      "[1768780850] Step 1212/1488: training loss=0.62\n",
      "[1768780850] Step 1213/1488: training loss=0.94\n",
      "[1768780850] Step 1214/1488: training loss=0.89\n",
      "[1768780852] Step 1215/1488: training loss=0.95\n",
      "[1768780852] Step 1216/1488: training loss=0.82\n",
      "[1768780852] Step 1217/1488: training loss=0.95\n",
      "[1768780854] Step 1218/1488: training loss=0.41\n",
      "[1768780854] Step 1219/1488: training loss=0.59\n",
      "[1768780854] Step 1220/1488: training loss=0.58\n",
      "[1768780856] Step 1221/1488: training loss=0.68\n",
      "[1768780860] Step 1222/1488: training loss=0.74\n",
      "[1768780860] Step 1223/1488: training loss=0.41\n",
      "[1768780860] Step 1224/1488: training loss=0.82\n",
      "[1768780862] Step 1225/1488: training loss=0.48\n",
      "[1768780867] Step 1226/1488: training loss=0.69\n",
      "[1768780867] Step 1227/1488: training loss=0.67\n",
      "[1768780869] Step 1228/1488: training loss=0.56\n",
      "[1768780869] Step 1229/1488: training loss=0.74\n",
      "[1768780869] Step 1230/1488: training loss=0.55\n",
      "[1768780871] Step 1231/1488: training loss=0.76\n",
      "[1768780871] Step 1232/1488: training loss=0.77\n",
      "[1768780873] Step 1233/1488: training loss=0.78\n",
      "[1768780873] Step 1234/1488: training loss=1.01\n",
      "[1768780873] Step 1235/1488: training loss=0.55\n",
      "[1768780875] Step 1236/1488: training loss=0.53\n",
      "[1768780875] Step 1237/1488: training loss=0.75\n",
      "[1768780877] Step 1238/1488: training loss=0.41\n",
      "[1768780879] Step 1239/1488: training loss=0.79\n",
      "[1768780881] Step 1240/1488: training loss=0.59\n",
      "[1768780881] Step 1241/1488: training loss=0.94\n",
      "[1768780883] Step 1242/1488: training loss=0.69\n",
      "[1768780885] Step 1243/1488: training loss=0.77\n",
      "[1768780885] Step 1244/1488: training loss=0.80\n",
      "[1768780887] Step 1245/1488: training loss=0.72\n",
      "[1768780889] Step 1246/1488: training loss=0.35\n",
      "[1768780889] Step 1247/1488: training loss=0.73\n",
      "[1768780889] Step 1248/1488: training loss=0.72\n",
      "[1768780891] Step 1249/1488: training loss=0.89\n",
      "[1768780891] Step 1250/1488: training loss=0.49\n",
      "[1768780893] Step 1251/1488: training loss=0.66\n",
      "[1768780895] Step 1252/1488: training loss=0.91\n",
      "[1768780895] Step 1253/1488: training loss=0.77\n",
      "[1768780895] Step 1254/1488: training loss=0.50\n",
      "[1768780897] Step 1255/1488: training loss=0.71\n",
      "[1768780897] Step 1256/1488: training loss=0.60\n",
      "[1768780899] Step 1257/1488: training loss=0.90\n",
      "[1768780899] Step 1258/1488: training loss=0.72\n",
      "[1768780901] Step 1259/1488: training loss=0.57\n",
      "[1768780901] Step 1260/1488: training loss=0.42\n",
      "[1768780901] Step 1261/1488: training loss=0.82\n",
      "[1768780903] Step 1262/1488: training loss=0.64\n",
      "[1768780903] Step 1263/1488: training loss=0.72\n",
      "[1768780903] Step 1264/1488: training loss=0.79\n",
      "[1768780905] Step 1265/1488: training loss=0.71\n",
      "[1768780905] Step 1266/1488: training loss=0.69\n",
      "[1768780910] Step 1267/1488: training loss=0.67\n",
      "[1768780910] Step 1268/1488: training loss=0.68\n",
      "[1768780910] Step 1269/1488: training loss=0.59\n",
      "[1768780912] Step 1270/1488: training loss=0.40\n",
      "[1768780914] Step 1271/1488: training loss=0.63\n",
      "[1768780914] Step 1272/1488: training loss=0.49\n",
      "[1768780914] Step 1273/1488: training loss=0.64\n",
      "[1768780918] Step 1274/1488: training loss=0.97\n",
      "[1768780918] Step 1275/1488: training loss=0.73\n",
      "[1768780918] Step 1276/1488: training loss=0.57\n",
      "[1768780920] Step 1277/1488: training loss=0.49\n",
      "[1768780920] Step 1278/1488: training loss=0.56\n",
      "[1768780922] Step 1279/1488: training loss=0.73\n",
      "[1768780922] Step 1280/1488: training loss=0.74\n",
      "[1768780922] Step 1281/1488: training loss=0.60\n",
      "[1768780924] Step 1282/1488: training loss=0.69\n",
      "[1768780926] Step 1283/1488: training loss=0.74\n",
      "[1768780926] Step 1284/1488: training loss=0.57\n",
      "[1768780928] Step 1285/1488: training loss=0.60\n",
      "[1768780930] Step 1286/1488: training loss=0.93\n",
      "[1768780932] Step 1287/1488: training loss=1.00\n",
      "[1768780932] Step 1288/1488: training loss=0.61\n",
      "[1768780932] Step 1289/1488: training loss=0.46\n",
      "[1768780934] Step 1290/1488: training loss=0.59\n",
      "[1768780934] Step 1291/1488: training loss=0.52\n",
      "[1768780934] Step 1292/1488: training loss=0.77\n",
      "[1768780936] Step 1293/1488: training loss=0.68\n",
      "[1768780936] Step 1294/1488: training loss=0.48\n",
      "[1768780936] Step 1295/1488: training loss=0.40\n",
      "[1768780938] Step 1296/1488: training loss=0.67\n",
      "[1768780940] Step 1297/1488: training loss=0.81\n",
      "[1768780940] Step 1298/1488: training loss=0.99\n",
      "[1768780942] Step 1299/1488: training loss=0.52\n",
      "[1768780942] Step 1300/1488: training loss=0.54\n",
      "[1768780944] Step 1301/1488: training loss=0.52\n",
      "[1768780944] Step 1302/1488: training loss=0.55\n",
      "[1768780946] Step 1303/1488: training loss=1.09\n",
      "[1768780946] Step 1304/1488: training loss=0.49\n",
      "[1768780949] Step 1305/1488: training loss=1.19\n",
      "[1768780951] Step 1306/1488: training loss=0.68\n",
      "[1768780951] Step 1307/1488: training loss=0.72\n",
      "[1768780951] Step 1308/1488: training loss=0.61\n",
      "[1768780953] Step 1309/1488: training loss=0.47\n",
      "[1768780953] Step 1310/1488: training loss=0.68\n",
      "[1768780953] Step 1311/1488: training loss=0.86\n",
      "[1768780955] Step 1312/1488: training loss=0.82\n",
      "[1768780955] Step 1313/1488: training loss=0.62\n",
      "[1768780957] Step 1314/1488: training loss=0.44\n",
      "[1768780959] Step 1315/1488: training loss=0.76\n",
      "[1768780959] Step 1316/1488: training loss=0.68\n",
      "[1768780961] Step 1317/1488: training loss=0.46\n",
      "[1768780961] Step 1318/1488: training loss=0.79\n",
      "[1768780961] Step 1319/1488: training loss=0.76\n",
      "[1768780963] Step 1320/1488: training loss=0.66\n",
      "[1768780963] Step 1321/1488: training loss=0.64\n",
      "[1768780965] Step 1322/1488: training loss=0.46\n",
      "[1768780965] Step 1323/1488: training loss=0.88\n",
      "[1768780965] Step 1324/1488: training loss=0.77\n",
      "[1768780968] Step 1325/1488: training loss=0.50\n",
      "[1768780968] Step 1326/1488: training loss=0.67\n",
      "[1768780970] Step 1327/1488: training loss=0.82\n",
      "[1768780970] Step 1328/1488: training loss=0.76\n",
      "[1768780970] Step 1329/1488: training loss=0.54\n",
      "[1768780972] Step 1330/1488: training loss=1.28\n",
      "[1768780974] Step 1331/1488: training loss=0.96\n",
      "[1768780974] Step 1332/1488: training loss=0.97\n",
      "[1768780974] Step 1333/1488: training loss=0.55\n",
      "[1768780976] Step 1334/1488: training loss=0.69\n",
      "[1768780976] Step 1335/1488: training loss=0.88\n",
      "[1768780976] Step 1336/1488: training loss=0.42\n",
      "[1768780978] Step 1337/1488: training loss=0.64\n",
      "[1768780978] Step 1338/1488: training loss=0.33\n",
      "[1768780980] Step 1339/1488: training loss=0.36\n",
      "[1768780980] Step 1340/1488: training loss=0.39\n",
      "[1768780982] Step 1341/1488: training loss=0.97\n",
      "[1768780982] Step 1342/1488: training loss=0.75\n",
      "[1768780982] Step 1343/1488: training loss=0.67\n",
      "[1768780984] Step 1344/1488: training loss=0.94\n",
      "[1768780984] Step 1345/1488: training loss=0.88\n",
      "[1768780984] Step 1346/1488: training loss=0.52\n",
      "[1768780986] Step 1347/1488: training loss=0.62\n",
      "[1768780986] Step 1348/1488: training loss=0.63\n",
      "[1768780988] Step 1349/1488: training loss=0.92\n",
      "[1768780988] Step 1350/1488: training loss=0.70\n",
      "[1768780990] Step 1351/1488: training loss=0.65\n",
      "[1768780990] Step 1352/1488: training loss=0.65\n",
      "[1768780990] Step 1353/1488: training loss=0.68\n",
      "[1768780992] Step 1354/1488: training loss=1.06\n",
      "[1768780994] Step 1355/1488: training loss=0.67\n",
      "[1768780994] Step 1356/1488: training loss=1.00\n",
      "[1768780994] Step 1357/1488: training loss=0.50\n",
      "[1768780996] Step 1358/1488: training loss=0.95\n",
      "[1768780996] Step 1359/1488: training loss=0.63\n",
      "[1768780996] Step 1360/1488: training loss=0.45\n",
      "[1768780998] Step 1361/1488: training loss=0.56\n",
      "[1768781002] Step 1362/1488: training loss=0.97\n",
      "[1768781002] Step 1363/1488: training loss=0.31\n",
      "[1768781002] Step 1364/1488: training loss=0.49\n",
      "[1768781002] Step 1365/1488: training loss=0.94\n",
      "[1768781004] Step 1366/1488: training loss=0.36\n",
      "[1768781005] Step 1367/1488: training loss=0.72\n",
      "[1768781007] Step 1368/1488: training loss=0.25\n",
      "[1768781007] Step 1369/1488: training loss=0.74\n",
      "[1768781007] Step 1370/1488: training loss=0.82\n",
      "[1768781009] Step 1371/1488: training loss=0.47\n",
      "[1768781009] Step 1372/1488: training loss=0.65\n",
      "[1768781011] Step 1373/1488: training loss=0.47\n",
      "[1768781011] Step 1374/1488: training loss=0.66\n",
      "[1768781011] Step 1375/1488: training loss=0.29\n",
      "[1768781013] Step 1376/1488: training loss=0.79\n",
      "[1768781013] Step 1377/1488: training loss=0.70\n",
      "[1768781013] Step 1378/1488: training loss=0.86\n",
      "[1768781015] Step 1379/1488: training loss=0.46\n",
      "[1768781015] Step 1380/1488: training loss=0.83\n",
      "[1768781017] Step 1381/1488: training loss=0.96\n",
      "[1768781017] Step 1382/1488: training loss=0.40\n",
      "[1768781017] Step 1383/1488: training loss=0.60\n",
      "[1768781019] Step 1384/1488: training loss=1.14\n",
      "[1768781019] Step 1385/1488: training loss=0.52\n",
      "[1768781019] Step 1386/1488: training loss=0.55\n",
      "[1768781021] Step 1387/1488: training loss=0.78\n",
      "[1768781021] Step 1388/1488: training loss=0.72\n",
      "[1768781021] Step 1389/1488: training loss=1.05\n",
      "[1768781023] Step 1390/1488: training loss=0.43\n",
      "[1768781023] Step 1391/1488: training loss=0.53\n",
      "[1768781023] Step 1392/1488: training loss=0.83\n",
      "[1768781025] Step 1393/1488: training loss=0.88\n",
      "[1768781025] Step 1394/1488: training loss=0.72\n",
      "[1768781027] Step 1395/1488: training loss=0.36\n",
      "[1768781027] Step 1396/1488: training loss=0.62\n",
      "[1768781029] Step 1397/1488: training loss=0.55\n",
      "[1768781031] Step 1398/1488: training loss=0.77\n",
      "[1768781031] Step 1399/1488: training loss=0.63\n",
      "[1768781033] Step 1400/1488: training loss=0.65\n",
      "[1768781033] Step 1401/1488: training loss=0.96\n",
      "[1768781035] Step 1402/1488: training loss=0.72\n",
      "[1768781037] Step 1403/1488: training loss=1.26\n",
      "[1768781039] Step 1404/1488: training loss=1.15\n",
      "[1768781041] Step 1405/1488: training loss=0.61\n",
      "[1768781043] Step 1406/1488: training loss=0.76\n",
      "[1768781043] Step 1407/1488: training loss=0.70\n",
      "[1768781045] Step 1408/1488: training loss=0.42\n",
      "[1768781048] Step 1409/1488: training loss=1.00\n",
      "[1768781048] Step 1410/1488: training loss=0.79\n",
      "[1768781050] Step 1411/1488: training loss=0.74\n",
      "[1768781052] Step 1412/1488: training loss=0.72\n",
      "[1768781052] Step 1413/1488: training loss=0.38\n",
      "[1768781052] Step 1414/1488: training loss=0.53\n",
      "[1768781054] Step 1415/1488: training loss=0.47\n",
      "[1768781056] Step 1416/1488: training loss=0.83\n",
      "[1768781056] Step 1417/1488: training loss=0.77\n",
      "[1768781058] Step 1418/1488: training loss=0.62\n",
      "[1768781058] Step 1419/1488: training loss=0.96\n",
      "[1768781060] Step 1420/1488: training loss=0.67\n",
      "[1768781062] Step 1421/1488: training loss=0.56\n",
      "[1768781062] Step 1422/1488: training loss=1.07\n",
      "[1768781064] Step 1423/1488: training loss=0.65\n",
      "[1768781064] Step 1424/1488: training loss=0.93\n",
      "[1768781064] Step 1425/1488: training loss=0.58\n",
      "[1768781066] Step 1426/1488: training loss=0.83\n",
      "[1768781068] Step 1427/1488: training loss=0.62\n",
      "[1768781068] Step 1428/1488: training loss=0.80\n",
      "[1768781068] Step 1429/1488: training loss=0.61\n",
      "[1768781070] Step 1430/1488: training loss=0.44\n",
      "[1768781070] Step 1431/1488: training loss=0.63\n",
      "[1768781072] Step 1432/1488: training loss=0.57\n",
      "[1768781072] Step 1433/1488: training loss=0.95\n",
      "[1768781074] Step 1434/1488: training loss=1.17\n",
      "[1768781076] Step 1435/1488: training loss=0.43\n",
      "[1768781076] Step 1436/1488: training loss=0.62\n",
      "[1768781076] Step 1437/1488: training loss=0.65\n",
      "[1768781078] Step 1438/1488: training loss=0.90\n",
      "[1768781078] Step 1439/1488: training loss=0.65\n",
      "[1768781080] Step 1440/1488: training loss=0.73\n",
      "[1768781080] Step 1441/1488: training loss=0.93\n",
      "[1768781082] Step 1442/1488: training loss=0.71\n",
      "[1768781082] Step 1443/1488: training loss=0.49\n",
      "[1768781082] Step 1444/1488: training loss=0.50\n",
      "[1768781084] Step 1445/1488: training loss=1.31\n",
      "[1768781087] Step 1446/1488: training loss=0.64\n",
      "[1768781087] Step 1447/1488: training loss=0.62\n",
      "[1768781087] Step 1448/1488: training loss=0.73\n",
      "[1768781089] Step 1449/1488: training loss=0.72\n",
      "[1768781089] Step 1450/1488: training loss=0.69\n",
      "[1768781091] Step 1451/1488: training loss=0.71\n",
      "[1768781091] Step 1452/1488: training loss=0.39\n",
      "[1768781093] Step 1453/1488: training loss=0.77\n",
      "[1768781093] Step 1454/1488: training loss=1.10\n",
      "[1768781095] Step 1455/1488: training loss=0.67\n",
      "[1768781095] Step 1456/1488: training loss=0.61\n",
      "[1768781095] Step 1457/1488: training loss=0.39\n",
      "[1768781097] Step 1458/1488: training loss=0.73\n",
      "[1768781097] Step 1459/1488: training loss=0.67\n",
      "[1768781097] Step 1460/1488: training loss=0.51\n",
      "[1768781099] Step 1461/1488: training loss=0.52\n",
      "[1768781099] Step 1462/1488: training loss=0.66\n",
      "[1768781099] Step 1463/1488: training loss=0.57\n",
      "[1768781101] Step 1464/1488: training loss=0.56\n",
      "[1768781103] Step 1465/1488: training loss=0.93\n",
      "[1768781103] Step 1466/1488: training loss=0.97\n",
      "[1768781103] Step 1467/1488: training loss=0.78\n",
      "[1768781105] Step 1468/1488: training loss=0.85\n",
      "[1768781105] Step 1469/1488: training loss=1.03\n",
      "[1768781107] Step 1470/1488: training loss=1.10\n",
      "[1768781107] Step 1471/1488: training loss=0.76\n",
      "[1768781109] Step 1472/1488: training loss=0.63\n",
      "[1768781109] Step 1473/1488: training loss=0.71\n",
      "[1768781111] Step 1474/1488: training loss=1.10\n",
      "[1768781111] Step 1475/1488: training loss=0.63\n",
      "[1768781115] Step 1476/1488: training loss=0.82\n",
      "[1768781115] Step 1477/1488: training loss=1.20\n",
      "[1768781117] Step 1478/1488: training loss=0.79\n",
      "[1768781117] Step 1479/1488: training loss=0.56\n",
      "[1768781117] Step 1480/1488: training loss=0.69\n",
      "[1768781119] Step 1481/1488: training loss=0.73\n",
      "[1768781119] Step 1482/1488: training loss=0.82\n",
      "[1768781119] Step 1483/1488: training loss=0.62\n",
      "[1768781122] Step 1484/1488: training loss=0.39\n",
      "[1768781122] Step 1485/1488: training loss=0.45\n",
      "[1768781122] Step 1486/1488: training loss=0.99\n",
      "[1768781124] Step 1487/1488: training loss=0.52\n",
      "[1768781124] Step 1488/1488: training loss=0.44\n",
      "[1768781141] Checkpoint created at step 496\n",
      "[1768781141] Checkpoint created at step 992\n",
      "[1768781141] New fine-tuned model created\n",
      "[1768781141] Evaluating model against our usage policies\n",
      "[1768781867] Moderation checks for snapshot ft:gpt-4.1-nano-2025-04-14:tavily::CzX41hjk passed.\n",
      "[1768781867] Usage policy evaluations completed, model is now enabled for sampling\n",
      "[1768781869] The job has successfully completed\n",
      "\n",
      "============================================================\n",
      "✓ Fine-tuning completed!\n",
      "Fine-tuned model: ft:gpt-4.1-nano-2025-04-14:tavily::CzX41hjk\n",
      "Trained tokens: 14,982,684\n",
      "Actual cost: $22.47\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ gpt-4.1-nano-2025-04-14 → ft:gpt-4.1-nano-2025-04-14:tavily::CzX41hjk\n",
      "\n",
      "\n",
      "======================================================================\n",
      "All fine-tuning jobs completed!\n",
      "======================================================================\n",
      "gpt-4.1-mini-2025-04-14\n",
      "  → ft:gpt-4.1-mini-2025-04-14:tavily::CzWAcE6p\n",
      "gpt-4.1-nano-2025-04-14\n",
      "  → ft:gpt-4.1-nano-2025-04-14:tavily::CzX41hjk\n"
     ]
    }
   ],
   "source": [
    "from train.finetune import prepare_and_train\n",
    "\n",
    "# Fine-tune both models using the training data\n",
    "models_to_train = [\n",
    "    \"gpt-4.1-mini-2025-04-14\",\n",
    "    \"gpt-4.1-nano-2025-04-14\"\n",
    "]\n",
    "\n",
    "finetuned_models = {}\n",
    "\n",
    "for model in models_to_train:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Starting fine-tuning for {model}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    finetuned_model_id = prepare_and_train(\n",
    "        model=model,\n",
    "        train_json_path='data/goldstandard_train.json',\n",
    "        n_epochs=3\n",
    "    )\n",
    "    \n",
    "    finetuned_models[model] = finetuned_model_id\n",
    "    print(f\"\\n✓ {model} → {finetuned_model_id}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"All fine-tuning jobs completed!\")\n",
    "print(f\"{'='*70}\")\n",
    "for base, ft in finetuned_models.items():\n",
    "    print(f\"{base}\")\n",
    "    print(f\"  → {ft}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24339b5b",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "At this point I'll define a small benchmark of 20% of the validation set for model selection, for cost and time efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029538b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 496 items from gold standard validation set\n",
      "Using tokenizer: o200k_base\n",
      "\n",
      "Items under 250,000 tokens: 494\n",
      "Items over 250,000 tokens: 2\n",
      "\n",
      "Benchmark set: 99 items (all under token limit)\n",
      "\n",
      "✓ Benchmark set saved to data/goldstandard_validation_benchmark.json\n",
      "✓ All benchmarking examples are within the 250,000 token limit for fine-tuning\n",
      "\n",
      "Creating baseline subset with same URLs...\n",
      "Matched 99 baseline items\n",
      "✓ Baseline subset saved to data/baseline_validation_benchmark.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "PAGE_MAX_TOKENS = 250000\n",
    "\n",
    "# Load the gold standard validation dataset\n",
    "with open('data/goldstandard_validation.json', 'r', encoding='utf-8') as f:\n",
    "    gold_standard = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(gold_standard)} items from gold standard validation set\")\n",
    "\n",
    "# Initialize tokenizer for token counting (using gpt-4.1 family)\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")  # gpt-4.1 uses same tokenizer as gpt-4o\n",
    "print(f\"Using tokenizer: {tokenizer.name}\")\n",
    "\n",
    "# Split data based on token count\n",
    "items_under_limit = []\n",
    "items_over_limit = []\n",
    "\n",
    "for item in gold_standard:\n",
    "    token_count = len(tokenizer.encode(item['markdown_content']))\n",
    "    if token_count <= PAGE_MAX_TOKENS:\n",
    "        items_under_limit.append(item)\n",
    "    else:\n",
    "        items_over_limit.append(item)\n",
    "\n",
    "print(f\"\\nItems under {PAGE_MAX_TOKENS:,} tokens: {len(items_under_limit)}\")\n",
    "print(f\"Items over {PAGE_MAX_TOKENS:,} tokens: {len(items_over_limit)}\")\n",
    "\n",
    "# Calculate target benchmarking size (20% of total validation set, ~100 items)\n",
    "target_size = len(gold_standard) // 5\n",
    "\n",
    "# Take up to 20% of total for benchmarking (only from items under limit)\n",
    "validation_data = items_under_limit[:target_size]\n",
    "\n",
    "print(f\"\\nBenchmark set: {len(validation_data)} items (all under token limit)\")\n",
    "\n",
    "# Save benchmark set\n",
    "bm_path = 'data/goldstandard_validation_benchmark.json'\n",
    "with open(bm_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(validation_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Benchmark set saved to {bm_path}\")\n",
    "print(f\"✓ All benchmarking examples are within the {PAGE_MAX_TOKENS:,} token limit for fine-tuning\")\n",
    "\n",
    "# Create baseline subset matching the same URLs\n",
    "print(\"\\nCreating baseline subset with same URLs...\")\n",
    "with open('data/baseline_1k.json', 'r', encoding='utf-8') as f:\n",
    "    baseline_full = json.load(f)\n",
    "\n",
    "# Extract URLs from validation_data\n",
    "benchmark_urls = {item['url'] for item in validation_data}\n",
    "\n",
    "# Filter baseline data to match benchmark URLs\n",
    "baseline_subset = [item for item in baseline_full['data'] if item['url'] in benchmark_urls]\n",
    "\n",
    "print(f\"Matched {len(baseline_subset)} baseline items\")\n",
    "\n",
    "# Save baseline subset\n",
    "baseline_subset_path = 'data/baseline_validation_benchmark.json'\n",
    "with open(baseline_subset_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(baseline_subset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Baseline subset saved to {baseline_subset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590b2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model 'gpt-5.2-2025-12-11' not recognized by tiktoken. Using 'gpt-5' tokenizer as fallback.\n",
      "\n",
      "======================================================================\n",
      "Model 1/10: Baseline\n",
      "======================================================================\n",
      "Loading baseline summaries from data/baseline_validation_benchmark.json\n",
      "✓ Filtered to 99 baseline summaries matching eval subset\n",
      "✓ Loaded 99 baseline summaries\n",
      "✓ Saved to data/inference_baseline.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/10] Judging:   0%|          | 0/99 [00:00<?, ?sample/s]"
     ]
    }
   ],
   "source": [
    "from evaluation.benchmark import BenchmarkingSuite\n",
    "from agents.config import RATES\n",
    "\n",
    "# 1. Define models to test (from your RATES keys)\n",
    "test_models = ['Baseline'] + [model for model in RATES.keys()]\n",
    "\n",
    "# 2. Prepare subset (validation subset already defined)\n",
    "subset_data = validation_data\n",
    "\n",
    "# 3. Run flow\n",
    "suite = BenchmarkingSuite(rates=RATES)\n",
    "report_df = suite.run_benchmark(subset_data, test_models)\n",
    "\n",
    "# 4. Display stylized table\n",
    "report_df.set_index('model').style.background_gradient(cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
